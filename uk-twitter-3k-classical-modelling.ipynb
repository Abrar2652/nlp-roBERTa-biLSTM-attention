{"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Md Abrar Jahin","metadata":{"id":"gtbOK3YG_2Sb"}},{"cell_type":"markdown","source":"# Modelling","metadata":{"id":"ZqIoLLGtAhix"}},{"cell_type":"code","source":"#@title\nimport sklearn\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.metrics import davies_bouldin_score, pairwise_distances, silhouette_samples, silhouette_score, confusion_matrix, roc_curve, auc\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, AgglomerativeClustering \n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom time import time\nfrom datetime import datetime\nimport matplotlib.cm as cm\nfrom scipy.spatial.distance import cdist\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom datetime import datetime\nfrom joblib import Parallel, delayed\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport joblib\n\n#Import libraries:\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom dask.distributed import Client\n\n# It's recommended to use dask_cuda for GPU assignment\n!pip install dask dask_cuda dask-ml \n!pip install tensorflow\n\nfrom dask_cuda import LocalCUDACluster\nfrom dask import array as da\n\n%matplotlib inline","metadata":{"id":"sqmHQn42g2Wn","scrolled":true,"execution":{"iopub.status.busy":"2023-05-18T16:15:38.853168Z","iopub.execute_input":"2023-05-18T16:15:38.853693Z","iopub.status.idle":"2023-05-18T16:16:35.445776Z","shell.execute_reply.started":"2023-05-18T16:15:38.853643Z","shell.execute_reply":"2023-05-18T16:16:35.443825Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: dask in /opt/conda/lib/python3.7/site-packages (2022.2.0)\nCollecting dask_cuda\n  Downloading dask_cuda-22.2.0-py3-none-any.whl (82 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting dask-ml\n  Downloading dask_ml-2022.5.27-py3-none-any.whl (148 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.5/148.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.7/site-packages (from dask) (6.0)\nRequirement already satisfied: partd>=0.3.10 in /opt/conda/lib/python3.7/site-packages (from dask) (1.3.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from dask) (23.0)\nRequirement already satisfied: cloudpickle>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from dask) (2.2.1)\nRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from dask) (2023.1.0)\nRequirement already satisfied: toolz>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from dask) (0.11.2)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from dask_cuda) (1.21.6)\nCollecting distributed<=2022.01.0,>=2021.11.1\n  Downloading distributed-2022.1.0-py3-none-any.whl (822 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m822.1/822.1 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting dask\n  Downloading dask-2022.1.0-py3-none-any.whl (1.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numba>=0.53.1 in /opt/conda/lib/python3.7/site-packages (from dask_cuda) (0.56.4)\nCollecting pynvml>=11.0.0\n  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas>=0.24.2 in /opt/conda/lib/python3.7/site-packages (from dask-ml) (1.3.5)\nRequirement already satisfied: multipledispatch>=0.4.9 in /opt/conda/lib/python3.7/site-packages (from dask-ml) (0.6.0)\nCollecting dask-glm>=0.2.0\n  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from dask-ml) (1.7.3)\nRequirement already satisfied: scikit-learn>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from dask-ml) (1.0.2)\nCollecting dask[array,dataframe]>=2.4.0\n  Downloading dask-2022.1.1-py3-none-any.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.7/site-packages (from distributed<=2022.01.0,>=2021.11.1->dask_cuda) (5.9.3)\nRequirement already satisfied: msgpack>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from distributed<=2022.01.0,>=2021.11.1->dask_cuda) (1.0.4)\nRequirement already satisfied: tblib>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from distributed<=2022.01.0,>=2021.11.1->dask_cuda) (1.7.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from distributed<=2022.01.0,>=2021.11.1->dask_cuda) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from distributed<=2022.01.0,>=2021.11.1->dask_cuda) (59.8.0)\nRequirement already satisfied: tornado>=5 in /opt/conda/lib/python3.7/site-packages (from distributed<=2022.01.0,>=2021.11.1->dask_cuda) (6.2)\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /opt/conda/lib/python3.7/site-packages (from distributed<=2022.01.0,>=2021.11.1->dask_cuda) (2.4.0)\nRequirement already satisfied: zict>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from distributed<=2022.01.0,>=2021.11.1->dask_cuda) (2.2.0)\nRequirement already satisfied: click>=6.6 in /opt/conda/lib/python3.7/site-packages (from distributed<=2022.01.0,>=2021.11.1->dask_cuda) (8.1.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from multipledispatch>=0.4.9->dask-ml) (1.16.0)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.53.1->dask_cuda) (0.39.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from numba>=0.53.1->dask_cuda) (4.11.4)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.2->dask-ml) (2022.7.1)\nRequirement already satisfied: locket in /opt/conda/lib/python3.7/site-packages (from partd>=0.3.10->dask) (1.0.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->dask-ml) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->dask-ml) (1.2.0)\nRequirement already satisfied: heapdict in /opt/conda/lib/python3.7/site-packages (from zict>=0.1.3->distributed<=2022.01.0,>=2021.11.1->dask_cuda) (1.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->numba>=0.53.1->dask_cuda) (3.11.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->numba>=0.53.1->dask_cuda) (4.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->distributed<=2022.01.0,>=2021.11.1->dask_cuda) (2.1.1)\nInstalling collected packages: pynvml, dask, distributed, dask-glm, dask_cuda, dask-ml\n  Attempting uninstall: dask\n    Found existing installation: dask 2022.2.0\n    Uninstalling dask-2022.2.0:\n      Successfully uninstalled dask-2022.2.0\n  Attempting uninstall: distributed\n    Found existing installation: distributed 2022.2.0\n    Uninstalling distributed-2022.2.0:\n      Successfully uninstalled distributed-2022.2.0\nSuccessfully installed dask-2022.1.0 dask-glm-0.2.0 dask-ml-2022.5.27 dask_cuda-22.2.0 distributed-2022.1.0 pynvml-11.5.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.11.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.4.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.51.1)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.29.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.2.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (15.0.6.1)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.21.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.0)\nRequirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.1.21)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.2)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.35.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.11.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\nInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.83.0 which is incompatible.\nonnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\napache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.19.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#@title\nimport pandas as pd\nimport numpy as np\nimport gensim \nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndf = pd.read_csv('/kaggle/input/uk-twitter-covid19-dataset/sample_data.csv')","metadata":{"cellView":"form","id":"gUNndAYvOM4a","execution":{"iopub.status.busy":"2023-05-18T16:16:35.448852Z","iopub.execute_input":"2023-05-18T16:16:35.449304Z","iopub.status.idle":"2023-05-18T16:16:36.381794Z","shell.execute_reply.started":"2023-05-18T16:16:35.449257Z","shell.execute_reply":"2023-05-18T16:16:36.380467Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#@title\n#after manual check\n'''\nnegative_num=len(df[df['senti_vader'] < 0]): This line filters the DataFrame 'df' based on the condition df['senti_vader'] < 0, \nwhich selects all rows where the 'senti_vader' column has a value less than zero (negative). \nThe len() function is then used to determine the number of rows in the filtered DataFrame, \nrepresenting the count of negative values. The count is assigned to the variable 'negative_num'.\n'''\nnegative_num=len(df[df['senti_vader'] < 0])\nprint(\"negative:\", negative_num)\nneutral_num=len(df[df['senti_vader'] == 0])\nprint(\"neutral\", neutral_num)\npositive_num=len(df[df['senti_vader'] > 0])\nprint(\"positive\", positive_num) ","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"xNZq1Lp3_2TA","outputId":"69918135-7fb9-4f56-ee87-8df8dd8f13d8","execution":{"iopub.status.busy":"2023-05-18T16:16:36.383538Z","iopub.execute_input":"2023-05-18T16:16:36.383962Z","iopub.status.idle":"2023-05-18T16:16:36.405038Z","shell.execute_reply.started":"2023-05-18T16:16:36.383923Z","shell.execute_reply":"2023-05-18T16:16:36.403402Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"negative: 1000\nneutral 1000\npositive 1000\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title\n# Funtion for bag-of-words or BoW\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \nfrom nltk.tokenize import word_tokenize\n\ndef bag_of_words(df):\n    bow_vectorizer = CountVectorizer() \n    #Transforming the input text data into a bag-of-words representation\n    bow = bow_vectorizer.fit_transform(df['lemma_sentence(with POS)']) \n    #Printing the size of the vocabulary created by the vectorizer.\n    print(len(bow_vectorizer.vocabulary_))\n    #Returning the bag-of-words representation of the input data\n    return bow\n\ndf_bow=bag_of_words(df)\ndf_bow.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjY0NcjW_2TA","outputId":"003a6005-346b-4372-bc01-acf6301279e3","execution":{"iopub.status.busy":"2023-05-18T16:16:36.408975Z","iopub.execute_input":"2023-05-18T16:16:36.409875Z","iopub.status.idle":"2023-05-18T16:16:36.512367Z","shell.execute_reply.started":"2023-05-18T16:16:36.409821Z","shell.execute_reply":"2023-05-18T16:16:36.511071Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"7222\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(3000, 7222)"},"metadata":{}}]},{"cell_type":"code","source":"#@title\n# function for TF-IDF\ndef tf_idf(df):\n    tf_idf_vectorizer = TfidfVectorizer(norm='l2') #extract features\n    tfidf = tf_idf_vectorizer.fit_transform(df['lemma_sentence(with POS)']) #vectors\n    return tfidf\ndf_tfidf=tf_idf(df)\ndf_tfidf.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofxvGqW9_2TA","outputId":"83af53a9-6b0c-4f5d-f91f-96833cd650d1","execution":{"iopub.status.busy":"2023-05-18T16:16:36.514004Z","iopub.execute_input":"2023-05-18T16:16:36.515145Z","iopub.status.idle":"2023-05-18T16:16:36.615573Z","shell.execute_reply.started":"2023-05-18T16:16:36.515098Z","shell.execute_reply":"2023-05-18T16:16:36.614028Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(3000, 7222)"},"metadata":{}}]},{"cell_type":"code","source":"#@title\n# function for Word2vec\n#reference：https://www.pythonf.cn/read/93491\n#https://github.com/Shwetago/Sentiment_Analysis/blob/master/Twitter_Sentiment_Analysis.ipynb\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nTokenize_tweet = df['lemma_sentence(with POS)'].apply(word_tokenize) #Tokenizing the 'lemma_sentence(with POS)' column in the dataframe\nprint(Tokenize_tweet)\n\n#Creating a Word2Vec model with specified parameters\nModel_W2V = gensim.models.Word2Vec(Tokenize_tweet, vector_size=200, #features\n                                   window=5, \n                                   min_count=1, \n                                   sg=1,  #skip-gram model\n                                   hs=0,\n                                   negative=10, \n                                   workers=2, \n                                   seed=34) ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R0sBj_zD_2TB","outputId":"aa7da10d-b8be-4be8-a41a-27c3bb5f9611","execution":{"iopub.status.busy":"2023-05-18T16:16:36.617497Z","iopub.execute_input":"2023-05-18T16:16:36.617907Z","iopub.status.idle":"2023-05-18T16:16:39.286993Z","shell.execute_reply.started":"2023-05-18T16:16:36.617866Z","shell.execute_reply":"2023-05-18T16:16:39.285642Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n0       [kid, not, wana, homework, sparkle, covid, spa...\n1       [not, mess, covid, wear, mask, time, london, u...\n2       [problem, airfield, cost, remain, open, verse,...\n3       [remain, astonished, stock, market, not, face,...\n4       [dear, lord, jesus, christ, grace, listen, pra...\n                              ...                        \n2995    [lockdown, not, deal, covid, deal, stupidity, ...\n2996    [regulator, refuse, approve, mass, covid, test...\n2997    [covid, vaccine, week, ago, manchester, real, ...\n2998    [coronavirustrain, covid, covid, wearamask, we...\n2999    [news, cycle, number, covid, death, car, crash...\nName: lemma_sentence(with POS), Length: 3000, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title\n# Each word can get its own vector. The representation of a tweets can the vector sum of each word divided by the total number(average) \n# or just the sum of each word vector\ndef word2vec_tweet(tokens, size):\n    vector=np.zeros(size).reshape((1,size)) #Initializing a zero-filled vector of given size\n    vector_cnt = 0\n    for word in tokens:\n        vector += Model_W2V.wv[word].reshape((1, size)) #Adding word vectors to the tweet vector\n        vector_cnt += 1\n    return vector/vector_cnt  #Calculating the average of tweet vectors\n\ndef word2vec_tweet_2(tokens, size):\n    vector=np.zeros(size).reshape((1,size))\n    vector_cnt = 0\n    for word in tokens:\n        vector += Model_W2V.wv[word].reshape((1, size))\n    return vector  #Returning the sum of tweet vectors\n\ntweet_arr=np.zeros((len(Tokenize_tweet), 200)) #Initializing a zero-filled array for storing tweet vectors.\n\nfor i in range (len(Tokenize_tweet)):\n    tweet_arr[i,:] = word2vec_tweet(Tokenize_tweet[i], 200) #Assigning the average tweet vector to the corresponding row in the array\ntweet_vec_df = pd.DataFrame(tweet_arr)","metadata":{"id":"p5432152_2TB","execution":{"iopub.status.busy":"2023-05-18T16:16:39.288390Z","iopub.execute_input":"2023-05-18T16:16:39.288748Z","iopub.status.idle":"2023-05-18T16:16:39.614487Z","shell.execute_reply.started":"2023-05-18T16:16:39.288710Z","shell.execute_reply":"2023-05-18T16:16:39.612935Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\nfrom gensim.models import KeyedVectors\nimport tensorflow_hub as hub\nimport tensorflow as tf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"KYFnc4boYxK2","outputId":"442c5ab0-df77-4057-a663-8cfdfc8276e5","execution":{"iopub.status.busy":"2023-05-18T16:16:39.617520Z","iopub.execute_input":"2023-05-18T16:16:39.618889Z","iopub.status.idle":"2023-05-18T16:16:47.661990Z","shell.execute_reply.started":"2023-05-18T16:16:39.618825Z","shell.execute_reply":"2023-05-18T16:16:47.660373Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# parameters in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)] #tree number\nmax_features = ['auto', 'sqrt','log2']\nmax_depth = [10,20,30,40,50]\nmin_samples_split = [2, 5, 10, 15]\nmin_samples_leaf = [1, 2, 5, 10]\n\n# Create the param grid\nparam_grid_forest = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\nprint(param_grid_forest)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTeE-51o_2TB","outputId":"6972340a-1b7b-4171-df68-7a8f824774e2","execution":{"iopub.status.busy":"2023-05-18T16:16:47.664583Z","iopub.execute_input":"2023-05-18T16:16:47.666126Z","iopub.status.idle":"2023-05-18T16:16:47.683198Z","shell.execute_reply.started":"2023-05-18T16:16:47.666048Z","shell.execute_reply":"2023-05-18T16:16:47.681146Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"{'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [10, 20, 30, 40, 50], 'min_samples_split': [2, 5, 10, 15], 'min_samples_leaf': [1, 2, 5, 10]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# parameters in Multinomial NB\nparam_grid_nb = {'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0],\n                'fit_prior':[True, False]}\nprint(param_grid_nb)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0Y_cIVT_2TB","outputId":"25290398-b48c-46da-ee67-ac05c7a8b499","execution":{"iopub.status.busy":"2023-05-18T16:16:47.688280Z","iopub.execute_input":"2023-05-18T16:16:47.688743Z","iopub.status.idle":"2023-05-18T16:16:47.698474Z","shell.execute_reply.started":"2023-05-18T16:16:47.688689Z","shell.execute_reply":"2023-05-18T16:16:47.697363Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"{'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0], 'fit_prior': [True, False]}\n","output_type":"stream"}]},{"cell_type":"code","source":"#parameters in SVC\n# c_list=list(range(1,51))\nparam_grid_svc = {'C': [1, 10, 100, 1000],\n                  'kernel': ['linear','poly','rbf','sigmoid'],\n                  'degree': [1,2,3,4]}\nprint(param_grid_svc)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7UwWLyM_2TB","outputId":"d81768c1-9c7b-4ba4-ef50-40cf033997fe","execution":{"iopub.status.busy":"2023-05-18T16:16:47.699863Z","iopub.execute_input":"2023-05-18T16:16:47.700844Z","iopub.status.idle":"2023-05-18T16:16:47.712954Z","shell.execute_reply.started":"2023-05-18T16:16:47.700785Z","shell.execute_reply":"2023-05-18T16:16:47.711677Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{'C': [1, 10, 100, 1000], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'degree': [1, 2, 3, 4]}\n","output_type":"stream"}]},{"cell_type":"code","source":"#Parameters in XGBoost\nlearning_rate = [0.01, 0.1, 0.5, 1, 0.3]\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)] # tree number\nmax_depth = [20,30,40,50]\nmin_child_weight = [1, 3, 5, 7]\ngamma = [0.0, 0.1, 0.2, 0.3]\nsubsample = [0.6, 0.7, 0.8, 0.9]\ncolsample_bytree = [0.6, 0.7, 0.8, 0.9]\nbooster=['gbtree','gblinear']\n\n#Create the param grid for XGBoost\nparam_grid_xgb = {'learning_rate': learning_rate,\n'n_estimators': n_estimators,\n'max_depth': max_depth,\n'min_child_weight': min_child_weight,\n'objective': ['multi:softmax','multi:softprob'],\n'gamma': gamma,\n'subsample': subsample,\n'colsample_bytree': colsample_bytree,\n'booster': booster}\n\nprint(param_grid_xgb)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmAji60rvUb3","outputId":"5ea61b73-a3aa-465b-efa2-2e0a95216b14","execution":{"iopub.status.busy":"2023-05-18T16:16:47.716635Z","iopub.execute_input":"2023-05-18T16:16:47.717100Z","iopub.status.idle":"2023-05-18T16:16:47.729304Z","shell.execute_reply.started":"2023-05-18T16:16:47.717057Z","shell.execute_reply":"2023-05-18T16:16:47.727706Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"{'learning_rate': [0.01, 0.1, 0.5, 1, 0.3], 'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200], 'max_depth': [20, 30, 40, 50], 'min_child_weight': [1, 3, 5, 7], 'objective': ['multi:softmax', 'multi:softprob'], 'gamma': [0.0, 0.1, 0.2, 0.3], 'subsample': [0.6, 0.7, 0.8, 0.9], 'colsample_bytree': [0.6, 0.7, 0.8, 0.9], 'booster': ['gbtree', 'gblinear']}\n","output_type":"stream"}]},{"cell_type":"code","source":"#parameters in Gradient Boosting Machine (GBM)\nlearning_rate = [0.01, 0.1, 0.5, 1]\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)] # tree number\nmax_features = ['auto', 'sqrt','log2']\nmax_depth = [10,20,30,40]\nmin_samples_split = [2, 5, 10, 15]\nmin_samples_leaf = [1, 2, 5, 10]\n\n#Create the param grid for GBM\nparam_grid_gbm = {'learning_rate': learning_rate,\n'n_estimators': n_estimators,\n'max_features': max_features,\n'max_depth': max_depth,\n'min_samples_split': min_samples_split,\n'min_samples_leaf': min_samples_leaf}\nprint(param_grid_gbm)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eWEt89E9g5_u","outputId":"0f1c1119-2aae-40ed-bcc7-5165f01162f6","execution":{"iopub.status.busy":"2023-05-18T16:16:47.731369Z","iopub.execute_input":"2023-05-18T16:16:47.731914Z","iopub.status.idle":"2023-05-18T16:16:47.743094Z","shell.execute_reply.started":"2023-05-18T16:16:47.731857Z","shell.execute_reply":"2023-05-18T16:16:47.741993Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{'learning_rate': [0.01, 0.1, 0.5, 1], 'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [10, 20, 30, 40], 'min_samples_split': [2, 5, 10, 15], 'min_samples_leaf': [1, 2, 5, 10]}\n","output_type":"stream"}]},{"cell_type":"code","source":"#Parameters in LightGBM\nlearning_rate = [0.01, 0.1, 0.5, 1]\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)] # tree number\nmax_depth = [5, 10, 15, 20]\nnum_leaves = [10, 20, 30, 40]\nmin_child_samples = [1, 3, 5, 7]\nsubsample = [0.6, 0.7, 0.8, 0.9]\ncolsample_bytree = [0.6, 0.7, 0.8, 0.9]\n\n#Create the param grid for LightGBM\nparam_grid_lgbm = {'learning_rate': learning_rate,\n'n_estimators': n_estimators,\n'max_depth': max_depth,\n'num_leaves': num_leaves,\n'objective': ['multiclass'],\n'min_child_samples': min_child_samples,\n'subsample': subsample,\n'colsample_bytree': colsample_bytree}\nprint(param_grid_lgbm)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flim9UVQy5VO","outputId":"e2df1ac6-e922-4aad-f2d4-36865e3b0c43","execution":{"iopub.status.busy":"2023-05-18T16:16:47.744454Z","iopub.execute_input":"2023-05-18T16:16:47.745093Z","iopub.status.idle":"2023-05-18T16:16:47.760946Z","shell.execute_reply.started":"2023-05-18T16:16:47.745049Z","shell.execute_reply":"2023-05-18T16:16:47.759694Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"{'learning_rate': [0.01, 0.1, 0.5, 1], 'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200], 'max_depth': [5, 10, 15, 20], 'num_leaves': [10, 20, 30, 40], 'objective': ['multiclass'], 'min_child_samples': [1, 3, 5, 7], 'subsample': [0.6, 0.7, 0.8, 0.9], 'colsample_bytree': [0.6, 0.7, 0.8, 0.9]}\n","output_type":"stream"}]},{"cell_type":"code","source":"#parameters in CatBoost\nlearning_rate = [0.01, 0.1, 0.5, 1]\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)] # tree number\nmax_depth = [10,20,30,40]\nl2_leaf_reg = [1, 3, 5, 7, 9]\n\n#Create the param grid for CatBoost\nparam_grid_catboost = {'learning_rate': learning_rate,\n'n_estimators': n_estimators,\n'loss_function': ['MultiClass'],\n'max_depth': max_depth,\n'l2_leaf_reg': l2_leaf_reg}\nprint(param_grid_catboost)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8eR40BtA86yi","outputId":"222e4afd-401b-4436-a281-638e93523263","execution":{"iopub.status.busy":"2023-05-18T16:16:47.762563Z","iopub.execute_input":"2023-05-18T16:16:47.762934Z","iopub.status.idle":"2023-05-18T16:16:47.773941Z","shell.execute_reply.started":"2023-05-18T16:16:47.762898Z","shell.execute_reply":"2023-05-18T16:16:47.772695Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"{'learning_rate': [0.01, 0.1, 0.5, 1], 'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200], 'loss_function': ['MultiClass'], 'max_depth': [10, 20, 30, 40], 'l2_leaf_reg': [1, 3, 5, 7, 9]}\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title\n# initialize the classifiers\nfrom sklearn.metrics import classification_report \nfrom sklearn.ensemble._forest import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\nimport lightgbm as lgbm\n!pip install catboost\nfrom catboost import CatBoostClassifier\n\nmodel_forest = RandomForestClassifier()\nmodel_nb = MultinomialNB()\nmodel_svc = SVC()\nmodel_xgb = xgb.XGBClassifier()\nmodel_gbm = GradientBoostingClassifier()\nmodel_lgbm = lgbm.LGBMClassifier()\nmodel_cat = CatBoostClassifier()","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"ZPpuGYw8_2TB","outputId":"bc754380-c13c-48fc-f3a7-e14f88c63969","scrolled":true,"execution":{"iopub.status.busy":"2023-05-18T16:16:47.775678Z","iopub.execute_input":"2023-05-18T16:16:47.776093Z","iopub.status.idle":"2023-05-18T16:17:02.539886Z","shell.execute_reply.started":"2023-05-18T16:16:47.776049Z","shell.execute_reply":"2023-05-18T16:17:02.538300Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n.datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAAkCAYAAACE7WrnAAAAdElEQVR42mP4wyMVQQ3M8P///whqYBSDkG2A8bGJo+tBMQifIbgMQ5ZjwGUIPjY2wxiwOZWQZrxhhM0F6IYjq8PqNWyBh4+NN7CpGv2jBo0aNGrQqEGjBtHFIIoLf5pUR2RXkFStsqnSiKBqs4bi6KdW0w8AxFl+XL1lK8wAAAAASUVORK5CYII=');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n</style>\n"},"metadata":{}},{"name":"stdout","text":"Requirement already satisfied: catboost in /opt/conda/lib/python3.7/site-packages (1.1.1)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from catboost) (1.21.6)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from catboost) (0.8.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from catboost) (1.16.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from catboost) (3.5.3)\nRequirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from catboost) (1.3.5)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.7/site-packages (from catboost) (5.13.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from catboost) (1.7.3)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->catboost) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->catboost) (2022.7.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->catboost) (4.38.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->catboost) (3.0.9)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->catboost) (23.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->catboost) (9.4.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->catboost) (1.4.4)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->catboost) (0.11.0)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from plotly->catboost) (8.1.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# BoW/TFIDF/word2vec for classical models","metadata":{"id":"qjm5kByVczVF"}},{"cell_type":"code","source":"#@title\n#BoW for all classification models\n#split the train and test datasets\nfrom sklearn.model_selection import train_test_split\n# df_bow, df_tfidf, tweet_vec_df\n#x_train, x_test, y_train, y_test = train_test_split(df_bow, df['senti_textblob'],test_size = 0.2, random_state=42)\n#x_train, x_test, y_train, y_test = train_test_split(df_tfidf, df['senti_textblob'],test_size = 0.2, random_state=42)\nx_train, x_test, y_train, y_test = train_test_split(tweet_vec_df, df['senti_textblob'],test_size = 0.2, random_state=42)","metadata":{"id":"ZAXTg6oe_2TB","execution":{"iopub.status.busy":"2023-05-18T16:19:02.895433Z","iopub.execute_input":"2023-05-18T16:19:02.896400Z","iopub.status.idle":"2023-05-18T16:19:02.928048Z","shell.execute_reply.started":"2023-05-18T16:19:02.896330Z","shell.execute_reply":"2023-05-18T16:19:02.926059Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#@title\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfrom sklearn.preprocessing import OneHotEncoder\n\n# create an instance of the OneHotEncoder class\nencoder = OneHotEncoder(sparse=False)\n\ny_train_encoded = le.fit_transform(y_train)\ny_test_encoded = le.fit_transform(y_test)","metadata":{"id":"AvF_bN8184xT","execution":{"iopub.status.busy":"2023-05-18T16:19:04.600643Z","iopub.execute_input":"2023-05-18T16:19:04.601882Z","iopub.status.idle":"2023-05-18T16:19:04.610800Z","shell.execute_reply.started":"2023-05-18T16:19:04.601823Z","shell.execute_reply":"2023-05-18T16:19:04.608944Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"### ALL MODELS #######\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom joblib import parallel_backend\n\nx_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny_test = y_test.astype(np.float32)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Scale the input data\nscaler = MinMaxScaler()\nx_train_scaled = scaler.fit_transform(x_train)\n\nmodels_list = [model_forest,model_nb,model_svc,model_gbm,model_lgbm,model_xgb,model_cat]\nmodels_name = ['rf','nb','svc','gbm','lgbm','xgb','cat']\nparam_grids = [param_grid_forest,param_grid_nb,param_grid_svc,param_grid_gbm,param_grid_lgbm,param_grid_xgb,param_grid_catboost]\n\nfor model,name,param_grid in zip(models_list, models_name, param_grids):\n    #best parameters for model (with BoW)\n    RandomGrid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=2, verbose=0, n_jobs=4, random_state=40)\n    with parallel_backend('multiprocessing'):\n        if(name!='xgb' and name!='nb'):\n            RandomGrid.fit(x_train, y_train)\n        elif(name=='nb'):\n            RandomGrid.fit(x_train_scaled, y_train)\n        else:\n            RandomGrid.fit(x_train, y_train_encoded)\n    print('Best params for ' + name+':')\n    print(RandomGrid.best_params_)\n\n    print('Classification report for ' + name+':')\n    if(name!='xgb' and name!='nb'):\n        print(classification_report(y_test, RandomGrid.predict(x_test)))\n    elif(name=='nb'):\n        print(classification_report(y_test, RandomGrid.predict(x_test)))\n    else:\n        print(classification_report(y_test_encoded, RandomGrid.predict(x_test)))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"id":"lcuKCmsfvqdh","outputId":"e7b9567e-59bd-43bb-b3b6-22548b130cd6","scrolled":true,"execution":{"iopub.status.busy":"2023-05-18T16:50:28.846102Z","iopub.execute_input":"2023-05-18T16:50:28.846547Z","iopub.status.idle":"2023-05-18T17:14:23.153108Z","shell.execute_reply.started":"2023-05-18T16:50:28.846510Z","shell.execute_reply":"2023-05-18T17:14:23.151676Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Best params for rf:\n{'n_estimators': 190, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 40}\nClassification report for rf:\n              precision    recall  f1-score   support\n\n        -1.0       0.54      0.16      0.25       135\n         0.0       0.61      0.67      0.64       238\n         1.0       0.50      0.66      0.57       227\n\n    accuracy                           0.55       600\n   macro avg       0.55      0.50      0.49       600\nweighted avg       0.55      0.55      0.53       600\n\nBest params for nb:\n{'fit_prior': True, 'alpha': 0.1}\nClassification report for nb:\n              precision    recall  f1-score   support\n\n        -1.0       0.00      0.00      0.00       135\n         0.0       0.44      0.02      0.03       238\n         1.0       0.38      0.98      0.54       227\n\n    accuracy                           0.38       600\n   macro avg       0.27      0.33      0.19       600\nweighted avg       0.32      0.38      0.22       600\n\nBest params for svc:\n{'kernel': 'linear', 'degree': 2, 'C': 100}\nClassification report for svc:\n              precision    recall  f1-score   support\n\n        -1.0       1.00      0.02      0.04       135\n         0.0       0.65      0.67      0.66       238\n         1.0       0.49      0.76      0.60       227\n\n    accuracy                           0.56       600\n   macro avg       0.71      0.48      0.43       600\nweighted avg       0.67      0.56      0.50       600\n\nBest params for gbm:\n{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'auto', 'max_depth': 30, 'learning_rate': 0.1}\nClassification report for gbm:\n              precision    recall  f1-score   support\n\n        -1.0       0.41      0.21      0.27       135\n         0.0       0.59      0.66      0.62       238\n         1.0       0.52      0.61      0.57       227\n\n    accuracy                           0.54       600\n   macro avg       0.51      0.49      0.49       600\nweighted avg       0.52      0.54      0.52       600\n\nBest params for lgbm:\n{'subsample': 0.9, 'objective': 'multiclass', 'num_leaves': 10, 'n_estimators': 20, 'min_child_samples': 1, 'max_depth': 15, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\nClassification report for lgbm:\n              precision    recall  f1-score   support\n\n        -1.0       0.47      0.05      0.09       135\n         0.0       0.59      0.68      0.63       238\n         1.0       0.50      0.68      0.57       227\n\n    accuracy                           0.54       600\n   macro avg       0.52      0.47      0.43       600\nweighted avg       0.53      0.54      0.49       600\n\n[17:02:38] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[17:02:38] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[17:02:38] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[17:02:38] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n\n[17:02:39] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[17:02:39] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n[17:02:43] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[17:02:43] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[17:02:45] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[17:02:45] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n[17:02:47] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[17:02:47] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n[17:03:42] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[17:03:43] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[17:04:00] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\nBest params for xgb:\n{'subsample': 0.8, 'objective': 'multi:softmax', 'n_estimators': 150, 'min_child_weight': 1, 'max_depth': 20, 'learning_rate': 1, 'gamma': 0.2, 'colsample_bytree': 0.8, 'booster': 'gblinear'}\nClassification report for xgb:\n              precision    recall  f1-score   support\n\n           0       0.54      0.10      0.16       135\n           1       0.64      0.70      0.67       238\n           2       0.52      0.73      0.61       227\n\n    accuracy                           0.57       600\n   macro avg       0.57      0.51      0.48       600\nweighted avg       0.57      0.57      0.53       600\n\n0:\tlearn: 1.0956953\ttotal: 3.75s\tremaining: 4m 56s\n0:\tlearn: 0.9895611\ttotal: 3.78s\tremaining: 3m 42s\n0:\tlearn: 0.9783194\ttotal: 3.83s\tremaining: 3m 45s\n0:\tlearn: 1.0959013\ttotal: 3.87s\tremaining: 5m 6s\n1:\tlearn: 0.9065064\ttotal: 7.31s\tremaining: 3m 32s\n1:\tlearn: 1.0926639\ttotal: 7.43s\tremaining: 4m 49s\n1:\tlearn: 0.8998818\ttotal: 7.57s\tremaining: 3m 39s\n1:\tlearn: 1.0927685\ttotal: 7.78s\tremaining: 5m 3s\n2:\tlearn: 0.8490080\ttotal: 10.9s\tremaining: 3m 27s\n2:\tlearn: 1.0896756\ttotal: 11.1s\tremaining: 4m 43s\n2:\tlearn: 0.8366367\ttotal: 11.4s\tremaining: 3m 37s\n2:\tlearn: 1.0895092\ttotal: 11.7s\tremaining: 5m\n3:\tlearn: 0.7855352\ttotal: 14.7s\tremaining: 3m 26s\n3:\tlearn: 1.0865178\ttotal: 14.9s\tremaining: 4m 42s\n3:\tlearn: 0.7818456\ttotal: 15.1s\tremaining: 3m 31s\n3:\tlearn: 1.0862678\ttotal: 15.6s\tremaining: 4m 55s\n4:\tlearn: 0.7552633\ttotal: 18.5s\tremaining: 3m 23s\n4:\tlearn: 1.0831718\ttotal: 18.6s\tremaining: 4m 38s\n4:\tlearn: 0.7283906\ttotal: 18.8s\tremaining: 3m 27s\n4:\tlearn: 1.0835715\ttotal: 19.3s\tremaining: 4m 49s\n5:\tlearn: 0.7173614\ttotal: 22.1s\tremaining: 3m 19s\n5:\tlearn: 1.0799908\ttotal: 22.1s\tremaining: 4m 32s\n5:\tlearn: 0.6866560\ttotal: 22.5s\tremaining: 3m 22s\n5:\tlearn: 1.0805823\ttotal: 23s\tremaining: 4m 43s\n6:\tlearn: 0.6634462\ttotal: 26.1s\tremaining: 3m 17s\n6:\tlearn: 1.0769220\ttotal: 26.2s\tremaining: 4m 33s\n6:\tlearn: 0.6509964\ttotal: 26.5s\tremaining: 3m 20s\n6:\tlearn: 1.0774990\ttotal: 26.9s\tremaining: 4m 40s\n7:\tlearn: 0.6474347\ttotal: 29.7s\tremaining: 3m 13s\n7:\tlearn: 1.0738633\ttotal: 29.9s\tremaining: 4m 28s\n7:\tlearn: 0.6098924\ttotal: 30.3s\tremaining: 3m 16s\n7:\tlearn: 1.0745150\ttotal: 30.6s\tremaining: 4m 35s\n8:\tlearn: 0.6198923\ttotal: 33.3s\tremaining: 3m 8s\n8:\tlearn: 1.0708308\ttotal: 33.5s\tremaining: 4m 23s\n8:\tlearn: 0.5782653\ttotal: 33.9s\tremaining: 3m 12s\n8:\tlearn: 1.0720326\ttotal: 34.3s\tremaining: 4m 30s\n9:\tlearn: 0.5742041\ttotal: 36.9s\tremaining: 3m 4s\n9:\tlearn: 1.0685863\ttotal: 37.1s\tremaining: 4m 19s\n9:\tlearn: 0.5390201\ttotal: 37.6s\tremaining: 3m 7s\n9:\tlearn: 1.0692361\ttotal: 38s\tremaining: 4m 25s\n10:\tlearn: 0.5550463\ttotal: 40.5s\tremaining: 3m\n10:\tlearn: 1.0657039\ttotal: 40.8s\tremaining: 4m 15s\n10:\tlearn: 0.5105203\ttotal: 41.2s\tremaining: 3m 3s\n10:\tlearn: 1.0669347\ttotal: 41.6s\tremaining: 4m 20s\n11:\tlearn: 0.5261804\ttotal: 44.1s\tremaining: 2m 56s\n11:\tlearn: 1.0628418\ttotal: 44.4s\tremaining: 4m 11s\n11:\tlearn: 0.4815942\ttotal: 44.8s\tremaining: 2m 59s\n11:\tlearn: 1.0643783\ttotal: 45.2s\tremaining: 4m 16s\n12:\tlearn: 0.4942796\ttotal: 47.7s\tremaining: 2m 52s\n12:\tlearn: 1.0601109\ttotal: 48s\tremaining: 4m 7s\n12:\tlearn: 0.4648674\ttotal: 48.5s\tremaining: 2m 55s\n12:\tlearn: 1.0614527\ttotal: 48.9s\tremaining: 4m 12s\n13:\tlearn: 0.4693092\ttotal: 51.3s\tremaining: 2m 48s\n13:\tlearn: 1.0572242\ttotal: 51.6s\tremaining: 4m 3s\n13:\tlearn: 0.4492336\ttotal: 52.1s\tremaining: 2m 51s\n13:\tlearn: 1.0587750\ttotal: 52.6s\tremaining: 4m 7s\n14:\tlearn: 0.4500595\ttotal: 55.6s\tremaining: 2m 46s\n14:\tlearn: 1.0544611\ttotal: 55.7s\tremaining: 4m 1s\n14:\tlearn: 1.0560618\ttotal: 55.9s\tremaining: 4m 2s\n14:\tlearn: 0.4294134\ttotal: 56.4s\tremaining: 2m 49s\n15:\tlearn: 0.4280944\ttotal: 59.2s\tremaining: 2m 42s\n15:\tlearn: 1.0516716\ttotal: 59.3s\tremaining: 3m 57s\n15:\tlearn: 1.0534135\ttotal: 59.6s\tremaining: 3m 58s\n15:\tlearn: 0.4114649\ttotal: 1m\tremaining: 2m 45s\n16:\tlearn: 0.4042616\ttotal: 1m 2s\tremaining: 2m 39s\n16:\tlearn: 1.0489643\ttotal: 1m 3s\tremaining: 3m 53s\n16:\tlearn: 1.0508397\ttotal: 1m 3s\tremaining: 3m 54s\n16:\tlearn: 0.3883333\ttotal: 1m 3s\tremaining: 2m 40s\n17:\tlearn: 0.3729954\ttotal: 1m 6s\tremaining: 2m 35s\n17:\tlearn: 1.0466917\ttotal: 1m 6s\tremaining: 3m 49s\n17:\tlearn: 1.0481928\ttotal: 1m 6s\tremaining: 3m 50s\n17:\tlearn: 0.3747432\ttotal: 1m 7s\tremaining: 2m 36s\n18:\tlearn: 0.3481898\ttotal: 1m 10s\tremaining: 2m 31s\n18:\tlearn: 1.0440983\ttotal: 1m 10s\tremaining: 3m 45s\n18:\tlearn: 1.0454250\ttotal: 1m 10s\tremaining: 3m 46s\n18:\tlearn: 0.3520237\ttotal: 1m 10s\tremaining: 2m 33s\n19:\tlearn: 0.3242553\ttotal: 1m 13s\tremaining: 2m 27s\n19:\tlearn: 1.0415857\ttotal: 1m 13s\tremaining: 3m 41s\n19:\tlearn: 1.0424665\ttotal: 1m 14s\tremaining: 3m 42s\n19:\tlearn: 0.3369831\ttotal: 1m 14s\tremaining: 2m 29s\n20:\tlearn: 0.3116241\ttotal: 1m 17s\tremaining: 2m 23s\n20:\tlearn: 1.0393014\ttotal: 1m 17s\tremaining: 3m 37s\n20:\tlearn: 1.0404107\ttotal: 1m 17s\tremaining: 3m 38s\n20:\tlearn: 0.3191075\ttotal: 1m 18s\tremaining: 2m 25s\n21:\tlearn: 0.2989209\ttotal: 1m 21s\tremaining: 2m 20s\n21:\tlearn: 1.0369805\ttotal: 1m 21s\tremaining: 3m 33s\n21:\tlearn: 1.0374098\ttotal: 1m 21s\tremaining: 3m 34s\n21:\tlearn: 0.3032893\ttotal: 1m 21s\tremaining: 2m 21s\n22:\tlearn: 0.2797854\ttotal: 1m 24s\tremaining: 2m 16s\n22:\tlearn: 1.0346052\ttotal: 1m 24s\tremaining: 3m 30s\n22:\tlearn: 1.0345926\ttotal: 1m 25s\tremaining: 3m 31s\n22:\tlearn: 0.2900912\ttotal: 1m 25s\tremaining: 2m 17s\n23:\tlearn: 0.2666376\ttotal: 1m 28s\tremaining: 2m 13s\n23:\tlearn: 1.0320839\ttotal: 1m 28s\tremaining: 3m 27s\n23:\tlearn: 1.0321794\ttotal: 1m 29s\tremaining: 3m 28s\n23:\tlearn: 0.2777912\ttotal: 1m 29s\tremaining: 2m 14s\n24:\tlearn: 1.0296282\ttotal: 1m 32s\tremaining: 3m 23s\n24:\tlearn: 0.2566127\ttotal: 1m 32s\tremaining: 2m 9s\n24:\tlearn: 1.0294518\ttotal: 1m 33s\tremaining: 3m 24s\n24:\tlearn: 0.2631826\ttotal: 1m 33s\tremaining: 2m 10s\n25:\tlearn: 1.0274035\ttotal: 1m 36s\tremaining: 3m 19s\n25:\tlearn: 0.2473994\ttotal: 1m 36s\tremaining: 2m 5s\n25:\tlearn: 1.0273283\ttotal: 1m 36s\tremaining: 3m 20s\n25:\tlearn: 0.2524500\ttotal: 1m 37s\tremaining: 2m 7s\n26:\tlearn: 1.0252163\ttotal: 1m 39s\tremaining: 3m 15s\n26:\tlearn: 0.2332180\ttotal: 1m 39s\tremaining: 2m 1s\n26:\tlearn: 1.0247585\ttotal: 1m 40s\tremaining: 3m 17s\n26:\tlearn: 0.2379918\ttotal: 1m 40s\tremaining: 2m 3s\n27:\tlearn: 1.0228465\ttotal: 1m 43s\tremaining: 3m 11s\n27:\tlearn: 0.2172277\ttotal: 1m 43s\tremaining: 1m 58s\n27:\tlearn: 1.0227345\ttotal: 1m 44s\tremaining: 3m 13s\n27:\tlearn: 0.2253041\ttotal: 1m 44s\tremaining: 1m 59s\n28:\tlearn: 1.0203714\ttotal: 1m 46s\tremaining: 3m 8s\n28:\tlearn: 0.2075862\ttotal: 1m 47s\tremaining: 1m 54s\n28:\tlearn: 1.0199257\ttotal: 1m 47s\tremaining: 3m 9s\n28:\tlearn: 0.2105322\ttotal: 1m 48s\tremaining: 1m 55s\n29:\tlearn: 1.0179400\ttotal: 1m 50s\tremaining: 3m 4s\n29:\tlearn: 0.1933853\ttotal: 1m 50s\tremaining: 1m 50s\n29:\tlearn: 1.0180757\ttotal: 1m 51s\tremaining: 3m 5s\n29:\tlearn: 0.2028004\ttotal: 1m 51s\tremaining: 1m 51s\n30:\tlearn: 1.0155182\ttotal: 1m 54s\tremaining: 3m\n30:\tlearn: 0.1844895\ttotal: 1m 54s\tremaining: 1m 47s\n30:\tlearn: 1.0161158\ttotal: 1m 55s\tremaining: 3m 2s\n30:\tlearn: 0.1927566\ttotal: 1m 55s\tremaining: 1m 48s\n31:\tlearn: 1.0133033\ttotal: 1m 58s\tremaining: 2m 57s\n31:\tlearn: 0.1755995\ttotal: 1m 58s\tremaining: 1m 43s\n31:\tlearn: 1.0134306\ttotal: 1m 59s\tremaining: 2m 58s\n31:\tlearn: 0.1838232\ttotal: 1m 59s\tremaining: 1m 44s\n32:\tlearn: 1.0105616\ttotal: 2m 1s\tremaining: 2m 53s\n32:\tlearn: 0.1693689\ttotal: 2m 2s\tremaining: 1m 39s\n32:\tlearn: 1.0108263\ttotal: 2m 2s\tremaining: 2m 54s\n32:\tlearn: 0.1728462\ttotal: 2m 3s\tremaining: 1m 40s\n33:\tlearn: 1.0081063\ttotal: 2m 5s\tremaining: 2m 49s\n33:\tlearn: 0.1639031\ttotal: 2m 5s\tremaining: 1m 36s\n33:\tlearn: 1.0087072\ttotal: 2m 6s\tremaining: 2m 51s\n33:\tlearn: 0.1667799\ttotal: 2m 6s\tremaining: 1m 37s\n34:\tlearn: 1.0053979\ttotal: 2m 9s\tremaining: 2m 46s\n34:\tlearn: 0.1571089\ttotal: 2m 9s\tremaining: 1m 32s\n34:\tlearn: 1.0062714\ttotal: 2m 10s\tremaining: 2m 47s\n34:\tlearn: 0.1592278\ttotal: 2m 10s\tremaining: 1m 33s\n35:\tlearn: 1.0027287\ttotal: 2m 12s\tremaining: 2m 42s\n35:\tlearn: 0.1509399\ttotal: 2m 13s\tremaining: 1m 28s\n35:\tlearn: 1.0040326\ttotal: 2m 13s\tremaining: 2m 43s\n35:\tlearn: 0.1504070\ttotal: 2m 14s\tremaining: 1m 29s\n36:\tlearn: 1.0005286\ttotal: 2m 16s\tremaining: 2m 38s\n36:\tlearn: 0.1468348\ttotal: 2m 16s\tremaining: 1m 25s\n36:\tlearn: 1.0018788\ttotal: 2m 17s\tremaining: 2m 40s\n36:\tlearn: 0.1427774\ttotal: 2m 18s\tremaining: 1m 25s\n37:\tlearn: 0.9981764\ttotal: 2m 20s\tremaining: 2m 34s\n37:\tlearn: 0.1398273\ttotal: 2m 20s\tremaining: 1m 21s\n37:\tlearn: 0.9994613\ttotal: 2m 21s\tremaining: 2m 36s\n37:\tlearn: 0.1387304\ttotal: 2m 21s\tremaining: 1m 22s\n38:\tlearn: 0.9961613\ttotal: 2m 23s\tremaining: 2m 31s\n38:\tlearn: 0.1332321\ttotal: 2m 24s\tremaining: 1m 17s\n38:\tlearn: 0.9974497\ttotal: 2m 25s\tremaining: 2m 32s\n38:\tlearn: 0.1339554\ttotal: 2m 25s\tremaining: 1m 18s\n39:\tlearn: 0.9938061\ttotal: 2m 27s\tremaining: 2m 27s\n39:\tlearn: 0.1268090\ttotal: 2m 27s\tremaining: 1m 13s\n39:\tlearn: 0.9950578\ttotal: 2m 28s\tremaining: 2m 28s\n39:\tlearn: 0.1265183\ttotal: 2m 29s\tremaining: 1m 14s\n40:\tlearn: 0.9916903\ttotal: 2m 31s\tremaining: 2m 24s\n40:\tlearn: 0.1204249\ttotal: 2m 31s\tremaining: 1m 10s\n40:\tlearn: 0.9929908\ttotal: 2m 32s\tremaining: 2m 25s\n40:\tlearn: 0.1221627\ttotal: 2m 33s\tremaining: 1m 10s\n41:\tlearn: 0.9897970\ttotal: 2m 35s\tremaining: 2m 20s\n41:\tlearn: 0.1166100\ttotal: 2m 35s\tremaining: 1m 6s\n41:\tlearn: 0.9911833\ttotal: 2m 36s\tremaining: 2m 21s\n41:\tlearn: 0.1169815\ttotal: 2m 36s\tremaining: 1m 7s\n42:\tlearn: 0.9877368\ttotal: 2m 38s\tremaining: 2m 16s\n42:\tlearn: 0.1132499\ttotal: 2m 39s\tremaining: 1m 3s\n42:\tlearn: 0.9894817\ttotal: 2m 39s\tremaining: 2m 17s\n42:\tlearn: 0.1132084\ttotal: 2m 40s\tremaining: 1m 3s\n43:\tlearn: 0.9854788\ttotal: 2m 42s\tremaining: 2m 12s\n43:\tlearn: 0.1085601\ttotal: 2m 42s\tremaining: 59.3s\n43:\tlearn: 0.9874175\ttotal: 2m 43s\tremaining: 2m 13s\n43:\tlearn: 0.1086281\ttotal: 2m 44s\tremaining: 59.7s\n44:\tlearn: 0.9835737\ttotal: 2m 45s\tremaining: 2m 8s\n44:\tlearn: 0.1050866\ttotal: 2m 46s\tremaining: 55.5s\n44:\tlearn: 0.9852971\ttotal: 2m 47s\tremaining: 2m 9s\n44:\tlearn: 0.1061414\ttotal: 2m 48s\tremaining: 56s\n45:\tlearn: 0.9812106\ttotal: 2m 49s\tremaining: 2m 5s\n45:\tlearn: 0.1009706\ttotal: 2m 50s\tremaining: 51.8s\n45:\tlearn: 0.9834018\ttotal: 2m 50s\tremaining: 2m 6s\n45:\tlearn: 0.1020219\ttotal: 2m 51s\tremaining: 52.3s\n46:\tlearn: 0.9795861\ttotal: 2m 53s\tremaining: 2m 1s\n46:\tlearn: 0.0989181\ttotal: 2m 53s\tremaining: 48.1s\n46:\tlearn: 0.9813145\ttotal: 2m 54s\tremaining: 2m 2s\n46:\tlearn: 0.0992909\ttotal: 2m 55s\tremaining: 48.5s\n47:\tlearn: 0.9775422\ttotal: 2m 56s\tremaining: 1m 57s\n47:\tlearn: 0.0960601\ttotal: 2m 57s\tremaining: 44.4s\n47:\tlearn: 0.9790994\ttotal: 2m 58s\tremaining: 1m 58s\n47:\tlearn: 0.0962630\ttotal: 2m 59s\tremaining: 44.8s\n48:\tlearn: 0.9758068\ttotal: 3m\tremaining: 1m 54s\n48:\tlearn: 0.0922228\ttotal: 3m 1s\tremaining: 40.7s\n48:\tlearn: 0.9770676\ttotal: 3m 2s\tremaining: 1m 55s\n48:\tlearn: 0.0931664\ttotal: 3m 3s\tremaining: 41.1s\n49:\tlearn: 0.9736452\ttotal: 3m 4s\tremaining: 1m 50s\n49:\tlearn: 0.0891735\ttotal: 3m 5s\tremaining: 37s\n49:\tlearn: 0.9751721\ttotal: 3m 6s\tremaining: 1m 51s\n49:\tlearn: 0.0917366\ttotal: 3m 6s\tremaining: 37.4s\n50:\tlearn: 0.9718935\ttotal: 3m 7s\tremaining: 1m 46s\n50:\tlearn: 0.0868803\ttotal: 3m 8s\tremaining: 33.3s\n50:\tlearn: 0.9731761\ttotal: 3m 9s\tremaining: 1m 47s\n50:\tlearn: 0.0896573\ttotal: 3m 10s\tremaining: 33.6s\n51:\tlearn: 0.9698922\ttotal: 3m 11s\tremaining: 1m 43s\n51:\tlearn: 0.0839318\ttotal: 3m 12s\tremaining: 29.6s\n51:\tlearn: 0.9711535\ttotal: 3m 13s\tremaining: 1m 44s\n51:\tlearn: 0.0859094\ttotal: 3m 14s\tremaining: 29.9s\n52:\tlearn: 0.9677774\ttotal: 3m 15s\tremaining: 1m 39s\n52:\tlearn: 0.0822815\ttotal: 3m 16s\tremaining: 25.9s\n52:\tlearn: 0.9692458\ttotal: 3m 17s\tremaining: 1m 40s\n52:\tlearn: 0.0828712\ttotal: 3m 18s\tremaining: 26.2s\n53:\tlearn: 0.9658182\ttotal: 3m 19s\tremaining: 1m 35s\n53:\tlearn: 0.0795149\ttotal: 3m 20s\tremaining: 22.3s\n53:\tlearn: 0.9674023\ttotal: 3m 21s\tremaining: 1m 36s\n53:\tlearn: 0.0808149\ttotal: 3m 22s\tremaining: 22.5s\n54:\tlearn: 0.9636770\ttotal: 3m 22s\tremaining: 1m 32s\n54:\tlearn: 0.0772756\ttotal: 3m 24s\tremaining: 18.5s\n54:\tlearn: 0.9652168\ttotal: 3m 24s\tremaining: 1m 33s\n54:\tlearn: 0.0773648\ttotal: 3m 25s\tremaining: 18.7s\n55:\tlearn: 0.9616822\ttotal: 3m 26s\tremaining: 1m 28s\n55:\tlearn: 0.0745258\ttotal: 3m 27s\tremaining: 14.8s\n55:\tlearn: 0.9631674\ttotal: 3m 28s\tremaining: 1m 29s\n55:\tlearn: 0.0748709\ttotal: 3m 29s\tremaining: 15s\n56:\tlearn: 0.9597435\ttotal: 3m 30s\tremaining: 1m 24s\n56:\tlearn: 0.0729312\ttotal: 3m 31s\tremaining: 11.1s\n56:\tlearn: 0.9612329\ttotal: 3m 32s\tremaining: 1m 25s\n56:\tlearn: 0.0723131\ttotal: 3m 33s\tremaining: 11.3s\n57:\tlearn: 0.9580899\ttotal: 3m 34s\tremaining: 1m 21s\n57:\tlearn: 0.0709154\ttotal: 3m 35s\tremaining: 7.42s\n57:\tlearn: 0.9595476\ttotal: 3m 35s\tremaining: 1m 21s\n57:\tlearn: 0.0702469\ttotal: 3m 37s\tremaining: 7.5s\n58:\tlearn: 0.9563333\ttotal: 3m 37s\tremaining: 1m 17s\n58:\tlearn: 0.0689582\ttotal: 3m 38s\tremaining: 3.71s\n58:\tlearn: 0.9577750\ttotal: 3m 39s\tremaining: 1m 18s\n58:\tlearn: 0.0689603\ttotal: 3m 41s\tremaining: 3.75s\n59:\tlearn: 0.9545347\ttotal: 3m 41s\tremaining: 1m 13s\n59:\tlearn: 0.0673317\ttotal: 3m 42s\tremaining: 0us\n59:\tlearn: 0.9560751\ttotal: 3m 43s\tremaining: 1m 14s\n59:\tlearn: 0.0670605\ttotal: 3m 44s\tremaining: 0us\n60:\tlearn: 0.9525741\ttotal: 3m 44s\tremaining: 1m 10s\n60:\tlearn: 0.9542795\ttotal: 3m 46s\tremaining: 1m 10s\n0:\tlearn: 1.0974311\ttotal: 3.67s\tremaining: 7m 52s\n61:\tlearn: 0.9510266\ttotal: 3m 48s\tremaining: 1m 6s\n0:\tlearn: 1.0975724\ttotal: 3.71s\tremaining: 7m 58s\n61:\tlearn: 0.9526081\ttotal: 3m 50s\tremaining: 1m 6s\n1:\tlearn: 1.0958105\ttotal: 7.33s\tremaining: 7m 49s\n62:\tlearn: 0.9490413\ttotal: 3m 51s\tremaining: 1m 2s\n1:\tlearn: 1.0962660\ttotal: 7.36s\tremaining: 7m 50s\n62:\tlearn: 0.9510893\ttotal: 3m 53s\tremaining: 1m 3s\n2:\tlearn: 1.0945009\ttotal: 11s\tremaining: 7m 47s\n63:\tlearn: 0.9469418\ttotal: 3m 55s\tremaining: 58.9s\n2:\tlearn: 1.0948790\ttotal: 11s\tremaining: 7m 45s\n63:\tlearn: 0.9489058\ttotal: 3m 57s\tremaining: 59.4s\n3:\tlearn: 1.0929715\ttotal: 14.7s\tremaining: 7m 43s\n64:\tlearn: 0.9451299\ttotal: 3m 59s\tremaining: 55.2s\n3:\tlearn: 1.0934578\ttotal: 14.7s\tremaining: 7m 42s\n64:\tlearn: 0.9468679\ttotal: 4m 1s\tremaining: 55.7s\n4:\tlearn: 1.0915157\ttotal: 18.3s\tremaining: 7m 38s\n65:\tlearn: 0.9432004\ttotal: 4m 2s\tremaining: 51.5s\n4:\tlearn: 1.0920713\ttotal: 18.4s\tremaining: 7m 41s\n65:\tlearn: 0.9452409\ttotal: 4m 5s\tremaining: 52s\n5:\tlearn: 1.0902692\ttotal: 22.3s\tremaining: 7m 40s\n66:\tlearn: 0.9415434\ttotal: 4m 7s\tremaining: 47.9s\n5:\tlearn: 1.0907380\ttotal: 22.5s\tremaining: 7m 45s\n66:\tlearn: 0.9434555\ttotal: 4m 9s\tremaining: 48.3s\n6:\tlearn: 1.0888865\ttotal: 26s\tremaining: 7m 36s\n67:\tlearn: 0.9396524\ttotal: 4m 10s\tremaining: 44.2s\n6:\tlearn: 1.0894688\ttotal: 26.2s\tremaining: 7m 40s\n67:\tlearn: 0.9415025\ttotal: 4m 12s\tremaining: 44.6s\n7:\tlearn: 1.0875014\ttotal: 29.7s\tremaining: 7m 32s\n68:\tlearn: 0.9378207\ttotal: 4m 14s\tremaining: 40.6s\n7:\tlearn: 1.0880939\ttotal: 30s\tremaining: 7m 38s\n68:\tlearn: 0.9394080\ttotal: 4m 16s\tremaining: 40.9s\n8:\tlearn: 1.0862646\ttotal: 33.5s\tremaining: 7m 29s\n69:\tlearn: 0.9359176\ttotal: 4m 18s\tremaining: 36.9s\n8:\tlearn: 1.0869065\ttotal: 33.7s\tremaining: 7m 33s\n69:\tlearn: 0.9376781\ttotal: 4m 20s\tremaining: 37.2s\n9:\tlearn: 1.0851192\ttotal: 37.1s\tremaining: 7m 24s\n70:\tlearn: 0.9346019\ttotal: 4m 21s\tremaining: 33.2s\n9:\tlearn: 1.0855517\ttotal: 37.4s\tremaining: 7m 28s\n70:\tlearn: 0.9361077\ttotal: 4m 23s\tremaining: 33.5s\n10:\tlearn: 1.0840145\ttotal: 40.7s\tremaining: 7m 20s\n71:\tlearn: 0.9332136\ttotal: 4m 25s\tremaining: 29.5s\n10:\tlearn: 1.0844070\ttotal: 41s\tremaining: 7m 23s\n71:\tlearn: 0.9343493\ttotal: 4m 27s\tremaining: 29.7s\n11:\tlearn: 1.0825719\ttotal: 44.5s\tremaining: 7m 17s\n72:\tlearn: 0.9313372\ttotal: 4m 28s\tremaining: 25.8s\n11:\tlearn: 1.0830935\ttotal: 44.6s\tremaining: 7m 18s\n72:\tlearn: 0.9325438\ttotal: 4m 31s\tremaining: 26s\n12:\tlearn: 1.0813530\ttotal: 48.1s\tremaining: 7m 13s\n73:\tlearn: 0.9289839\ttotal: 4m 32s\tremaining: 22.1s\n12:\tlearn: 1.0817042\ttotal: 48.2s\tremaining: 7m 14s\n73:\tlearn: 0.9308147\ttotal: 4m 35s\tremaining: 22.3s\n13:\tlearn: 1.0801366\ttotal: 51.8s\tremaining: 7m 9s\n74:\tlearn: 0.9270387\ttotal: 4m 36s\tremaining: 18.4s\n13:\tlearn: 1.0806018\ttotal: 52.3s\tremaining: 7m 13s\n14:\tlearn: 1.0790422\ttotal: 55.5s\tremaining: 7m 5s\n74:\tlearn: 0.9288171\ttotal: 4m 39s\tremaining: 18.6s\n75:\tlearn: 0.9250439\ttotal: 4m 39s\tremaining: 14.7s\n14:\tlearn: 1.0792949\ttotal: 56.1s\tremaining: 7m 9s\n15:\tlearn: 1.0777307\ttotal: 59.1s\tremaining: 7m 1s\n75:\tlearn: 0.9267261\ttotal: 4m 42s\tremaining: 14.9s\n76:\tlearn: 0.9234929\ttotal: 4m 43s\tremaining: 11s\n15:\tlearn: 1.0781819\ttotal: 59.7s\tremaining: 7m 5s\n16:\tlearn: 1.0766019\ttotal: 1m 2s\tremaining: 6m 57s\n76:\tlearn: 0.9248222\ttotal: 4m 46s\tremaining: 11.2s\n77:\tlearn: 0.9218207\ttotal: 4m 47s\tremaining: 7.36s\n16:\tlearn: 1.0769484\ttotal: 1m 3s\tremaining: 7m 1s\n17:\tlearn: 1.0754508\ttotal: 1m 6s\tremaining: 6m 53s\n77:\tlearn: 0.9234857\ttotal: 4m 50s\tremaining: 7.45s\n78:\tlearn: 0.9199280\ttotal: 4m 50s\tremaining: 3.68s\n17:\tlearn: 1.0759630\ttotal: 1m 7s\tremaining: 6m 57s\n18:\tlearn: 1.0741821\ttotal: 1m 10s\tremaining: 6m 49s\n78:\tlearn: 0.9220424\ttotal: 4m 54s\tremaining: 3.72s\n79:\tlearn: 0.9184022\ttotal: 4m 54s\tremaining: 0us\n18:\tlearn: 1.0747036\ttotal: 1m 10s\tremaining: 6m 51s\n19:\tlearn: 1.0730168\ttotal: 1m 13s\tremaining: 6m 41s\n79:\tlearn: 0.9205342\ttotal: 4m 56s\tremaining: 0us\n19:\tlearn: 1.0734427\ttotal: 1m 12s\tremaining: 6m 40s\n20:\tlearn: 1.0718811\ttotal: 1m 14s\tremaining: 6m 28s\n20:\tlearn: 1.0722235\ttotal: 1m 14s\tremaining: 6m 26s\n21:\tlearn: 1.0706326\ttotal: 1m 16s\tremaining: 6m 17s\n21:\tlearn: 1.0709161\ttotal: 1m 16s\tremaining: 6m 14s\n22:\tlearn: 1.0694471\ttotal: 1m 18s\tremaining: 6m 5s\n22:\tlearn: 1.0697012\ttotal: 1m 18s\tremaining: 6m 3s\n23:\tlearn: 1.0682373\ttotal: 1m 20s\tremaining: 5m 55s\n23:\tlearn: 1.0685914\ttotal: 1m 19s\tremaining: 5m 52s\n24:\tlearn: 1.0669909\ttotal: 1m 22s\tremaining: 5m 45s\n24:\tlearn: 1.0674856\ttotal: 1m 21s\tremaining: 5m 43s\n25:\tlearn: 1.0657314\ttotal: 1m 24s\tremaining: 5m 37s\n25:\tlearn: 1.0661252\ttotal: 1m 23s\tremaining: 5m 35s\n26:\tlearn: 1.0646520\ttotal: 1m 26s\tremaining: 5m 28s\n26:\tlearn: 1.0651375\ttotal: 1m 25s\tremaining: 5m 27s\n27:\tlearn: 1.0635846\ttotal: 1m 27s\tremaining: 5m 20s\n27:\tlearn: 1.0642466\ttotal: 1m 27s\tremaining: 5m 19s\n28:\tlearn: 1.0624977\ttotal: 1m 29s\tremaining: 5m 12s\n28:\tlearn: 1.0631943\ttotal: 1m 29s\tremaining: 5m 11s\n29:\tlearn: 1.0615486\ttotal: 1m 31s\tremaining: 5m 5s\n29:\tlearn: 1.0622094\ttotal: 1m 31s\tremaining: 5m 4s\n30:\tlearn: 1.0605104\ttotal: 1m 33s\tremaining: 4m 58s\n30:\tlearn: 1.0610892\ttotal: 1m 33s\tremaining: 4m 57s\n31:\tlearn: 1.0593864\ttotal: 1m 35s\tremaining: 4m 52s\n31:\tlearn: 1.0600509\ttotal: 1m 34s\tremaining: 4m 50s\n32:\tlearn: 1.0580933\ttotal: 1m 37s\tremaining: 4m 45s\n32:\tlearn: 1.0589035\ttotal: 1m 36s\tremaining: 4m 44s\n33:\tlearn: 1.0569875\ttotal: 1m 39s\tremaining: 4m 39s\n33:\tlearn: 1.0578332\ttotal: 1m 38s\tremaining: 4m 37s\n34:\tlearn: 1.0558838\ttotal: 1m 40s\tremaining: 4m 33s\n34:\tlearn: 1.0567923\ttotal: 1m 40s\tremaining: 4m 31s\n35:\tlearn: 1.0549376\ttotal: 1m 42s\tremaining: 4m 28s\n35:\tlearn: 1.0558720\ttotal: 1m 41s\tremaining: 4m 26s\n36:\tlearn: 1.0539130\ttotal: 1m 44s\tremaining: 4m 22s\n36:\tlearn: 1.0549176\ttotal: 1m 43s\tremaining: 4m 20s\n37:\tlearn: 1.0528616\ttotal: 1m 46s\tremaining: 4m 17s\n37:\tlearn: 1.0537495\ttotal: 1m 45s\tremaining: 4m 15s\n38:\tlearn: 1.0516118\ttotal: 1m 48s\tremaining: 4m 12s\n38:\tlearn: 1.0527151\ttotal: 1m 47s\tremaining: 4m 10s\n39:\tlearn: 1.0506017\ttotal: 1m 49s\tremaining: 4m 7s\n39:\tlearn: 1.0515087\ttotal: 1m 49s\tremaining: 4m 5s\n40:\tlearn: 1.0496560\ttotal: 1m 51s\tremaining: 4m 2s\n40:\tlearn: 1.0504653\ttotal: 1m 51s\tremaining: 4m 1s\n41:\tlearn: 1.0484791\ttotal: 1m 53s\tremaining: 3m 57s\n41:\tlearn: 1.0494193\ttotal: 1m 52s\tremaining: 3m 56s\n42:\tlearn: 1.0475038\ttotal: 1m 55s\tremaining: 3m 53s\n42:\tlearn: 1.0483732\ttotal: 1m 55s\tremaining: 3m 53s\n43:\tlearn: 1.0462347\ttotal: 1m 57s\tremaining: 3m 49s\n43:\tlearn: 1.0472903\ttotal: 1m 57s\tremaining: 3m 48s\n44:\tlearn: 1.0453961\ttotal: 1m 59s\tremaining: 3m 44s\n44:\tlearn: 1.0462734\ttotal: 1m 58s\tremaining: 3m 44s\n45:\tlearn: 1.0443652\ttotal: 2m\tremaining: 3m 40s\n45:\tlearn: 1.0454134\ttotal: 2m\tremaining: 3m 40s\n46:\tlearn: 1.0434183\ttotal: 2m 2s\tremaining: 3m 36s\n46:\tlearn: 1.0443830\ttotal: 2m 2s\tremaining: 3m 36s\n47:\tlearn: 1.0424134\ttotal: 2m 4s\tremaining: 3m 32s\n47:\tlearn: 1.0433077\ttotal: 2m 4s\tremaining: 3m 32s\n48:\tlearn: 1.0415144\ttotal: 2m 6s\tremaining: 3m 28s\n48:\tlearn: 1.0424520\ttotal: 2m 6s\tremaining: 3m 28s\n49:\tlearn: 1.0401165\ttotal: 2m 7s\tremaining: 3m 24s\n49:\tlearn: 1.0414619\ttotal: 2m 8s\tremaining: 3m 24s\n50:\tlearn: 1.0392324\ttotal: 2m 9s\tremaining: 3m 21s\n50:\tlearn: 1.0404820\ttotal: 2m 9s\tremaining: 3m 21s\n51:\tlearn: 1.0381029\ttotal: 2m 11s\tremaining: 3m 17s\n51:\tlearn: 1.0394244\ttotal: 2m 11s\tremaining: 3m 17s\n52:\tlearn: 1.0371210\ttotal: 2m 13s\tremaining: 3m 13s\n52:\tlearn: 1.0385064\ttotal: 2m 13s\tremaining: 3m 14s\n53:\tlearn: 1.0361771\ttotal: 2m 15s\tremaining: 3m 10s\n53:\tlearn: 1.0376327\ttotal: 2m 15s\tremaining: 3m 10s\n54:\tlearn: 1.0352008\ttotal: 2m 16s\tremaining: 3m 6s\n54:\tlearn: 1.0367067\ttotal: 2m 17s\tremaining: 3m 7s\n55:\tlearn: 1.0342552\ttotal: 2m 18s\tremaining: 3m 3s\n55:\tlearn: 1.0357386\ttotal: 2m 18s\tremaining: 3m 3s\n56:\tlearn: 1.0332855\ttotal: 2m 20s\tremaining: 3m\n56:\tlearn: 1.0348447\ttotal: 2m 20s\tremaining: 3m\n57:\tlearn: 1.0322980\ttotal: 2m 22s\tremaining: 2m 56s\n57:\tlearn: 1.0338957\ttotal: 2m 22s\tremaining: 2m 56s\n58:\tlearn: 1.0312701\ttotal: 2m 24s\tremaining: 2m 53s\n58:\tlearn: 1.0329345\ttotal: 2m 24s\tremaining: 2m 53s\n59:\tlearn: 1.0301769\ttotal: 2m 26s\tremaining: 2m 50s\n59:\tlearn: 1.0320008\ttotal: 2m 26s\tremaining: 2m 50s\n60:\tlearn: 1.0292371\ttotal: 2m 28s\tremaining: 2m 47s\n60:\tlearn: 1.0309749\ttotal: 2m 28s\tremaining: 2m 47s\n61:\tlearn: 1.0283803\ttotal: 2m 30s\tremaining: 2m 44s\n61:\tlearn: 1.0300715\ttotal: 2m 30s\tremaining: 2m 44s\n62:\tlearn: 1.0273080\ttotal: 2m 32s\tremaining: 2m 41s\n62:\tlearn: 1.0291679\ttotal: 2m 31s\tremaining: 2m 41s\n63:\tlearn: 1.0263585\ttotal: 2m 33s\tremaining: 2m 38s\n63:\tlearn: 1.0282149\ttotal: 2m 33s\tremaining: 2m 38s\n64:\tlearn: 1.0254250\ttotal: 2m 35s\tremaining: 2m 35s\n64:\tlearn: 1.0271990\ttotal: 2m 35s\tremaining: 2m 35s\n65:\tlearn: 1.0243304\ttotal: 2m 37s\tremaining: 2m 32s\n65:\tlearn: 1.0263356\ttotal: 2m 37s\tremaining: 2m 32s\n66:\tlearn: 1.0235017\ttotal: 2m 39s\tremaining: 2m 29s\n66:\tlearn: 1.0254396\ttotal: 2m 39s\tremaining: 2m 29s\n67:\tlearn: 1.0225523\ttotal: 2m 41s\tremaining: 2m 26s\n67:\tlearn: 1.0244615\ttotal: 2m 40s\tremaining: 2m 26s\n68:\tlearn: 1.0215342\ttotal: 2m 42s\tremaining: 2m 24s\n68:\tlearn: 1.0235278\ttotal: 2m 42s\tremaining: 2m 23s\n69:\tlearn: 1.0206861\ttotal: 2m 44s\tremaining: 2m 21s\n69:\tlearn: 1.0226083\ttotal: 2m 44s\tremaining: 2m 21s\n70:\tlearn: 1.0198009\ttotal: 2m 46s\tremaining: 2m 18s\n70:\tlearn: 1.0217832\ttotal: 2m 46s\tremaining: 2m 18s\n71:\tlearn: 1.0190086\ttotal: 2m 48s\tremaining: 2m 15s\n71:\tlearn: 1.0209750\ttotal: 2m 48s\tremaining: 2m 15s\n72:\tlearn: 1.0181855\ttotal: 2m 50s\tremaining: 2m 12s\n72:\tlearn: 1.0200756\ttotal: 2m 50s\tremaining: 2m 12s\n73:\tlearn: 1.0172538\ttotal: 2m 51s\tremaining: 2m 10s\n73:\tlearn: 1.0190668\ttotal: 2m 51s\tremaining: 2m 10s\n74:\tlearn: 1.0163471\ttotal: 2m 53s\tremaining: 2m 7s\n74:\tlearn: 1.0180738\ttotal: 2m 53s\tremaining: 2m 7s\n75:\tlearn: 1.0154216\ttotal: 2m 55s\tremaining: 2m 4s\n75:\tlearn: 1.0171294\ttotal: 2m 55s\tremaining: 2m 4s\n76:\tlearn: 1.0145755\ttotal: 2m 57s\tremaining: 2m 2s\n76:\tlearn: 1.0162431\ttotal: 2m 57s\tremaining: 2m 2s\n77:\tlearn: 1.0138062\ttotal: 2m 59s\tremaining: 1m 59s\n77:\tlearn: 1.0152686\ttotal: 2m 59s\tremaining: 1m 59s\n78:\tlearn: 1.0130346\ttotal: 3m 1s\tremaining: 1m 57s\n78:\tlearn: 1.0143447\ttotal: 3m 1s\tremaining: 1m 56s\n79:\tlearn: 1.0120644\ttotal: 3m 3s\tremaining: 1m 54s\n79:\tlearn: 1.0135920\ttotal: 3m 2s\tremaining: 1m 54s\n80:\tlearn: 1.0112135\ttotal: 3m 5s\tremaining: 1m 51s\n80:\tlearn: 1.0126574\ttotal: 3m 4s\tremaining: 1m 51s\n81:\tlearn: 1.0102317\ttotal: 3m 6s\tremaining: 1m 49s\n81:\tlearn: 1.0117758\ttotal: 3m 6s\tremaining: 1m 49s\n82:\tlearn: 1.0094149\ttotal: 3m 8s\tremaining: 1m 46s\n82:\tlearn: 1.0107263\ttotal: 3m 8s\tremaining: 1m 46s\n83:\tlearn: 1.0085378\ttotal: 3m 10s\tremaining: 1m 44s\n83:\tlearn: 1.0098409\ttotal: 3m 10s\tremaining: 1m 44s\n84:\tlearn: 1.0076557\ttotal: 3m 12s\tremaining: 1m 41s\n84:\tlearn: 1.0090135\ttotal: 3m 12s\tremaining: 1m 41s\n85:\tlearn: 1.0068802\ttotal: 3m 14s\tremaining: 1m 39s\n85:\tlearn: 1.0081596\ttotal: 3m 13s\tremaining: 1m 39s\n86:\tlearn: 1.0060561\ttotal: 3m 15s\tremaining: 1m 36s\n86:\tlearn: 1.0073921\ttotal: 3m 15s\tremaining: 1m 36s\n87:\tlearn: 1.0051681\ttotal: 3m 17s\tremaining: 1m 34s\n87:\tlearn: 1.0065367\ttotal: 3m 17s\tremaining: 1m 34s\n88:\tlearn: 1.0043535\ttotal: 3m 19s\tremaining: 1m 31s\n88:\tlearn: 1.0058076\ttotal: 3m 19s\tremaining: 1m 31s\n89:\tlearn: 1.0035390\ttotal: 3m 21s\tremaining: 1m 29s\n89:\tlearn: 1.0049757\ttotal: 3m 21s\tremaining: 1m 29s\n90:\tlearn: 1.0025708\ttotal: 3m 23s\tremaining: 1m 27s\n90:\tlearn: 1.0042155\ttotal: 3m 22s\tremaining: 1m 26s\n91:\tlearn: 1.0017168\ttotal: 3m 24s\tremaining: 1m 24s\n91:\tlearn: 1.0033037\ttotal: 3m 24s\tremaining: 1m 24s\n92:\tlearn: 1.0007471\ttotal: 3m 26s\tremaining: 1m 22s\n92:\tlearn: 1.0023946\ttotal: 3m 26s\tremaining: 1m 22s\n93:\tlearn: 0.9999397\ttotal: 3m 28s\tremaining: 1m 19s\n93:\tlearn: 1.0015735\ttotal: 3m 28s\tremaining: 1m 20s\n94:\tlearn: 0.9990680\ttotal: 3m 30s\tremaining: 1m 17s\n94:\tlearn: 1.0007677\ttotal: 3m 30s\tremaining: 1m 17s\n95:\tlearn: 0.9982965\ttotal: 3m 32s\tremaining: 1m 15s\n95:\tlearn: 0.9999548\ttotal: 3m 32s\tremaining: 1m 15s\n96:\tlearn: 0.9975207\ttotal: 3m 34s\tremaining: 1m 12s\n96:\tlearn: 0.9991360\ttotal: 3m 34s\tremaining: 1m 12s\n97:\tlearn: 0.9966610\ttotal: 3m 35s\tremaining: 1m 10s\n97:\tlearn: 0.9983836\ttotal: 3m 36s\tremaining: 1m 10s\n98:\tlearn: 0.9958996\ttotal: 3m 37s\tremaining: 1m 8s\n98:\tlearn: 0.9975042\ttotal: 3m 37s\tremaining: 1m 8s\n99:\tlearn: 0.9949960\ttotal: 3m 39s\tremaining: 1m 5s\n99:\tlearn: 0.9966459\ttotal: 3m 39s\tremaining: 1m 5s\n100:\tlearn: 0.9942894\ttotal: 3m 41s\tremaining: 1m 3s\n100:\tlearn: 0.9958771\ttotal: 3m 41s\tremaining: 1m 3s\n101:\tlearn: 0.9936188\ttotal: 3m 43s\tremaining: 1m 1s\n101:\tlearn: 0.9952079\ttotal: 3m 43s\tremaining: 1m 1s\n102:\tlearn: 0.9927908\ttotal: 3m 44s\tremaining: 59s\n102:\tlearn: 0.9942532\ttotal: 3m 45s\tremaining: 59s\n103:\tlearn: 0.9920507\ttotal: 3m 46s\tremaining: 56.7s\n103:\tlearn: 0.9934216\ttotal: 3m 46s\tremaining: 56.7s\n104:\tlearn: 0.9913823\ttotal: 3m 48s\tremaining: 54.4s\n104:\tlearn: 0.9925645\ttotal: 3m 48s\tremaining: 54.5s\n105:\tlearn: 0.9905700\ttotal: 3m 50s\tremaining: 52.2s\n105:\tlearn: 0.9917087\ttotal: 3m 50s\tremaining: 52.2s\n106:\tlearn: 0.9898607\ttotal: 3m 52s\tremaining: 49.9s\n106:\tlearn: 0.9910074\ttotal: 3m 52s\tremaining: 50s\n107:\tlearn: 0.9889169\ttotal: 3m 54s\tremaining: 47.7s\n107:\tlearn: 0.9903262\ttotal: 3m 54s\tremaining: 47.7s\n108:\tlearn: 0.9881622\ttotal: 3m 55s\tremaining: 45.5s\n108:\tlearn: 0.9894423\ttotal: 3m 56s\tremaining: 45.5s\n109:\tlearn: 0.9873107\ttotal: 3m 57s\tremaining: 43.2s\n109:\tlearn: 0.9887257\ttotal: 3m 57s\tremaining: 43.3s\n110:\tlearn: 0.9864883\ttotal: 3m 59s\tremaining: 41s\n110:\tlearn: 0.9880191\ttotal: 3m 59s\tremaining: 41.1s\n111:\tlearn: 0.9857036\ttotal: 4m 1s\tremaining: 38.8s\n111:\tlearn: 0.9871932\ttotal: 4m 1s\tremaining: 38.9s\n112:\tlearn: 0.9850274\ttotal: 4m 3s\tremaining: 36.6s\n112:\tlearn: 0.9864621\ttotal: 4m 3s\tremaining: 36.7s\n113:\tlearn: 0.9842200\ttotal: 4m 5s\tremaining: 34.4s\n113:\tlearn: 0.9857991\ttotal: 4m 5s\tremaining: 34.4s\n114:\tlearn: 0.9833041\ttotal: 4m 7s\tremaining: 32.2s\n114:\tlearn: 0.9850419\ttotal: 4m 7s\tremaining: 32.3s\n115:\tlearn: 0.9824977\ttotal: 4m 8s\tremaining: 30s\n115:\tlearn: 0.9841753\ttotal: 4m 9s\tremaining: 30.1s\n116:\tlearn: 0.9817103\ttotal: 4m 10s\tremaining: 27.9s\n116:\tlearn: 0.9834100\ttotal: 4m 10s\tremaining: 27.9s\n117:\tlearn: 0.9810538\ttotal: 4m 12s\tremaining: 25.7s\n117:\tlearn: 0.9826599\ttotal: 4m 12s\tremaining: 25.7s\n118:\tlearn: 0.9803012\ttotal: 4m 14s\tremaining: 23.5s\n118:\tlearn: 0.9818611\ttotal: 4m 14s\tremaining: 23.5s\n119:\tlearn: 0.9795641\ttotal: 4m 16s\tremaining: 21.3s\n119:\tlearn: 0.9810068\ttotal: 4m 16s\tremaining: 21.4s\n120:\tlearn: 0.9787827\ttotal: 4m 17s\tremaining: 19.2s\n120:\tlearn: 0.9803052\ttotal: 4m 18s\tremaining: 19.2s\n121:\tlearn: 0.9778889\ttotal: 4m 19s\tremaining: 17s\n121:\tlearn: 0.9797002\ttotal: 4m 20s\tremaining: 17.1s\n122:\tlearn: 0.9771373\ttotal: 4m 21s\tremaining: 14.9s\n122:\tlearn: 0.9788659\ttotal: 4m 22s\tremaining: 14.9s\n123:\tlearn: 0.9763933\ttotal: 4m 23s\tremaining: 12.8s\n124:\tlearn: 0.9755438\ttotal: 4m 25s\tremaining: 10.6s\n123:\tlearn: 0.9781489\ttotal: 4m 23s\tremaining: 12.8s\n125:\tlearn: 0.9748328\ttotal: 4m 27s\tremaining: 8.48s\n124:\tlearn: 0.9774135\ttotal: 4m 25s\tremaining: 10.6s\n126:\tlearn: 0.9742276\ttotal: 4m 28s\tremaining: 6.35s\n125:\tlearn: 0.9768836\ttotal: 4m 27s\tremaining: 8.49s\n127:\tlearn: 0.9734599\ttotal: 4m 30s\tremaining: 4.23s\n126:\tlearn: 0.9760943\ttotal: 4m 29s\tremaining: 6.37s\n128:\tlearn: 0.9728124\ttotal: 4m 32s\tremaining: 2.12s\n127:\tlearn: 0.9753120\ttotal: 4m 31s\tremaining: 4.24s\n129:\tlearn: 0.9721176\ttotal: 4m 34s\tremaining: 0us\n128:\tlearn: 0.9745829\ttotal: 4m 33s\tremaining: 2.12s\n129:\tlearn: 0.9738295\ttotal: 4m 34s\tremaining: 0us\n0:\tlearn: 1.0972255\ttotal: 927ms\tremaining: 1m 59s\n1:\tlearn: 1.0956352\ttotal: 1.8s\tremaining: 1m 55s\n2:\tlearn: 1.0940008\ttotal: 2.67s\tremaining: 1m 52s\n3:\tlearn: 1.0925018\ttotal: 3.54s\tremaining: 1m 51s\n4:\tlearn: 1.0909522\ttotal: 4.41s\tremaining: 1m 50s\n5:\tlearn: 1.0895317\ttotal: 5.29s\tremaining: 1m 49s\n6:\tlearn: 1.0883678\ttotal: 6.16s\tremaining: 1m 48s\n7:\tlearn: 1.0867854\ttotal: 7.04s\tremaining: 1m 47s\n8:\tlearn: 1.0854488\ttotal: 7.93s\tremaining: 1m 46s\n9:\tlearn: 1.0839933\ttotal: 8.83s\tremaining: 1m 46s\n10:\tlearn: 1.0828467\ttotal: 9.69s\tremaining: 1m 44s\n11:\tlearn: 1.0813899\ttotal: 10.6s\tremaining: 1m 43s\n12:\tlearn: 1.0799136\ttotal: 11.4s\tremaining: 1m 42s\n13:\tlearn: 1.0787595\ttotal: 12.3s\tremaining: 1m 41s\n14:\tlearn: 1.0774972\ttotal: 13.2s\tremaining: 1m 41s\n15:\tlearn: 1.0761554\ttotal: 14.1s\tremaining: 1m 40s\n16:\tlearn: 1.0749423\ttotal: 14.9s\tremaining: 1m 39s\n17:\tlearn: 1.0737884\ttotal: 15.8s\tremaining: 1m 38s\n18:\tlearn: 1.0724153\ttotal: 16.7s\tremaining: 1m 37s\n19:\tlearn: 1.0712479\ttotal: 17.6s\tremaining: 1m 36s\n20:\tlearn: 1.0700434\ttotal: 18.5s\tremaining: 1m 35s\n21:\tlearn: 1.0687789\ttotal: 19.4s\tremaining: 1m 35s\n22:\tlearn: 1.0675953\ttotal: 20.3s\tremaining: 1m 34s\n23:\tlearn: 1.0662528\ttotal: 21.1s\tremaining: 1m 33s\n24:\tlearn: 1.0649892\ttotal: 22s\tremaining: 1m 32s\n25:\tlearn: 1.0637361\ttotal: 22.9s\tremaining: 1m 31s\n26:\tlearn: 1.0625885\ttotal: 23.8s\tremaining: 1m 30s\n27:\tlearn: 1.0614792\ttotal: 24.7s\tremaining: 1m 29s\n28:\tlearn: 1.0603431\ttotal: 25.5s\tremaining: 1m 28s\n29:\tlearn: 1.0591214\ttotal: 26.4s\tremaining: 1m 28s\n30:\tlearn: 1.0581059\ttotal: 27.3s\tremaining: 1m 27s\n31:\tlearn: 1.0568417\ttotal: 28.5s\tremaining: 1m 27s\n32:\tlearn: 1.0554323\ttotal: 29.4s\tremaining: 1m 26s\n33:\tlearn: 1.0541831\ttotal: 30.3s\tremaining: 1m 25s\n34:\tlearn: 1.0531123\ttotal: 31.2s\tremaining: 1m 24s\n35:\tlearn: 1.0519022\ttotal: 32.1s\tremaining: 1m 23s\n36:\tlearn: 1.0506010\ttotal: 32.9s\tremaining: 1m 22s\n37:\tlearn: 1.0496294\ttotal: 33.8s\tremaining: 1m 21s\n38:\tlearn: 1.0484283\ttotal: 34.7s\tremaining: 1m 21s\n39:\tlearn: 1.0472977\ttotal: 35.6s\tremaining: 1m 20s\n40:\tlearn: 1.0462698\ttotal: 36.5s\tremaining: 1m 19s\n41:\tlearn: 1.0452254\ttotal: 37.4s\tremaining: 1m 18s\n42:\tlearn: 1.0441787\ttotal: 38.2s\tremaining: 1m 17s\n43:\tlearn: 1.0430770\ttotal: 39.1s\tremaining: 1m 16s\n44:\tlearn: 1.0420301\ttotal: 40s\tremaining: 1m 15s\n45:\tlearn: 1.0409273\ttotal: 40.9s\tremaining: 1m 14s\n46:\tlearn: 1.0399078\ttotal: 41.8s\tremaining: 1m 13s\n47:\tlearn: 1.0388463\ttotal: 42.7s\tremaining: 1m 12s\n48:\tlearn: 1.0378469\ttotal: 43.5s\tremaining: 1m 11s\n49:\tlearn: 1.0368063\ttotal: 44.4s\tremaining: 1m 11s\n50:\tlearn: 1.0358886\ttotal: 45.3s\tremaining: 1m 10s\n51:\tlearn: 1.0348358\ttotal: 46.2s\tremaining: 1m 9s\n52:\tlearn: 1.0338077\ttotal: 47.1s\tremaining: 1m 8s\n53:\tlearn: 1.0328620\ttotal: 47.9s\tremaining: 1m 7s\n54:\tlearn: 1.0319495\ttotal: 48.8s\tremaining: 1m 6s\n55:\tlearn: 1.0311064\ttotal: 49.7s\tremaining: 1m 5s\n56:\tlearn: 1.0301321\ttotal: 50.6s\tremaining: 1m 4s\n57:\tlearn: 1.0291370\ttotal: 51.5s\tremaining: 1m 3s\n58:\tlearn: 1.0282007\ttotal: 52.4s\tremaining: 1m 3s\n59:\tlearn: 1.0273767\ttotal: 53.3s\tremaining: 1m 2s\n60:\tlearn: 1.0264110\ttotal: 54.2s\tremaining: 1m 1s\n61:\tlearn: 1.0253973\ttotal: 55.1s\tremaining: 1m\n62:\tlearn: 1.0243390\ttotal: 55.9s\tremaining: 59.5s\n63:\tlearn: 1.0234261\ttotal: 56.8s\tremaining: 58.6s\n64:\tlearn: 1.0225360\ttotal: 57.7s\tremaining: 57.7s\n65:\tlearn: 1.0215804\ttotal: 58.6s\tremaining: 56.8s\n66:\tlearn: 1.0206093\ttotal: 59.6s\tremaining: 56.1s\n67:\tlearn: 1.0196412\ttotal: 1m\tremaining: 55.3s\n68:\tlearn: 1.0185914\ttotal: 1m 1s\tremaining: 54.4s\n69:\tlearn: 1.0177161\ttotal: 1m 2s\tremaining: 53.5s\n70:\tlearn: 1.0167407\ttotal: 1m 3s\tremaining: 52.6s\n71:\tlearn: 1.0159013\ttotal: 1m 4s\tremaining: 51.8s\n72:\tlearn: 1.0150039\ttotal: 1m 5s\tremaining: 50.9s\n73:\tlearn: 1.0141219\ttotal: 1m 6s\tremaining: 50s\n74:\tlearn: 1.0132104\ttotal: 1m 6s\tremaining: 49.1s\n75:\tlearn: 1.0123058\ttotal: 1m 7s\tremaining: 48.2s\n76:\tlearn: 1.0115111\ttotal: 1m 8s\tremaining: 47.3s\n77:\tlearn: 1.0107241\ttotal: 1m 9s\tremaining: 46.4s\n78:\tlearn: 1.0099557\ttotal: 1m 10s\tremaining: 45.5s\n79:\tlearn: 1.0091729\ttotal: 1m 11s\tremaining: 44.6s\n80:\tlearn: 1.0082886\ttotal: 1m 12s\tremaining: 43.7s\n81:\tlearn: 1.0073745\ttotal: 1m 13s\tremaining: 42.8s\n82:\tlearn: 1.0066030\ttotal: 1m 13s\tremaining: 41.9s\n83:\tlearn: 1.0057763\ttotal: 1m 14s\tremaining: 41s\n84:\tlearn: 1.0050067\ttotal: 1m 15s\tremaining: 40.1s\n85:\tlearn: 1.0042350\ttotal: 1m 16s\tremaining: 39.2s\n86:\tlearn: 1.0033399\ttotal: 1m 17s\tremaining: 38.3s\n87:\tlearn: 1.0025321\ttotal: 1m 18s\tremaining: 37.4s\n88:\tlearn: 1.0016909\ttotal: 1m 19s\tremaining: 36.5s\n89:\tlearn: 1.0008586\ttotal: 1m 20s\tremaining: 35.6s\n90:\tlearn: 0.9999955\ttotal: 1m 21s\tremaining: 34.7s\n91:\tlearn: 0.9991536\ttotal: 1m 21s\tremaining: 33.8s\n92:\tlearn: 0.9983789\ttotal: 1m 22s\tremaining: 32.9s\n93:\tlearn: 0.9976792\ttotal: 1m 23s\tremaining: 32.1s\n94:\tlearn: 0.9968760\ttotal: 1m 24s\tremaining: 31.2s\n95:\tlearn: 0.9960922\ttotal: 1m 25s\tremaining: 30.3s\n96:\tlearn: 0.9952548\ttotal: 1m 26s\tremaining: 29.4s\n97:\tlearn: 0.9947118\ttotal: 1m 27s\tremaining: 28.5s\n98:\tlearn: 0.9940059\ttotal: 1m 28s\tremaining: 27.6s\n99:\tlearn: 0.9932608\ttotal: 1m 29s\tremaining: 26.7s\n100:\tlearn: 0.9924981\ttotal: 1m 29s\tremaining: 25.8s\n101:\tlearn: 0.9917020\ttotal: 1m 30s\tremaining: 24.9s\n102:\tlearn: 0.9909189\ttotal: 1m 31s\tremaining: 24.1s\n103:\tlearn: 0.9901326\ttotal: 1m 32s\tremaining: 23.2s\n104:\tlearn: 0.9893296\ttotal: 1m 33s\tremaining: 22.3s\n105:\tlearn: 0.9886455\ttotal: 1m 34s\tremaining: 21.4s\n106:\tlearn: 0.9879449\ttotal: 1m 35s\tremaining: 20.5s\n107:\tlearn: 0.9871667\ttotal: 1m 36s\tremaining: 19.7s\n108:\tlearn: 0.9864219\ttotal: 1m 37s\tremaining: 18.8s\n109:\tlearn: 0.9857885\ttotal: 1m 38s\tremaining: 17.9s\n110:\tlearn: 0.9851855\ttotal: 1m 39s\tremaining: 17s\n111:\tlearn: 0.9844614\ttotal: 1m 40s\tremaining: 16.1s\n112:\tlearn: 0.9836292\ttotal: 1m 40s\tremaining: 15.2s\n113:\tlearn: 0.9829809\ttotal: 1m 41s\tremaining: 14.3s\n114:\tlearn: 0.9822709\ttotal: 1m 42s\tremaining: 13.4s\n115:\tlearn: 0.9814921\ttotal: 1m 43s\tremaining: 12.5s\n116:\tlearn: 0.9808303\ttotal: 1m 44s\tremaining: 11.6s\n117:\tlearn: 0.9801924\ttotal: 1m 45s\tremaining: 10.7s\n118:\tlearn: 0.9794654\ttotal: 1m 46s\tremaining: 9.81s\n119:\tlearn: 0.9787256\ttotal: 1m 47s\tremaining: 8.92s\n120:\tlearn: 0.9781398\ttotal: 1m 47s\tremaining: 8.03s\n121:\tlearn: 0.9774514\ttotal: 1m 48s\tremaining: 7.13s\n122:\tlearn: 0.9767455\ttotal: 1m 49s\tremaining: 6.24s\n123:\tlearn: 0.9761625\ttotal: 1m 50s\tremaining: 5.35s\n124:\tlearn: 0.9753794\ttotal: 1m 51s\tremaining: 4.46s\n125:\tlearn: 0.9748082\ttotal: 1m 52s\tremaining: 3.56s\n126:\tlearn: 0.9742158\ttotal: 1m 53s\tremaining: 2.67s\n127:\tlearn: 0.9734885\ttotal: 1m 54s\tremaining: 1.78s\n128:\tlearn: 0.9728199\ttotal: 1m 55s\tremaining: 893ms\n129:\tlearn: 0.9721885\ttotal: 1m 56s\tremaining: 0us\nBest params for cat:\n{'n_estimators': 130, 'max_depth': 10, 'loss_function': 'MultiClass', 'learning_rate': 0.01, 'l2_leaf_reg': 9}\nClassification report for cat:\n              precision    recall  f1-score   support\n\n        -1.0       1.00      0.02      0.04       135\n         0.0       0.60      0.64      0.62       238\n         1.0       0.47      0.70      0.56       227\n\n    accuracy                           0.53       600\n   macro avg       0.69      0.46      0.41       600\nweighted avg       0.64      0.53      0.47       600\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## RF ensembles","metadata":{"id":"PmLTGLAi6fLz"}},{"cell_type":"code","source":"#@title\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom joblib import parallel_backend\n\n'''\nThis code defines a stacking ensemble using RandomForestClassifier, KNeighborsClassifier, and MLPClassifier \nas base models, and MLPClassifier as the final estimator. It then uses a GridSearchCV object to search over \na parameter grid for the best hyperparameters. Finally, it prints the classification report for the predictions made on the test set.\n'''\n\n# define base models\nmodel1 = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = MLPClassifier(hidden_layer_sizes=(32, 32), random_state=42)\n\n# define stacking ensemble\nestimators = [('rf', model1),\n              ('knn', model2),\n              ('mlp', model3)]\n\nclf = StackingClassifier(estimators=estimators, \n                         final_estimator=MLPClassifier(hidden_layer_sizes=(64, 64),\n                                                       random_state=42))\n\n\n# define parameter grid for randomized search\nparam_dist = {'rf__max_depth': [20,30,40],\n              'rf__max_features': ['sqrt'],\n              'rf__min_samples_leaf': [1, 2, 5, 10],\n              'rf__min_samples_split': [2, 5, 10, 15],\n              'knn__n_neighbors': [3, 5, 7],\n              'mlp__alpha': [0.001, 0.01, 0.1, 1],\n              'mlp__learning_rate_init': [0.001, 0.01, 0.1],\n              'final_estimator__alpha': [0.001, 0.01, 0.1, 1],\n              'final_estimator__learning_rate_init': [0.001, 0.01, 0.1]}\n\n# define RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=clf, \n                             param_distributions=param_dist, \n                             cv=3, \n                             n_jobs=-1, \n                             verbose=2)\n\n# fit the RandomizedSearchCV object to the data\nwith parallel_backend('multiprocessing'): # Enabling multiprocessing for parallel execution\n    random_search.fit(x_train, y_train)\n\n# make predictions on test set\ny_pred = random_search.predict(x_test)\n\n# print classification report\nprint(classification_report(y_test, y_pred))\n","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"4D1pgLW27K4d","outputId":"c713d691-6cfe-4a63-d5a8-083750e72d8c","scrolled":true,"execution":{"iopub.status.busy":"2023-04-16T14:55:08.361473Z","iopub.execute_input":"2023-04-16T14:55:08.362721Z","iopub.status.idle":"2023-04-16T15:00:45.390054Z","shell.execute_reply.started":"2023-04-16T14:55:08.362658Z","shell.execute_reply":"2023-04-16T15:00:45.388696Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 10 candidates, totalling 30 fits\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  31.2s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  31.5s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=15; total time=  17.0s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  30.0s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=15; total time=  19.0s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=15; total time=  15.7s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=30, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  16.1s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=30, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  15.1s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=30, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  16.1s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=15; total time=  42.4s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=15; total time=  47.0s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.1, rf__max_depth=30, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  15.7s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.1, rf__max_depth=30, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  15.5s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=15; total time=  43.8s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.1, rf__max_depth=30, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  14.0s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2; total time=  16.9s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2; total time=  14.7s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2; total time=  15.8s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=2; total time=  42.2s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=2; total time=  42.6s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=5; total time=  10.1s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=5; total time=  11.8s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=5; total time=   9.8s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=2; total time=  44.6s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.01, rf__max_depth=30, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=10; total time=   8.9s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.01, rf__max_depth=30, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=10; total time=   8.2s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.01, rf__max_depth=30, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=10; total time=   9.2s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2; total time=  13.2s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2; total time=  13.8s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2; total time=   9.1s\n              precision    recall  f1-score   support\n\n        -1.0       0.75      0.50      0.60       135\n         0.0       0.73      0.92      0.81       238\n         1.0       0.77      0.71      0.74       227\n\n    accuracy                           0.74       600\n   macro avg       0.75      0.71      0.72       600\nweighted avg       0.75      0.74      0.74       600\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## RF+bagging","metadata":{"id":"8pVEUNjP7bX6"}},{"cell_type":"code","source":"#@title\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nimport xgboost as xgb\nimport sklearn\nx_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny_test = y_test.astype(np.float32)\ndef modelfit(x_train, y_train, x_test, y_test, params, early_stopping_rounds=100):        \n    # Initialize the base classifier\n    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n    \n    # Initialize the bagging classifier\n    # Initializing a Bagging classifier with Random Forest as the base estimator and 10 estimators\n    alg = BaggingClassifier(base_estimator=clf, n_estimators=10, random_state=0)\n    \n    # Performing randomized search cross-validation with Bagging classifier as the estimator and given parameter distributions.\n    grid_xgb = sklearn.model_selection.RandomizedSearchCV(estimator=alg, param_distributions=params, cv=3, verbose=2, scoring='roc_auc')\n\n    #Fit the algorithm on the data\n    grid_xgb.fit(x_train, y_train)\n    grid_xgb = grid_xgb\n    grid_result = grid_xgb.best_estimator_\n    \n    #train_history = grid_result.evals_result()\n    y_pred_test = grid_result.predict(x_test)\n    y_predprob_test = grid_result.predict_proba(x_test)[:,-1] #Computing the probability of the positive class for each test sample\n\n    return grid_xgb, grid_result, y_pred_test, y_predprob_test\n\nfrom joblib import parallel_backend\n\nparam_grid = {\n'n_estimators':[int(x) for x in np.linspace(start = 10, stop = 200, num = 20)],\n}\n\n#with LocalCUDACluster() as cluster:\n#    with Client(cluster) as client:\nwith parallel_backend('multiprocessing'): #Enabling multiprocessing for parallel execution\n        grid_xgb, grid_result_xgb, y_pred_test_xgb, y_predprob_test_xgb = modelfit(x_train, y_train, x_test, y_test, param_grid)\n\nprint(sklearn.metrics.classification_report(y_test, y_pred_test_xgb))\n","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"BCHRpWDH7K9t","outputId":"325e80d9-9caf-4018-d702-0dd0c19d78cd","scrolled":true,"execution":{"iopub.status.busy":"2023-05-15T20:43:01.276428Z","iopub.execute_input":"2023-05-15T20:43:01.278736Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 10 candidates, totalling 30 fits\n[CV] END ....................................n_estimators=40; total time=  55.0s\n[CV] END ....................................n_estimators=40; total time=  55.6s\n[CV] END ....................................n_estimators=40; total time=  55.0s\n[CV] END ....................................n_estimators=80; total time= 1.8min\n[CV] END ...................................n_estimators=200; total time= 4.6min\n[CV] END ...................................n_estimators=200; total time= 4.6min\n[CV] END ...................................n_estimators=200; total time= 4.6min\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## RF+GB stacking and voting","metadata":{}},{"cell_type":"code","source":"#@title\n'''\nIn this example, we are using a stacking classifier as the hybrid ensemble. We first define two base estimators, RandomForestClassifier and GradientBoostingClassifier, \nand then stack them using a StackingClassifier with a RandomForestClassifier as the final estimator. We also define a VotingClassifier that uses the same base estimators.\n'''\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Define the base estimators\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\ngb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n\n# Define the stacking estimator\nestimators = [\n    ('rf', rf),\n    ('gb', gb)\n]\nstacking_clf = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier()) # stacking classifier combining rf and gb\n\n# Define the voting classifier\nvoting_clf = VotingClassifier(estimators=estimators) # voting classifier using rf and gb\n\n# Perform grid search to find the best hyperparameters for the stacking classifier\ngrid_search_stacking = RandomizedSearchCV(stacking_clf, param_distributions = param_grid_lgbm, cv=5, verbose=2)\nwith parallel_backend('multiprocessing'): # Enabling multiprocessing for parallel execution\n    grid_search_stacking.fit(x_train, y_train)\n\n    # Fit the voting classifier on the training data\n    voting_clf.fit(x_train, y_train)\n\n# Make predictions on the test data\ny_pred_stacking = grid_search_stacking.predict(x_test)\ny_pred_voting = voting_clf.predict(x_test)\n\n# Print the accuracy scores\nprint('Stacking classifier CR: ', sklearn.metrics.classification_report(y_test, y_pred_stacking))\nprint('Voting classifier CR: ', sklearn.metrics.classification_report(y_test, y_pred_voting))\n","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"qgyO6ZQo81tq","outputId":"4a54a9a4-0702-44fc-a951-cad7292ba441","execution":{"iopub.status.busy":"2023-04-16T15:14:58.067831Z","iopub.execute_input":"2023-04-16T15:14:58.068203Z","iopub.status.idle":"2023-04-16T15:17:55.544203Z","shell.execute_reply.started":"2023-04-16T15:14:58.068169Z","shell.execute_reply":"2023-04-16T15:17:55.542838Z"},"scrolled":true,"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 1 candidates, totalling 5 fits\n[CV] END final_estimator__max_depth=30, final_estimator__max_features=sqrt, final_estimator__min_samples_leaf=1, final_estimator__min_samples_split=5, final_estimator__n_estimators=80; total time=  51.3s\n[CV] END final_estimator__max_depth=30, final_estimator__max_features=sqrt, final_estimator__min_samples_leaf=1, final_estimator__min_samples_split=5, final_estimator__n_estimators=80; total time=  51.6s\n[CV] END final_estimator__max_depth=30, final_estimator__max_features=sqrt, final_estimator__min_samples_leaf=1, final_estimator__min_samples_split=5, final_estimator__n_estimators=80; total time=  49.9s\n[CV] END final_estimator__max_depth=30, final_estimator__max_features=sqrt, final_estimator__min_samples_leaf=1, final_estimator__min_samples_split=5, final_estimator__n_estimators=80; total time=  50.0s\n[CV] END final_estimator__max_depth=30, final_estimator__max_features=sqrt, final_estimator__min_samples_leaf=1, final_estimator__min_samples_split=5, final_estimator__n_estimators=80; total time=  31.3s\nStacking classifier CR:                precision    recall  f1-score   support\n\n        -1.0       0.69      0.63      0.66       135\n         0.0       0.78      0.91      0.84       238\n         1.0       0.83      0.74      0.78       227\n\n    accuracy                           0.78       600\n   macro avg       0.77      0.76      0.76       600\nweighted avg       0.78      0.78      0.78       600\n\nVoting classifier CR:                precision    recall  f1-score   support\n\n        -1.0       0.77      0.49      0.60       135\n         0.0       0.69      0.95      0.80       238\n         1.0       0.78      0.65      0.71       227\n\n    accuracy                           0.73       600\n   macro avg       0.75      0.69      0.70       600\nweighted avg       0.74      0.73      0.72       600\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title\ngrid_search_stacking.best_params_","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"oRRF17UBJpoC","outputId":"d59644e2-821e-4870-f149-83b5337837a4","execution":{"iopub.status.busy":"2023-04-16T15:00:46.380717Z","iopub.status.idle":"2023-04-16T15:00:46.381601Z","shell.execute_reply.started":"2023-04-16T15:00:46.381327Z","shell.execute_reply":"2023-04-16T15:00:46.381353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM ensemble","metadata":{}},{"cell_type":"code","source":"#@title\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom joblib import parallel_backend\n\n'''\nThis code defines a stacking ensemble using LGBM Classifier, KNeighborsClassifier, and MLPClassifier as base models, and MLPClassifier as the final estimator. \nIt then uses a GridSearchCV object to search over a parameter grid for the best hyperparameters. Finally, it prints the classification report for the predictions made on the test set.\n'''\nx_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny_test = y_test.astype(np.float32)\n\n# define base models\nmodel1 = lgbm.LGBMClassifier(n_estimators=100, random_state=42, \n                          colsample_bytree = 0.6,\n                          subsample = 0.8,\n                          min_child_samples = 1,\n                          objective = 'multiclass',\n                          num_leaves = 20,\n                          max_depth = 10,\n                          learning_rate = 0.5\n                         ) #RandomForestClassifier(n_estimators=100, random_state=42)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = MLPClassifier(hidden_layer_sizes=(32, 32), random_state=42)\n\n# define stacking ensemble\nestimators = [('lg', model1),\n              ('knn', model2),\n              ('mlp', model3)]\n\nclf = StackingClassifier(estimators=estimators, \n                         final_estimator=MLPClassifier(hidden_layer_sizes=(64, 64),\n                                                       random_state=42))\n\n\n# define parameter grid for randomized search\nparam_dist = {\n              'knn__n_neighbors': [3, 5, 7],\n              'mlp__alpha': [0.001, 0.01, 0.1, 1],\n              'mlp__learning_rate_init': [0.001, 0.01, 0.1],\n              'final_estimator__alpha': [0.001, 0.01, 0.1, 1],\n              'final_estimator__learning_rate_init': [0.001, 0.01, 0.1]}\n\n# define RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=clf, \n                             param_distributions=param_dist, \n                             cv=3, \n                             n_jobs=-1, \n                             verbose=2)\n\n# fit the RandomizedSearchCV object to the data\nwith parallel_backend('multiprocessing'):\n      random_search.fit(x_train, y_train)\n\n# make predictions on test set\ny_pred = random_search.predict(x_test)\n\n# print classification report\nprint(classification_report(y_test, y_pred))\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-20T21:00:26.386889Z","iopub.execute_input":"2023-04-20T21:00:26.387386Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 10 candidates, totalling 30 fits\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time= 1.6min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time= 1.6min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 3.9min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 3.9min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 4.2min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time= 1.9min\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1; total time= 1.7min\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1; total time= 1.8min\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1; total time= 1.7min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.01; total time= 1.6min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.01; total time= 1.6min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.01; total time= 1.7min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.1; total time= 2.1min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.1; total time= 2.2min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.1; total time= 2.3min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.01; total time= 2.4min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.01; total time= 2.5min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.01; total time= 2.4min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.01; total time= 2.4min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.01; total time= 2.5min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.01; total time= 2.5min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.1; total time= 2.1min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.1; total time= 2.0min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.1; total time= 1.8min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.001; total time= 2.4min\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1; total time= 1.8min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.001; total time= 2.7min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.001; total time= 2.5min\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1; total time= 1.4min\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1; total time= 1.2min\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT","metadata":{"id":"T3hL4iUCgHKB"}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #Checking for CUDA availability to decide the device\n\n#Loading a pre-trained DistilBERT tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n\n#Tokenizing the input sentences, padding and truncating them\ndf_bert = tokenizer(df['lemma_sentence(with POS)'].values.tolist(), padding = True, truncation = True, return_tensors=\"pt\")\n\nprint(df_bert.keys())\n\n#Moving the tokenized input to the available device (GPU)\ntokenized_train = {k:torch.tensor(v).to(device) for k,v in df_bert.items()}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ng0YlGjZV_SZ","outputId":"396134a9-6b00-4f56-a24f-51a360c08da2","execution":{"iopub.status.busy":"2023-05-15T17:27:42.714158Z","iopub.execute_input":"2023-05-15T17:27:42.717841Z","iopub.status.idle":"2023-05-15T17:27:50.901938Z","shell.execute_reply.started":"2023-05-15T17:27:42.717785Z","shell.execute_reply":"2023-05-15T17:27:50.900681Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cbdeced04e74a189726f40a8bd66a1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9472d1b7ef341c785d3d07a215d2787"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5be1ecbdc38d4a97a1896a9e2c617d72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66b29628adb84523a621a85075bc4dbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6726e89497fc4e2d93e7064a6f0deb65"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask'])\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad(): #Context manager to disable gradient calculation for efficiency\n    #\"model(**tokenized_train)\": Model inference using tokenized input\n    #\"hidden_train = ...\": Obtaining hidden states from the model's output\n    hidden_train = model(**tokenized_train) #dim : [batch_size(nr_sentences), tokens, emb_dim]\n\n#get only the [CLS] hidden states\ncls_train = hidden_train.last_hidden_state[:,0,:] #Extracting only the [CLS] token's hidden state for each sentence","metadata":{"id":"bhDrGpMDXXSD","execution":{"iopub.status.busy":"2023-05-15T17:27:50.904554Z","iopub.execute_input":"2023-05-15T17:27:50.905001Z","iopub.status.idle":"2023-05-15T17:27:50.931206Z","shell.execute_reply.started":"2023-05-15T17:27:50.904958Z","shell.execute_reply":"2023-05-15T17:27:50.929990Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(cls_train.to(\"cpu\"), df['senti_textblob'],test_size = 0.2)\n#@title\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfrom sklearn.preprocessing import OneHotEncoder\ny_train_encoded = le.fit_transform(y_train)\ny_test_encoded = le.fit_transform(y_test)\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bO1nwn7B_2TF","outputId":"05299ff9-c4c2-4b2c-9955-230cb1a7123b","execution":{"iopub.status.busy":"2023-05-15T17:27:50.932618Z","iopub.execute_input":"2023-05-15T17:27:50.933010Z","iopub.status.idle":"2023-05-15T17:27:56.632044Z","shell.execute_reply.started":"2023-05-15T17:27:50.932967Z","shell.execute_reply":"2023-05-15T17:27:56.630706Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"torch.Size([2400, 768]) (2400,) torch.Size([600, 768]) (600,)\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train = np.array(x_train, dtype='float32')\ny_train = np.array(y_train, dtype='float32')\nx_test = np.array(x_test, dtype='float32')\ny_test = np.array(y_test, dtype='float32')","metadata":{"id":"ZqnKXCncVg12","execution":{"iopub.status.busy":"2023-05-15T17:27:56.634645Z","iopub.execute_input":"2023-05-15T17:27:56.634993Z","iopub.status.idle":"2023-05-15T17:27:56.647166Z","shell.execute_reply.started":"2023-05-15T17:27:56.634960Z","shell.execute_reply":"2023-05-15T17:27:56.646044Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"### ALL MODELS #######\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom joblib import parallel_backend\n\nx_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny_test = y_test.astype(np.float32)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Scale the input data\nscaler = MinMaxScaler()\nx_train_scaled = scaler.fit_transform(x_train)\n\nmodels_list = [model_forest,model_nb,model_svc,model_gbm,model_lgbm,model_xgb,model_cat]\nmodels_name = ['rf','nb','svc','gbm','lgbm','xgb','cat']\nparam_grids = [param_grid_forest,param_grid_nb,param_grid_svc,param_grid_gbm,param_grid_lgbm,param_grid_xgb,param_grid_catboost]\n\nfor model,name,param_grid in zip(models_list, models_name, param_grids):\n    #best parameters for model (with BoW)\n    RandomGrid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=2, verbose=0, n_jobs=4, random_state=40)\n    with parallel_backend('multiprocessing'):\n        if(name!='xgb' and name!='nb'):\n            RandomGrid.fit(x_train, y_train)\n        elif(name=='nb'):\n            RandomGrid.fit(x_train_scaled, y_train)\n        else:\n            RandomGrid.fit(x_train, y_train_encoded)\n    print('Best params for ' + name+':')\n    print(RandomGrid.best_params_)\n\n    print('Classification report for ' + name+':')\n    if(name!='xgb' and name!='nb'):\n        print(classification_report(y_test, RandomGrid.predict(x_test)))\n    elif(name=='nb'):\n        print(classification_report(y_test, RandomGrid.predict(x_test)))\n    else:\n        print(classification_report(y_test_encoded, RandomGrid.predict(x_test)))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-Q5JPSW_2TF","outputId":"5267cbbc-be23-4761-8a93-73b0308ca51c","scrolled":true},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Best params for rf:\n\n{'n_estimators': 190, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 40}\n\nClassification report for rf:\n\n              precision    recall  f1-score   support\n\n\n\n        -1.0       0.40      0.14      0.20       125\n\n         0.0       0.67      0.70      0.68       257\n\n         1.0       0.53      0.70      0.60       218\n\n\n\n    accuracy                           0.58       600\n\n   macro avg       0.53      0.51      0.49       600\n\nweighted avg       0.56      0.58      0.55       600\n\n\n\nBest params for nb:\n\n{'fit_prior': True, 'alpha': 0.5}\n\nClassification report for nb:\n\n              precision    recall  f1-score   support\n\n\n\n        -1.0       0.38      0.44      0.41       125\n\n         0.0       0.62      0.63      0.62       257\n\n         1.0       0.51      0.45      0.48       218\n\n\n\n    accuracy                           0.53       600\n\n   macro avg       0.50      0.51      0.50       600\n\nweighted avg       0.53      0.53      0.53       600\n\n\n\nBest params for svc:\n\n{'kernel': 'sigmoid', 'degree': 2, 'C': 10}\n\nClassification report for svc:\n\n              precision    recall  f1-score   support\n\n\n\n        -1.0       0.34      0.33      0.33       125\n\n         0.0       0.67      0.67      0.67       257\n\n         1.0       0.54      0.56      0.55       218\n\n\n\n    accuracy                           0.56       600\n\n   macro avg       0.52      0.52      0.52       600\n\nweighted avg       0.56      0.56      0.56       600\n\n\n\nBest params for gbm:\n\n{'n_estimators': 70, 'min_samples_split': 15, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'max_depth': 20, 'learning_rate': 0.1}\n\nClassification report for gbm:\n\n              precision    recall  f1-score   support\n\n\n\n        -1.0       0.43      0.26      0.33       125\n\n         0.0       0.68      0.71      0.70       257\n\n         1.0       0.56      0.66      0.61       218\n\n\n\n    accuracy                           0.60       600\n\n   macro avg       0.56      0.54      0.54       600\n\nweighted avg       0.59      0.60      0.59       600\n\n\n\nBest params for lgbm:\n\n{'subsample': 0.9, 'objective': 'multiclass', 'num_leaves': 20, 'n_estimators': 130, 'min_child_samples': 5, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.6}\n\nClassification report for lgbm:\n\n              precision    recall  f1-score   support\n\n\n\n        -1.0       0.45      0.36      0.40       125\n\n         0.0       0.69      0.70      0.69       257\n\n         1.0       0.59      0.65      0.62       218\n\n\n\n    accuracy                           0.61       600\n\n   macro avg       0.58      0.57      0.57       600\n\nweighted avg       0.60      0.61      0.61       600\n\n\n\n[17:00:38] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n[17:00:38] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n[17:00:38] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n[17:00:38] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\n\n\n\n\n\n\n[17:00:39] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\n[17:00:39] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\n[17:00:54] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\n[17:00:54] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\n[17:01:02] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\n[17:01:03] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\n[17:01:09] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\n[17:01:09] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\n[17:04:58] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\n[17:04:59] WARNING: ../src/learner.cc:767: \n\nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } are not used.\n\n\n\nBest params for xgb:\n\n{'subsample': 0.7, 'objective': 'multi:softmax', 'n_estimators': 170, 'min_child_weight': 5, 'max_depth': 40, 'learning_rate': 0.3, 'gamma': 0.0, 'colsample_bytree': 0.8, 'booster': 'gbtree'}\n\nClassification report for xgb:\n\n              precision    recall  f1-score   support\n\n\n\n           0       0.43      0.39      0.41       125\n\n           1       0.69      0.70      0.69       257\n\n           2       0.58      0.59      0.58       218\n\n\n\n    accuracy                           0.60       600\n\n   macro avg       0.56      0.56      0.56       600\n\nweighted avg       0.59      0.60      0.59       600\n\n\n\n0:\tlearn: 0.9228569\ttotal: 34.2s\tremaining: 33m 39s\n\n0:\tlearn: 0.9338481\ttotal: 34.6s\tremaining: 34m 2s\n\n0:\tlearn: 1.0946458\ttotal: 34.5s\tremaining: 45m 28s\n\n0:\tlearn: 1.0945568\ttotal: 35s\tremaining: 46m 7s\n\n1:\tlearn: 0.7948372\ttotal: 1m 9s\tremaining: 33m 30s\n\n1:\tlearn: 1.0907562\ttotal: 1m 10s\tremaining: 45m 36s\n\n1:\tlearn: 1.0905425\ttotal: 1m 10s\tremaining: 45m 50s\n\n1:\tlearn: 0.7976716\ttotal: 1m 10s\tremaining: 34m 1s\n\n2:\tlearn: 0.6833966\ttotal: 1m 41s\tremaining: 32m 13s\n\n2:\tlearn: 1.0863966\ttotal: 1m 42s\tremaining: 43m 51s\n\n2:\tlearn: 1.0866704\ttotal: 1m 42s\tremaining: 43m 58s\n\n2:\tlearn: 0.6906485\ttotal: 1m 42s\tremaining: 32m 35s\n\n3:\tlearn: 0.5838458\ttotal: 2m 11s\tremaining: 30m 46s\n\n3:\tlearn: 1.0824661\ttotal: 2m 13s\tremaining: 42m 11s\n\n3:\tlearn: 1.0818113\ttotal: 2m 13s\tremaining: 42m 25s\n\n3:\tlearn: 0.5897554\ttotal: 2m 14s\tremaining: 31m 26s\n\n4:\tlearn: 0.5171355\ttotal: 2m 43s\tremaining: 29m 53s\n\n4:\tlearn: 1.0782772\ttotal: 2m 44s\tremaining: 41m\n\n4:\tlearn: 1.0772290\ttotal: 2m 44s\tremaining: 41m 12s\n\n4:\tlearn: 0.5038173\ttotal: 2m 45s\tremaining: 30m 18s\n\n5:\tlearn: 0.4507647\ttotal: 3m 15s\tremaining: 29m 20s\n\n5:\tlearn: 1.0741065\ttotal: 3m 16s\tremaining: 40m 22s\n\n5:\tlearn: 1.0728617\ttotal: 3m 17s\tremaining: 40m 34s\n\n5:\tlearn: 0.4433889\ttotal: 3m 17s\tremaining: 29m 40s\n\n6:\tlearn: 0.4152437\ttotal: 3m 46s\tremaining: 28m 36s\n\n6:\tlearn: 1.0698799\ttotal: 3m 47s\tremaining: 39m 37s\n\n6:\tlearn: 0.3940103\ttotal: 3m 49s\tremaining: 29m 1s\n\n6:\tlearn: 1.0685314\ttotal: 3m 49s\tremaining: 39m 57s\n\n7:\tlearn: 0.3770685\ttotal: 4m 16s\tremaining: 27m 47s\n\n7:\tlearn: 1.0658297\ttotal: 4m 17s\tremaining: 38m 40s\n\n7:\tlearn: 0.3522215\ttotal: 4m 20s\tremaining: 28m 11s\n\n7:\tlearn: 1.0642977\ttotal: 4m 20s\tremaining: 39m 5s\n\n8:\tlearn: 0.3386666\ttotal: 4m 48s\tremaining: 27m 16s\n\n8:\tlearn: 1.0622627\ttotal: 4m 50s\tremaining: 38m 10s\n\n8:\tlearn: 1.0600664\ttotal: 4m 51s\tremaining: 38m 20s\n\n8:\tlearn: 0.3107020\ttotal: 4m 51s\tremaining: 27m 34s\n\n9:\tlearn: 0.3026491\ttotal: 5m 21s\tremaining: 26m 45s\n\n9:\tlearn: 1.0584652\ttotal: 5m 22s\tremaining: 37m 36s\n\n9:\tlearn: 1.0557840\ttotal: 5m 23s\tremaining: 37m 47s\n\n9:\tlearn: 0.2844572\ttotal: 5m 24s\tremaining: 27m\n\n10:\tlearn: 0.2724856\ttotal: 5m 50s\tremaining: 26m 1s\n\n10:\tlearn: 1.0545244\ttotal: 5m 52s\tremaining: 36m 48s\n\n10:\tlearn: 1.0516176\ttotal: 5m 55s\tremaining: 37m 7s\n\n10:\tlearn: 0.2589330\ttotal: 5m 55s\tremaining: 26m 24s\n\n11:\tlearn: 0.2501955\ttotal: 6m 22s\tremaining: 25m 29s\n\n11:\tlearn: 1.0510182\ttotal: 6m 23s\tremaining: 36m 11s\n\n11:\tlearn: 0.2406585\ttotal: 6m 25s\tremaining: 25m 40s\n\n11:\tlearn: 1.0476666\ttotal: 6m 25s\tremaining: 36m 23s\n\n12:\tlearn: 0.2305505\ttotal: 6m 54s\tremaining: 24m 58s\n\n12:\tlearn: 1.0470601\ttotal: 6m 55s\tremaining: 35m 42s\n\n12:\tlearn: 0.2212752\ttotal: 6m 57s\tremaining: 25m 8s\n\n12:\tlearn: 1.0435773\ttotal: 6m 58s\tremaining: 35m 55s\n\n13:\tlearn: 0.2088576\ttotal: 7m 25s\tremaining: 24m 24s\n\n13:\tlearn: 1.0433970\ttotal: 7m 26s\tremaining: 35m 6s\n\n13:\tlearn: 0.2098715\ttotal: 7m 29s\tremaining: 24m 36s\n\n13:\tlearn: 1.0398704\ttotal: 7m 30s\tremaining: 35m 24s\n\n14:\tlearn: 1.0397916\ttotal: 7m 55s\tremaining: 34m 22s\n\n14:\tlearn: 0.1939034\ttotal: 7m 55s\tremaining: 23m 47s\n\n14:\tlearn: 0.1895904\ttotal: 8m\tremaining: 24m 1s\n\n14:\tlearn: 1.0361908\ttotal: 8m 1s\tremaining: 34m 48s\n\n15:\tlearn: 1.0357699\ttotal: 8m 27s\tremaining: 33m 51s\n\n15:\tlearn: 0.1768897\ttotal: 8m 28s\tremaining: 23m 17s\n\n15:\tlearn: 0.1730610\ttotal: 8m 31s\tremaining: 23m 27s\n\n15:\tlearn: 1.0319118\ttotal: 8m 31s\tremaining: 34m 7s\n\n16:\tlearn: 1.0322020\ttotal: 9m\tremaining: 33m 22s\n\n16:\tlearn: 0.1625843\ttotal: 9m\tremaining: 22m 47s\n\n16:\tlearn: 0.1568588\ttotal: 9m 3s\tremaining: 22m 55s\n\n16:\tlearn: 1.0279196\ttotal: 9m 3s\tremaining: 33m 35s\n\n17:\tlearn: 1.0283879\ttotal: 9m 30s\tremaining: 32m 45s\n\n17:\tlearn: 0.1512928\ttotal: 9m 31s\tremaining: 22m 12s\n\n17:\tlearn: 0.1437434\ttotal: 9m 36s\tremaining: 22m 24s\n\n17:\tlearn: 1.0238600\ttotal: 9m 36s\tremaining: 33m 5s\n\n18:\tlearn: 1.0245879\ttotal: 10m 1s\tremaining: 32m 12s\n\n18:\tlearn: 0.1397554\ttotal: 10m 2s\tremaining: 21m 39s\n\n18:\tlearn: 0.1318751\ttotal: 10m 7s\tremaining: 21m 50s\n\n18:\tlearn: 1.0203509\ttotal: 10m 7s\tremaining: 32m 29s\n\n19:\tlearn: 1.0207596\ttotal: 10m 34s\tremaining: 31m 42s\n\n19:\tlearn: 0.1280849\ttotal: 10m 34s\tremaining: 21m 8s\n\n19:\tlearn: 1.0166344\ttotal: 10m 38s\tremaining: 31m 54s\n\n19:\tlearn: 0.1212175\ttotal: 10m 38s\tremaining: 21m 17s\n\n20:\tlearn: 1.0174790\ttotal: 11m 5s\tremaining: 31m 10s\n\n20:\tlearn: 0.1202050\ttotal: 11m 7s\tremaining: 20m 38s\n\n20:\tlearn: 1.0130758\ttotal: 11m 10s\tremaining: 31m 23s\n\n20:\tlearn: 0.1124555\ttotal: 11m 10s\tremaining: 20m 46s\n\n21:\tlearn: 1.0138668\ttotal: 11m 35s\tremaining: 30m 32s\n\n21:\tlearn: 0.1132665\ttotal: 11m 36s\tremaining: 20m 3s\n\n21:\tlearn: 1.0098357\ttotal: 11m 42s\tremaining: 30m 52s\n\n21:\tlearn: 0.1045059\ttotal: 11m 43s\tremaining: 20m 14s\n\n22:\tlearn: 1.0099160\ttotal: 12m 7s\tremaining: 30m 1s\n\n22:\tlearn: 0.1053272\ttotal: 12m 8s\tremaining: 19m 31s\n\n22:\tlearn: 1.0061976\ttotal: 12m 13s\tremaining: 30m 16s\n\n22:\tlearn: 0.0975538\ttotal: 12m 13s\tremaining: 19m 39s\n\n23:\tlearn: 1.0066013\ttotal: 12m 39s\tremaining: 29m 31s\n\n23:\tlearn: 0.0995943\ttotal: 12m 40s\tremaining: 19m 1s\n\n23:\tlearn: 1.0025028\ttotal: 12m 44s\tremaining: 29m 44s\n\n23:\tlearn: 0.0907212\ttotal: 12m 45s\tremaining: 19m 7s\n\n24:\tlearn: 1.0030281\ttotal: 13m 10s\tremaining: 28m 58s\n\n24:\tlearn: 0.0940104\ttotal: 13m 12s\tremaining: 18m 29s\n\n24:\tlearn: 0.9989044\ttotal: 13m 16s\tremaining: 29m 13s\n\n24:\tlearn: 0.0850629\ttotal: 13m 17s\tremaining: 18m 36s\n\n25:\tlearn: 0.9991082\ttotal: 13m 41s\tremaining: 28m 25s\n\n25:\tlearn: 0.0879647\ttotal: 13m 42s\tremaining: 17m 55s\n\n25:\tlearn: 0.9948619\ttotal: 13m 48s\tremaining: 28m 41s\n\n25:\tlearn: 0.0801433\ttotal: 13m 50s\tremaining: 18m 6s\n\n26:\tlearn: 0.9951658\ttotal: 14m 13s\tremaining: 27m 56s\n\n26:\tlearn: 0.0829645\ttotal: 14m 15s\tremaining: 17m 25s\n\n26:\tlearn: 0.9918193\ttotal: 14m 18s\tremaining: 28m 5s\n\n26:\tlearn: 0.0756940\ttotal: 14m 20s\tremaining: 17m 31s\n\n27:\tlearn: 0.9915821\ttotal: 14m 45s\tremaining: 27m 25s\n\n27:\tlearn: 0.0779093\ttotal: 14m 47s\tremaining: 16m 53s\n\n27:\tlearn: 0.9884800\ttotal: 14m 51s\tremaining: 27m 34s\n\n27:\tlearn: 0.0715646\ttotal: 14m 52s\tremaining: 17m\n\n28:\tlearn: 0.9879711\ttotal: 15m 16s\tremaining: 26m 51s\n\n28:\tlearn: 0.0732971\ttotal: 15m 19s\tremaining: 16m 22s\n\n28:\tlearn: 0.9848193\ttotal: 15m 23s\tremaining: 27m 4s\n\n28:\tlearn: 0.0678713\ttotal: 15m 25s\tremaining: 16m 29s\n\n29:\tlearn: 0.9841833\ttotal: 15m 48s\tremaining: 26m 21s\n\n29:\tlearn: 0.0696578\ttotal: 15m 50s\tremaining: 15m 50s\n\n29:\tlearn: 0.9810451\ttotal: 15m 54s\tremaining: 26m 31s\n\n29:\tlearn: 0.0648460\ttotal: 15m 57s\tremaining: 15m 57s\n\n30:\tlearn: 0.9805619\ttotal: 16m 20s\tremaining: 25m 50s\n\n30:\tlearn: 0.0659796\ttotal: 16m 22s\tremaining: 15m 19s\n\n30:\tlearn: 0.9772785\ttotal: 16m 25s\tremaining: 25m 58s\n\n30:\tlearn: 0.0611685\ttotal: 16m 28s\tremaining: 15m 24s\n\n31:\tlearn: 0.9770087\ttotal: 16m 53s\tremaining: 25m 19s\n\n31:\tlearn: 0.0624831\ttotal: 16m 56s\tremaining: 14m 49s\n\n31:\tlearn: 0.9738278\ttotal: 16m 58s\tremaining: 25m 28s\n\n31:\tlearn: 0.0580415\ttotal: 17m 1s\tremaining: 14m 53s\n\n32:\tlearn: 0.9738368\ttotal: 17m 26s\tremaining: 24m 50s\n\n32:\tlearn: 0.0597259\ttotal: 17m 30s\tremaining: 14m 19s\n\n32:\tlearn: 0.9706088\ttotal: 17m 33s\tremaining: 24m 59s\n\n32:\tlearn: 0.0550948\ttotal: 17m 34s\tremaining: 14m 23s\n\n33:\tlearn: 0.9703773\ttotal: 17m 58s\tremaining: 24m 18s\n\n33:\tlearn: 0.0570912\ttotal: 18m 2s\tremaining: 13m 47s\n\n33:\tlearn: 0.9674013\ttotal: 18m 7s\tremaining: 24m 30s\n\n33:\tlearn: 0.0526589\ttotal: 18m 8s\tremaining: 13m 52s\n\n34:\tlearn: 0.9667093\ttotal: 18m 31s\tremaining: 23m 49s\n\n34:\tlearn: 0.0547374\ttotal: 18m 35s\tremaining: 13m 17s\n\n34:\tlearn: 0.9644097\ttotal: 18m 39s\tremaining: 23m 59s\n\n34:\tlearn: 0.0505915\ttotal: 18m 43s\tremaining: 13m 22s\n\n35:\tlearn: 0.9635542\ttotal: 19m 9s\tremaining: 23m 24s\n\n35:\tlearn: 0.0524212\ttotal: 19m 13s\tremaining: 12m 49s\n\n35:\tlearn: 0.9611962\ttotal: 19m 16s\tremaining: 23m 33s\n\n35:\tlearn: 0.0486238\ttotal: 19m 20s\tremaining: 12m 53s\n\n36:\tlearn: 0.9599907\ttotal: 19m 46s\tremaining: 22m 59s\n\n36:\tlearn: 0.0502679\ttotal: 19m 51s\tremaining: 12m 20s\n\n36:\tlearn: 0.9580565\ttotal: 19m 54s\tremaining: 23m 7s\n\n36:\tlearn: 0.0467155\ttotal: 19m 57s\tremaining: 12m 24s\n\n37:\tlearn: 0.9560440\ttotal: 20m 23s\tremaining: 22m 31s\n\n37:\tlearn: 0.0482046\ttotal: 20m 28s\tremaining: 11m 51s\n\n37:\tlearn: 0.9550976\ttotal: 20m 31s\tremaining: 22m 41s\n\n37:\tlearn: 0.0448063\ttotal: 20m 34s\tremaining: 11m 54s\n\n38:\tlearn: 0.9532558\ttotal: 20m 59s\tremaining: 22m 4s\n\n38:\tlearn: 0.0465143\ttotal: 21m 5s\tremaining: 11m 21s\n\n38:\tlearn: 0.9520311\ttotal: 21m 9s\tremaining: 22m 14s\n\n38:\tlearn: 0.0431512\ttotal: 21m 11s\tremaining: 11m 24s\n\n39:\tlearn: 0.9502815\ttotal: 21m 36s\tremaining: 21m 36s\n\n39:\tlearn: 0.0448106\ttotal: 21m 43s\tremaining: 10m 51s\n\n39:\tlearn: 0.9487373\ttotal: 21m 46s\tremaining: 21m 46s\n\n39:\tlearn: 0.0415078\ttotal: 21m 48s\tremaining: 10m 54s\n\n40:\tlearn: 0.9472031\ttotal: 22m 13s\tremaining: 21m 8s\n\n40:\tlearn: 0.0429060\ttotal: 22m 21s\tremaining: 10m 21s\n\n40:\tlearn: 0.9450723\ttotal: 22m 24s\tremaining: 21m 18s\n\n40:\tlearn: 0.0400034\ttotal: 22m 26s\tremaining: 10m 23s\n\n41:\tlearn: 0.9439397\ttotal: 22m 49s\tremaining: 20m 39s\n\n41:\tlearn: 0.0414665\ttotal: 22m 58s\tremaining: 9m 50s\n\n41:\tlearn: 0.9418192\ttotal: 23m 1s\tremaining: 20m 50s\n\n41:\tlearn: 0.0385698\ttotal: 23m 3s\tremaining: 9m 52s\n\n42:\tlearn: 0.9404735\ttotal: 23m 25s\tremaining: 20m 9s\n\n42:\tlearn: 0.0399943\ttotal: 23m 35s\tremaining: 9m 19s\n\n42:\tlearn: 0.9383820\ttotal: 23m 39s\tremaining: 20m 21s\n\n42:\tlearn: 0.0372824\ttotal: 23m 40s\tremaining: 9m 21s\n\n43:\tlearn: 0.9373478\ttotal: 24m 2s\tremaining: 19m 40s\n\n43:\tlearn: 0.0385848\ttotal: 24m 12s\tremaining: 8m 48s\n\n43:\tlearn: 0.9348767\ttotal: 24m 17s\tremaining: 19m 52s\n\n43:\tlearn: 0.0359551\ttotal: 24m 17s\tremaining: 8m 50s\n\n44:\tlearn: 0.9343120\ttotal: 24m 39s\tremaining: 19m 10s\n\n44:\tlearn: 0.0372932\ttotal: 24m 49s\tremaining: 8m 16s\n\n44:\tlearn: 0.9312447\ttotal: 24m 53s\tremaining: 19m 21s\n\n44:\tlearn: 0.0346489\ttotal: 24m 54s\tremaining: 8m 18s\n\n45:\tlearn: 0.9311710\ttotal: 25m 16s\tremaining: 18m 40s\n\n45:\tlearn: 0.0362617\ttotal: 25m 26s\tremaining: 7m 44s\n\n45:\tlearn: 0.0335493\ttotal: 25m 31s\tremaining: 7m 46s\n\n45:\tlearn: 0.9282724\ttotal: 25m 31s\tremaining: 18m 52s\n\n46:\tlearn: 0.9280699\ttotal: 25m 52s\tremaining: 18m 10s\n\n46:\tlearn: 0.0351490\ttotal: 26m 3s\tremaining: 7m 12s\n\n46:\tlearn: 0.9248503\ttotal: 26m 9s\tremaining: 18m 21s\n\n46:\tlearn: 0.0325231\ttotal: 26m 9s\tremaining: 7m 14s\n\n47:\tlearn: 0.9250944\ttotal: 26m 29s\tremaining: 17m 39s\n\n47:\tlearn: 0.0340319\ttotal: 26m 40s\tremaining: 6m 40s\n\n47:\tlearn: 0.0315652\ttotal: 26m 46s\tremaining: 6m 41s\n\n47:\tlearn: 0.9214522\ttotal: 26m 46s\tremaining: 17m 51s\n\n48:\tlearn: 0.9218552\ttotal: 27m 6s\tremaining: 17m 9s\n\n48:\tlearn: 0.0330038\ttotal: 27m 17s\tremaining: 6m 7s\n\n48:\tlearn: 0.9179315\ttotal: 27m 23s\tremaining: 17m 19s\n\n48:\tlearn: 0.0306711\ttotal: 27m 24s\tremaining: 6m 9s\n\n49:\tlearn: 0.9184475\ttotal: 27m 44s\tremaining: 16m 38s\n\n49:\tlearn: 0.0319798\ttotal: 27m 55s\tremaining: 5m 35s\n\n49:\tlearn: 0.9147863\ttotal: 28m 1s\tremaining: 16m 48s\n\n49:\tlearn: 0.0297716\ttotal: 28m 1s\tremaining: 5m 36s\n\n50:\tlearn: 0.9155360\ttotal: 28m 22s\tremaining: 16m 7s\n\n50:\tlearn: 0.0310624\ttotal: 28m 32s\tremaining: 5m 2s\n\n50:\tlearn: 0.9114611\ttotal: 28m 39s\tremaining: 16m 17s\n\n50:\tlearn: 0.0288773\ttotal: 28m 40s\tremaining: 5m 3s\n\n51:\tlearn: 0.9128893\ttotal: 28m 59s\tremaining: 15m 36s\n\n51:\tlearn: 0.0302695\ttotal: 29m 9s\tremaining: 4m 29s\n\n51:\tlearn: 0.9085272\ttotal: 29m 16s\tremaining: 15m 45s\n\n51:\tlearn: 0.0280218\ttotal: 29m 16s\tremaining: 4m 30s\n\n52:\tlearn: 0.9099732\ttotal: 29m 34s\tremaining: 15m 3s\n\n52:\tlearn: 0.0294315\ttotal: 29m 42s\tremaining: 3m 55s\n\n52:\tlearn: 0.9053383\ttotal: 29m 52s\tremaining: 15m 13s\n\n52:\tlearn: 0.0273218\ttotal: 29m 52s\tremaining: 3m 56s\n\n53:\tlearn: 0.9069609\ttotal: 30m 8s\tremaining: 14m 30s\n\n53:\tlearn: 0.0286418\ttotal: 30m 14s\tremaining: 3m 21s\n\n53:\tlearn: 0.9023470\ttotal: 30m 26s\tremaining: 14m 39s\n\n53:\tlearn: 0.0266230\ttotal: 30m 26s\tremaining: 3m 22s\n\n54:\tlearn: 0.9037990\ttotal: 30m 41s\tremaining: 13m 57s\n\n54:\tlearn: 0.0278190\ttotal: 30m 49s\tremaining: 2m 48s\n\n54:\tlearn: 0.0259257\ttotal: 30m 59s\tremaining: 2m 49s\n\n54:\tlearn: 0.8988524\ttotal: 30m 59s\tremaining: 14m 5s\n\n55:\tlearn: 0.9005588\ttotal: 31m 13s\tremaining: 13m 22s\n\n55:\tlearn: 0.0270931\ttotal: 31m 22s\tremaining: 2m 14s\n\n55:\tlearn: 0.8958296\ttotal: 31m 31s\tremaining: 13m 30s\n\n55:\tlearn: 0.0251492\ttotal: 31m 31s\tremaining: 2m 15s\n\n56:\tlearn: 0.8979021\ttotal: 31m 46s\tremaining: 12m 49s\n\n56:\tlearn: 0.0263689\ttotal: 31m 58s\tremaining: 1m 40s\n\n56:\tlearn: 0.0245115\ttotal: 32m 8s\tremaining: 1m 41s\n\n56:\tlearn: 0.8926761\ttotal: 32m 9s\tremaining: 12m 58s\n\n57:\tlearn: 0.8951979\ttotal: 32m 24s\tremaining: 12m 17s\n\n57:\tlearn: 0.0257282\ttotal: 32m 36s\tremaining: 1m 7s\n\n57:\tlearn: 0.8897888\ttotal: 32m 46s\tremaining: 12m 25s\n\n57:\tlearn: 0.0239305\ttotal: 32m 47s\tremaining: 1m 7s\n\n58:\tlearn: 0.8923522\ttotal: 33m 3s\tremaining: 11m 45s\n\n58:\tlearn: 0.0251003\ttotal: 33m 14s\tremaining: 33.8s\n\n58:\tlearn: 0.0234194\ttotal: 33m 24s\tremaining: 34s\n\n58:\tlearn: 0.8863960\ttotal: 33m 24s\tremaining: 11m 53s\n\n59:\tlearn: 0.8894879\ttotal: 33m 40s\tremaining: 11m 13s\n\n59:\tlearn: 0.0245364\ttotal: 33m 51s\tremaining: 0us\n\n59:\tlearn: 0.0229139\ttotal: 34m 1s\tremaining: 0us\n\n59:\tlearn: 0.8831952\ttotal: 34m 2s\tremaining: 11m 20s\n\n60:\tlearn: 0.8866435\ttotal: 34m 18s\tremaining: 10m 41s\n\n0:\tlearn: 1.0976623\ttotal: 38.3s\tremaining: 1h 22m 14s\n\n60:\tlearn: 0.8801497\ttotal: 34m 40s\tremaining: 10m 48s\n\n0:\tlearn: 1.0974156\ttotal: 38s\tremaining: 1h 21m 36s\n\n61:\tlearn: 0.8835689\ttotal: 34m 55s\tremaining: 10m 8s\n\n1:\tlearn: 1.0964922\ttotal: 1m 16s\tremaining: 1h 21m 9s\n\n61:\tlearn: 0.8770943\ttotal: 35m 18s\tremaining: 10m 15s\n\n1:\tlearn: 1.0962915\ttotal: 1m 16s\tremaining: 1h 21m 22s\n\n62:\tlearn: 0.8807079\ttotal: 35m 33s\tremaining: 9m 35s\n\n2:\tlearn: 1.0955154\ttotal: 1m 53s\tremaining: 1h 20m 7s\n\n62:\tlearn: 0.8745248\ttotal: 35m 56s\tremaining: 9m 41s\n\n2:\tlearn: 1.0951050\ttotal: 1m 53s\tremaining: 1h 19m 55s\n\n63:\tlearn: 0.8777651\ttotal: 36m 11s\tremaining: 9m 2s\n\n3:\tlearn: 1.0945989\ttotal: 2m 31s\tremaining: 1h 19m 26s\n\n63:\tlearn: 0.8711940\ttotal: 36m 33s\tremaining: 9m 8s\n\n3:\tlearn: 1.0939277\ttotal: 2m 31s\tremaining: 1h 19m 23s\n\n64:\tlearn: 0.8747845\ttotal: 36m 49s\tremaining: 8m 29s\n\n4:\tlearn: 1.0936776\ttotal: 3m 9s\tremaining: 1h 18m 50s\n\n64:\tlearn: 0.8679839\ttotal: 37m 10s\tremaining: 8m 34s\n\n4:\tlearn: 1.0927982\ttotal: 3m 9s\tremaining: 1h 18m 51s\n\n65:\tlearn: 0.8721964\ttotal: 37m 26s\tremaining: 7m 56s\n\n5:\tlearn: 1.0927108\ttotal: 3m 46s\tremaining: 1h 18m 3s\n\n65:\tlearn: 0.8649782\ttotal: 37m 48s\tremaining: 8m 1s\n\n5:\tlearn: 1.0916457\ttotal: 3m 47s\tremaining: 1h 18m 19s\n\n66:\tlearn: 0.8693268\ttotal: 38m 3s\tremaining: 7m 23s\n\n6:\tlearn: 1.0917561\ttotal: 4m 24s\tremaining: 1h 17m 25s\n\n66:\tlearn: 0.8621873\ttotal: 38m 25s\tremaining: 7m 27s\n\n6:\tlearn: 1.0905489\ttotal: 4m 24s\tremaining: 1h 17m 36s\n\n67:\tlearn: 0.8663358\ttotal: 38m 41s\tremaining: 6m 49s\n\n7:\tlearn: 1.0906939\ttotal: 5m 2s\tremaining: 1h 16m 45s\n\n67:\tlearn: 0.8592056\ttotal: 39m 3s\tremaining: 6m 53s\n\n7:\tlearn: 1.0891853\ttotal: 5m 1s\tremaining: 1h 16m 44s\n\n68:\tlearn: 0.8634647\ttotal: 39m 17s\tremaining: 6m 15s\n\n8:\tlearn: 1.0897969\ttotal: 5m 39s\tremaining: 1h 15m 58s\n\n68:\tlearn: 0.8566728\ttotal: 39m 39s\tremaining: 6m 19s\n\n8:\tlearn: 1.0881936\ttotal: 5m 38s\tremaining: 1h 15m 51s\n\n69:\tlearn: 0.8610572\ttotal: 39m 53s\tremaining: 5m 41s\n\n9:\tlearn: 1.0888010\ttotal: 6m 15s\tremaining: 1h 15m 7s\n\n69:\tlearn: 0.8537116\ttotal: 40m 16s\tremaining: 5m 45s\n\n9:\tlearn: 1.0869666\ttotal: 6m 15s\tremaining: 1h 15m 3s\n\n70:\tlearn: 0.8582227\ttotal: 40m 28s\tremaining: 5m 7s\n\n10:\tlearn: 1.0875573\ttotal: 6m 51s\tremaining: 1h 14m 10s\n\n70:\tlearn: 0.8508342\ttotal: 40m 54s\tremaining: 5m 11s\n\n10:\tlearn: 1.0857581\ttotal: 6m 52s\tremaining: 1h 14m 18s\n\n71:\tlearn: 0.8552585\ttotal: 41m 5s\tremaining: 4m 33s\n\n11:\tlearn: 1.0866717\ttotal: 7m 26s\tremaining: 1h 13m 9s\n\n71:\tlearn: 0.8482548\ttotal: 41m 30s\tremaining: 4m 36s\n\n11:\tlearn: 1.0847542\ttotal: 7m 28s\tremaining: 1h 13m 29s\n\n72:\tlearn: 0.8525821\ttotal: 41m 40s\tremaining: 3m 59s\n\n12:\tlearn: 1.0857804\ttotal: 8m 4s\tremaining: 1h 12m 38s\n\n72:\tlearn: 0.8456273\ttotal: 42m 7s\tremaining: 4m 2s\n\n12:\tlearn: 1.0835115\ttotal: 8m 5s\tremaining: 1h 12m 53s\n\n73:\tlearn: 0.8498052\ttotal: 42m 17s\tremaining: 3m 25s\n\n13:\tlearn: 1.0848219\ttotal: 8m 40s\tremaining: 1h 11m 56s\n\n73:\tlearn: 0.8428867\ttotal: 42m 43s\tremaining: 3m 27s\n\n13:\tlearn: 1.0824701\ttotal: 8m 42s\tremaining: 1h 12m 11s\n\n74:\tlearn: 0.8471161\ttotal: 42m 53s\tremaining: 2m 51s\n\n14:\tlearn: 1.0839473\ttotal: 9m 17s\tremaining: 1h 11m 10s\n\n74:\tlearn: 0.8399447\ttotal: 43m 19s\tremaining: 2m 53s\n\n14:\tlearn: 1.0814376\ttotal: 9m 18s\tremaining: 1h 11m 24s\n\n75:\tlearn: 0.8443308\ttotal: 43m 29s\tremaining: 2m 17s\n\n15:\tlearn: 1.0830892\ttotal: 9m 53s\tremaining: 1h 10m 25s\n\n75:\tlearn: 0.8371754\ttotal: 43m 57s\tremaining: 2m 18s\n\n15:\tlearn: 1.0803995\ttotal: 9m 55s\tremaining: 1h 10m 46s\n\n76:\tlearn: 0.8413136\ttotal: 44m 5s\tremaining: 1m 43s\n\n16:\tlearn: 1.0822067\ttotal: 10m 29s\tremaining: 1h 9m 47s\n\n76:\tlearn: 0.8343368\ttotal: 44m 33s\tremaining: 1m 44s\n\n16:\tlearn: 1.0794470\ttotal: 10m 32s\tremaining: 1h 10m 3s\n\n77:\tlearn: 0.8385617\ttotal: 44m 42s\tremaining: 1m 8s\n\n17:\tlearn: 1.0812563\ttotal: 11m 7s\tremaining: 1h 9m 11s\n\n77:\tlearn: 0.8320896\ttotal: 45m 10s\tremaining: 1m 9s\n\n17:\tlearn: 1.0784040\ttotal: 11m 9s\tremaining: 1h 9m 25s\n\n78:\tlearn: 0.8362050\ttotal: 45m 18s\tremaining: 34.4s\n\n18:\tlearn: 1.0801953\ttotal: 11m 43s\tremaining: 1h 8m 29s\n\n78:\tlearn: 0.8290380\ttotal: 45m 46s\tremaining: 34.8s\n\n18:\tlearn: 1.0773560\ttotal: 11m 46s\tremaining: 1h 8m 47s\n\n79:\tlearn: 0.8336696\ttotal: 45m 55s\tremaining: 0us\n\n19:\tlearn: 1.0791123\ttotal: 12m 16s\tremaining: 1h 7m 30s\n\n79:\tlearn: 0.8267697\ttotal: 46m 15s\tremaining: 0us\n\n19:\tlearn: 1.0760886\ttotal: 12m 12s\tremaining: 1h 7m 10s\n\n20:\tlearn: 1.0782012\ttotal: 12m 36s\tremaining: 1h 5m 26s\n\n20:\tlearn: 1.0749497\ttotal: 12m 30s\tremaining: 1h 4m 56s\n\n21:\tlearn: 1.0772784\ttotal: 12m 54s\tremaining: 1h 3m 22s\n\n21:\tlearn: 1.0738017\ttotal: 12m 48s\tremaining: 1h 2m 52s\n\n22:\tlearn: 1.0763549\ttotal: 13m 12s\tremaining: 1h 1m 26s\n\n22:\tlearn: 1.0727362\ttotal: 13m 6s\tremaining: 1h 58s\n\n23:\tlearn: 1.0754273\ttotal: 13m 30s\tremaining: 59m 38s\n\n23:\tlearn: 1.0718186\ttotal: 13m 24s\tremaining: 59m 12s\n\n24:\tlearn: 1.0745851\ttotal: 13m 48s\tremaining: 57m 58s\n\n24:\tlearn: 1.0708718\ttotal: 13m 42s\tremaining: 57m 34s\n\n25:\tlearn: 1.0734949\ttotal: 14m 5s\tremaining: 56m 22s\n\n25:\tlearn: 1.0697536\ttotal: 14m\tremaining: 56m 1s\n\n26:\tlearn: 1.0725708\ttotal: 14m 22s\tremaining: 54m 50s\n\n26:\tlearn: 1.0688356\ttotal: 14m 17s\tremaining: 54m 31s\n\n27:\tlearn: 1.0714700\ttotal: 14m 40s\tremaining: 53m 26s\n\n27:\tlearn: 1.0677924\ttotal: 14m 35s\tremaining: 53m 9s\n\n28:\tlearn: 1.0705073\ttotal: 14m 58s\tremaining: 52m 8s\n\n28:\tlearn: 1.0667413\ttotal: 14m 53s\tremaining: 51m 51s\n\n29:\tlearn: 1.0696016\ttotal: 15m 15s\tremaining: 50m 53s\n\n29:\tlearn: 1.0655919\ttotal: 15m 11s\tremaining: 50m 38s\n\n30:\tlearn: 1.0686456\ttotal: 15m 33s\tremaining: 49m 41s\n\n30:\tlearn: 1.0645891\ttotal: 15m 29s\tremaining: 49m 27s\n\n31:\tlearn: 1.0676053\ttotal: 15m 51s\tremaining: 48m 32s\n\n31:\tlearn: 1.0635128\ttotal: 15m 47s\tremaining: 48m 21s\n\n32:\tlearn: 1.0667613\ttotal: 16m 7s\tremaining: 47m 23s\n\n32:\tlearn: 1.0624479\ttotal: 16m 3s\tremaining: 47m 11s\n\n33:\tlearn: 1.0657624\ttotal: 16m 24s\tremaining: 46m 18s\n\n33:\tlearn: 1.0614178\ttotal: 16m 18s\tremaining: 46m 3s\n\n34:\tlearn: 1.0647893\ttotal: 16m 40s\tremaining: 45m 15s\n\n34:\tlearn: 1.0603387\ttotal: 16m 34s\tremaining: 44m 58s\n\n35:\tlearn: 1.0638020\ttotal: 16m 57s\tremaining: 44m 15s\n\n35:\tlearn: 1.0593269\ttotal: 16m 51s\tremaining: 44m 1s\n\n36:\tlearn: 1.0629142\ttotal: 17m 13s\tremaining: 43m 18s\n\n36:\tlearn: 1.0581482\ttotal: 17m 7s\tremaining: 43m 3s\n\n37:\tlearn: 1.0618659\ttotal: 17m 30s\tremaining: 42m 23s\n\n37:\tlearn: 1.0571552\ttotal: 17m 24s\tremaining: 42m 9s\n\n38:\tlearn: 1.0610334\ttotal: 17m 46s\tremaining: 41m 29s\n\n38:\tlearn: 1.0562039\ttotal: 17m 41s\tremaining: 41m 16s\n\n39:\tlearn: 1.0600717\ttotal: 18m 1s\tremaining: 40m 34s\n\n39:\tlearn: 1.0550918\ttotal: 17m 58s\tremaining: 40m 25s\n\n40:\tlearn: 1.0590892\ttotal: 18m 18s\tremaining: 39m 43s\n\n40:\tlearn: 1.0539197\ttotal: 18m 14s\tremaining: 39m 36s\n\n41:\tlearn: 1.0581341\ttotal: 18m 34s\tremaining: 38m 55s\n\n41:\tlearn: 1.0529748\ttotal: 18m 30s\tremaining: 38m 47s\n\n42:\tlearn: 1.0571210\ttotal: 18m 51s\tremaining: 38m 9s\n\n42:\tlearn: 1.0518204\ttotal: 18m 46s\tremaining: 37m 58s\n\n43:\tlearn: 1.0560603\ttotal: 19m 8s\tremaining: 37m 23s\n\n43:\tlearn: 1.0507845\ttotal: 19m 1s\tremaining: 37m 11s\n\n44:\tlearn: 1.0551841\ttotal: 19m 24s\tremaining: 36m 40s\n\n44:\tlearn: 1.0496169\ttotal: 19m 18s\tremaining: 36m 28s\n\n45:\tlearn: 1.0541889\ttotal: 19m 41s\tremaining: 35m 57s\n\n45:\tlearn: 1.0486005\ttotal: 19m 35s\tremaining: 35m 45s\n\n46:\tlearn: 1.0532017\ttotal: 19m 56s\tremaining: 35m 13s\n\n46:\tlearn: 1.0476829\ttotal: 19m 51s\tremaining: 35m 3s\n\n47:\tlearn: 1.0522461\ttotal: 20m 12s\tremaining: 34m 30s\n\n47:\tlearn: 1.0466927\ttotal: 20m 7s\tremaining: 34m 22s\n\n48:\tlearn: 1.0513442\ttotal: 20m 27s\tremaining: 33m 48s\n\n48:\tlearn: 1.0457069\ttotal: 20m 23s\tremaining: 33m 41s\n\n49:\tlearn: 1.0504636\ttotal: 20m 43s\tremaining: 33m 9s\n\n49:\tlearn: 1.0447440\ttotal: 20m 38s\tremaining: 33m 1s\n\n50:\tlearn: 1.0496753\ttotal: 21m\tremaining: 32m 32s\n\n50:\tlearn: 1.0437912\ttotal: 20m 53s\tremaining: 32m 21s\n\n51:\tlearn: 1.0484615\ttotal: 21m 16s\tremaining: 31m 54s\n\n51:\tlearn: 1.0426194\ttotal: 21m 9s\tremaining: 31m 44s\n\n52:\tlearn: 1.0476250\ttotal: 21m 32s\tremaining: 31m 17s\n\n52:\tlearn: 1.0416462\ttotal: 21m 25s\tremaining: 31m 7s\n\n53:\tlearn: 1.0465580\ttotal: 21m 47s\tremaining: 30m 40s\n\n53:\tlearn: 1.0406070\ttotal: 21m 41s\tremaining: 30m 32s\n\n54:\tlearn: 1.0455791\ttotal: 22m 2s\tremaining: 30m 3s\n\n54:\tlearn: 1.0396165\ttotal: 21m 57s\tremaining: 29m 56s\n\n55:\tlearn: 1.0445552\ttotal: 22m 17s\tremaining: 29m 27s\n\n55:\tlearn: 1.0384760\ttotal: 22m 13s\tremaining: 29m 21s\n\n56:\tlearn: 1.0436061\ttotal: 22m 33s\tremaining: 28m 53s\n\n56:\tlearn: 1.0373274\ttotal: 22m 27s\tremaining: 28m 46s\n\n57:\tlearn: 1.0427161\ttotal: 22m 49s\tremaining: 28m 20s\n\n57:\tlearn: 1.0363724\ttotal: 22m 43s\tremaining: 28m 12s\n\n58:\tlearn: 1.0418012\ttotal: 23m 6s\tremaining: 27m 47s\n\n58:\tlearn: 1.0352520\ttotal: 22m 59s\tremaining: 27m 40s\n\n59:\tlearn: 1.0406957\ttotal: 23m 22s\tremaining: 27m 15s\n\n59:\tlearn: 1.0343005\ttotal: 23m 15s\tremaining: 27m 8s\n\n60:\tlearn: 1.0398706\ttotal: 23m 37s\tremaining: 26m 43s\n\n60:\tlearn: 1.0332908\ttotal: 23m 32s\tremaining: 26m 37s\n\n61:\tlearn: 1.0387690\ttotal: 23m 51s\tremaining: 26m 10s\n\n61:\tlearn: 1.0323001\ttotal: 23m 48s\tremaining: 26m 6s\n\n62:\tlearn: 1.0379120\ttotal: 24m 7s\tremaining: 25m 39s\n\n62:\tlearn: 1.0311263\ttotal: 24m 3s\tremaining: 25m 35s\n\n63:\tlearn: 1.0369167\ttotal: 24m 24s\tremaining: 25m 9s\n\n63:\tlearn: 1.0300470\ttotal: 24m 19s\tremaining: 25m 5s\n\n64:\tlearn: 1.0359922\ttotal: 24m 41s\tremaining: 24m 41s\n\n64:\tlearn: 1.0288752\ttotal: 24m 35s\tremaining: 24m 35s\n\n65:\tlearn: 1.0351212\ttotal: 24m 58s\tremaining: 24m 13s\n\n65:\tlearn: 1.0278943\ttotal: 24m 52s\tremaining: 24m 7s\n\n66:\tlearn: 1.0342323\ttotal: 25m 15s\tremaining: 23m 44s\n\n66:\tlearn: 1.0269892\ttotal: 25m 8s\tremaining: 23m 38s\n\n67:\tlearn: 1.0333062\ttotal: 25m 31s\tremaining: 23m 16s\n\n67:\tlearn: 1.0259591\ttotal: 25m 25s\tremaining: 23m 11s\n\n68:\tlearn: 1.0323267\ttotal: 25m 47s\tremaining: 22m 48s\n\n68:\tlearn: 1.0248124\ttotal: 25m 42s\tremaining: 22m 43s\n\n69:\tlearn: 1.0312960\ttotal: 26m 3s\tremaining: 22m 19s\n\n69:\tlearn: 1.0239410\ttotal: 25m 58s\tremaining: 22m 16s\n\n70:\tlearn: 1.0303739\ttotal: 26m 19s\tremaining: 21m 52s\n\n70:\tlearn: 1.0229316\ttotal: 26m 15s\tremaining: 21m 48s\n\n71:\tlearn: 1.0294471\ttotal: 26m 36s\tremaining: 21m 25s\n\n71:\tlearn: 1.0216574\ttotal: 26m 31s\tremaining: 21m 21s\n\n72:\tlearn: 1.0284959\ttotal: 26m 53s\tremaining: 20m 59s\n\n72:\tlearn: 1.0205193\ttotal: 26m 47s\tremaining: 20m 55s\n\n73:\tlearn: 1.0276281\ttotal: 27m 10s\tremaining: 20m 33s\n\n73:\tlearn: 1.0196264\ttotal: 27m 4s\tremaining: 20m 28s\n\n74:\tlearn: 1.0268140\ttotal: 27m 27s\tremaining: 20m 7s\n\n74:\tlearn: 1.0186647\ttotal: 27m 21s\tremaining: 20m 3s\n\n75:\tlearn: 1.0259669\ttotal: 27m 44s\tremaining: 19m 42s\n\n75:\tlearn: 1.0177327\ttotal: 27m 38s\tremaining: 19m 38s\n\n76:\tlearn: 1.0250313\ttotal: 28m 1s\tremaining: 19m 17s\n\n76:\tlearn: 1.0166917\ttotal: 27m 55s\tremaining: 19m 13s\n\n77:\tlearn: 1.0240959\ttotal: 28m 16s\tremaining: 18m 50s\n\n77:\tlearn: 1.0157102\ttotal: 28m 11s\tremaining: 18m 47s\n\n78:\tlearn: 1.0231218\ttotal: 28m 31s\tremaining: 18m 24s\n\n78:\tlearn: 1.0147300\ttotal: 28m 28s\tremaining: 18m 22s\n\n79:\tlearn: 1.0223489\ttotal: 28m 47s\tremaining: 17m 59s\n\n79:\tlearn: 1.0138761\ttotal: 28m 44s\tremaining: 17m 57s\n\n80:\tlearn: 1.0214574\ttotal: 29m 3s\tremaining: 17m 34s\n\n80:\tlearn: 1.0129666\ttotal: 28m 59s\tremaining: 17m 32s\n\n81:\tlearn: 1.0206751\ttotal: 29m 20s\tremaining: 17m 10s\n\n81:\tlearn: 1.0120319\ttotal: 29m 16s\tremaining: 17m 8s\n\n82:\tlearn: 1.0198346\ttotal: 29m 40s\tremaining: 16m 48s\n\n82:\tlearn: 1.0111217\ttotal: 29m 38s\tremaining: 16m 47s\n\n83:\tlearn: 1.0189952\ttotal: 30m 4s\tremaining: 16m 27s\n\n83:\tlearn: 1.0102105\ttotal: 30m 1s\tremaining: 16m 26s\n\n84:\tlearn: 1.0181037\ttotal: 30m 24s\tremaining: 16m 5s\n\n84:\tlearn: 1.0092748\ttotal: 30m 21s\tremaining: 16m 4s\n\n85:\tlearn: 1.0171152\ttotal: 30m 48s\tremaining: 15m 45s\n\n85:\tlearn: 1.0084013\ttotal: 30m 44s\tremaining: 15m 43s\n\n86:\tlearn: 1.0163359\ttotal: 31m 9s\tremaining: 15m 24s\n\n86:\tlearn: 1.0075141\ttotal: 31m 5s\tremaining: 15m 21s\n\n87:\tlearn: 1.0154746\ttotal: 31m 28s\tremaining: 15m 1s\n\n87:\tlearn: 1.0065390\ttotal: 31m 23s\tremaining: 14m 58s\n\n88:\tlearn: 1.0146364\ttotal: 31m 46s\tremaining: 14m 38s\n\n88:\tlearn: 1.0056345\ttotal: 31m 42s\tremaining: 14m 36s\n\n89:\tlearn: 1.0137129\ttotal: 32m 7s\tremaining: 14m 16s\n\n89:\tlearn: 1.0047692\ttotal: 32m 4s\tremaining: 14m 15s\n\n90:\tlearn: 1.0128190\ttotal: 32m 29s\tremaining: 13m 55s\n\n90:\tlearn: 1.0038061\ttotal: 32m 25s\tremaining: 13m 53s\n\n91:\tlearn: 1.0118496\ttotal: 32m 47s\tremaining: 13m 32s\n\n91:\tlearn: 1.0029509\ttotal: 32m 43s\tremaining: 13m 30s\n\n92:\tlearn: 1.0108916\ttotal: 33m 5s\tremaining: 13m 9s\n\n92:\tlearn: 1.0018662\ttotal: 33m\tremaining: 13m 7s\n\n93:\tlearn: 1.0099691\ttotal: 33m 23s\tremaining: 12m 47s\n\n93:\tlearn: 1.0011037\ttotal: 33m 18s\tremaining: 12m 45s\n\n94:\tlearn: 1.0089482\ttotal: 33m 40s\tremaining: 12m 24s\n\n94:\tlearn: 1.0001795\ttotal: 33m 36s\tremaining: 12m 22s\n\n95:\tlearn: 1.0081121\ttotal: 33m 58s\tremaining: 12m 2s\n\n95:\tlearn: 0.9993091\ttotal: 33m 54s\tremaining: 12m\n\n96:\tlearn: 1.0072896\ttotal: 34m 17s\tremaining: 11m 39s\n\n96:\tlearn: 0.9983255\ttotal: 34m 12s\tremaining: 11m 38s\n\n97:\tlearn: 1.0063287\ttotal: 34m 35s\tremaining: 11m 17s\n\n97:\tlearn: 0.9974656\ttotal: 34m 30s\tremaining: 11m 15s\n\n98:\tlearn: 1.0054872\ttotal: 34m 53s\tremaining: 10m 55s\n\n98:\tlearn: 0.9966521\ttotal: 34m 48s\tremaining: 10m 53s\n\n99:\tlearn: 1.0045962\ttotal: 35m 11s\tremaining: 10m 33s\n\n99:\tlearn: 0.9958121\ttotal: 35m 5s\tremaining: 10m 31s\n\n100:\tlearn: 1.0036853\ttotal: 35m 29s\tremaining: 10m 11s\n\n100:\tlearn: 0.9948905\ttotal: 35m 23s\tremaining: 10m 9s\n\n101:\tlearn: 1.0029130\ttotal: 35m 46s\tremaining: 9m 49s\n\n101:\tlearn: 0.9939114\ttotal: 35m 41s\tremaining: 9m 47s\n\n102:\tlearn: 1.0020762\ttotal: 36m 2s\tremaining: 9m 26s\n\n102:\tlearn: 0.9930779\ttotal: 35m 57s\tremaining: 9m 25s\n\n103:\tlearn: 1.0011859\ttotal: 36m 16s\tremaining: 9m 4s\n\n103:\tlearn: 0.9922446\ttotal: 36m 14s\tremaining: 9m 3s\n\n104:\tlearn: 1.0003800\ttotal: 36m 34s\tremaining: 8m 42s\n\n104:\tlearn: 0.9913468\ttotal: 36m 30s\tremaining: 8m 41s\n\n105:\tlearn: 0.9995407\ttotal: 36m 50s\tremaining: 8m 20s\n\n105:\tlearn: 0.9903763\ttotal: 36m 46s\tremaining: 8m 19s\n\n106:\tlearn: 0.9987584\ttotal: 37m 7s\tremaining: 7m 58s\n\n106:\tlearn: 0.9893996\ttotal: 37m 1s\tremaining: 7m 57s\n\n107:\tlearn: 0.9978928\ttotal: 37m 24s\tremaining: 7m 37s\n\n107:\tlearn: 0.9884309\ttotal: 37m 18s\tremaining: 7m 36s\n\n108:\tlearn: 0.9970113\ttotal: 37m 41s\tremaining: 7m 15s\n\n108:\tlearn: 0.9875805\ttotal: 37m 36s\tremaining: 7m 14s\n\n109:\tlearn: 0.9961808\ttotal: 37m 58s\tremaining: 6m 54s\n\n109:\tlearn: 0.9866192\ttotal: 37m 53s\tremaining: 6m 53s\n\n110:\tlearn: 0.9952985\ttotal: 38m 14s\tremaining: 6m 32s\n\n110:\tlearn: 0.9856288\ttotal: 38m 10s\tremaining: 6m 32s\n\n111:\tlearn: 0.9944103\ttotal: 38m 30s\tremaining: 6m 11s\n\n111:\tlearn: 0.9848361\ttotal: 38m 28s\tremaining: 6m 10s\n\n112:\tlearn: 0.9935983\ttotal: 38m 47s\tremaining: 5m 50s\n\n112:\tlearn: 0.9839654\ttotal: 38m 45s\tremaining: 5m 49s\n\n113:\tlearn: 0.9927804\ttotal: 39m 3s\tremaining: 5m 28s\n\n113:\tlearn: 0.9829547\ttotal: 39m 1s\tremaining: 5m 28s\n\n114:\tlearn: 0.9919706\ttotal: 39m 22s\tremaining: 5m 8s\n\n114:\tlearn: 0.9820136\ttotal: 39m 22s\tremaining: 5m 8s\n\n115:\tlearn: 0.9911460\ttotal: 39m 46s\tremaining: 4m 47s\n\n115:\tlearn: 0.9812429\ttotal: 39m 44s\tremaining: 4m 47s\n\n116:\tlearn: 0.9903940\ttotal: 40m 6s\tremaining: 4m 27s\n\n116:\tlearn: 0.9804620\ttotal: 40m 4s\tremaining: 4m 27s\n\n117:\tlearn: 0.9895948\ttotal: 40m 23s\tremaining: 4m 6s\n\n117:\tlearn: 0.9796269\ttotal: 40m 20s\tremaining: 4m 6s\n\n118:\tlearn: 0.9888813\ttotal: 40m 38s\tremaining: 3m 45s\n\n118:\tlearn: 0.9787972\ttotal: 40m 37s\tremaining: 3m 45s\n\n119:\tlearn: 0.9880279\ttotal: 40m 54s\tremaining: 3m 24s\n\n119:\tlearn: 0.9780208\ttotal: 40m 52s\tremaining: 3m 24s\n\n120:\tlearn: 0.9872316\ttotal: 41m 11s\tremaining: 3m 3s\n\n120:\tlearn: 0.9770930\ttotal: 41m 7s\tremaining: 3m 3s\n\n121:\tlearn: 0.9864992\ttotal: 41m 28s\tremaining: 2m 43s\n\n121:\tlearn: 0.9761071\ttotal: 41m 23s\tremaining: 2m 42s\n\n122:\tlearn: 0.9856666\ttotal: 41m 47s\tremaining: 2m 22s\n\n122:\tlearn: 0.9751419\ttotal: 41m 44s\tremaining: 2m 22s\n\n123:\tlearn: 0.9848027\ttotal: 42m 8s\tremaining: 2m 2s\n\n123:\tlearn: 0.9742549\ttotal: 42m 5s\tremaining: 2m 2s\n\n124:\tlearn: 0.9838202\ttotal: 42m 26s\tremaining: 1m 41s\n\n124:\tlearn: 0.9734155\ttotal: 42m 21s\tremaining: 1m 41s\n\n125:\tlearn: 0.9830020\ttotal: 42m 44s\tremaining: 1m 21s\n\n125:\tlearn: 0.9724353\ttotal: 42m 39s\tremaining: 1m 21s\n\n126:\tlearn: 0.9822282\ttotal: 43m 1s\tremaining: 1m\n\n126:\tlearn: 0.9714971\ttotal: 42m 56s\tremaining: 1m\n\n127:\tlearn: 0.9814874\ttotal: 43m 18s\tremaining: 40.6s\n\n127:\tlearn: 0.9706030\ttotal: 43m 14s\tremaining: 40.5s\n\n128:\tlearn: 0.9807170\ttotal: 43m 34s\tremaining: 20.3s\n\n128:\tlearn: 0.9699277\ttotal: 43m 31s\tremaining: 20.2s\n\n129:\tlearn: 0.9798157\ttotal: 43m 51s\tremaining: 0us\n\n129:\tlearn: 0.9689145\ttotal: 43m 45s\tremaining: 0us\n\n0:\tlearn: 1.0947940\ttotal: 6.7s\tremaining: 8m 49s\n\n1:\tlearn: 1.0907381\ttotal: 16.4s\tremaining: 10m 37s\n\n2:\tlearn: 1.0866868\ttotal: 23.1s\tremaining: 9m 52s\n\n3:\tlearn: 1.0825997\ttotal: 32.7s\tremaining: 10m 21s\n\n4:\tlearn: 1.0791704\ttotal: 39.3s\tremaining: 9m 49s\n\n5:\tlearn: 1.0753239\ttotal: 49.7s\tremaining: 10m 12s\n\n6:\tlearn: 1.0714206\ttotal: 56.8s\tremaining: 9m 51s\n\n7:\tlearn: 1.0676527\ttotal: 1m 6s\tremaining: 9m 57s\n\n8:\tlearn: 1.0637087\ttotal: 1m 13s\tremaining: 9m 36s\n\n9:\tlearn: 1.0599453\ttotal: 1m 22s\tremaining: 9m 40s\n\n10:\tlearn: 1.0558763\ttotal: 1m 29s\tremaining: 9m 22s\n\n11:\tlearn: 1.0523771\ttotal: 1m 39s\tremaining: 9m 22s\n\n12:\tlearn: 1.0490216\ttotal: 1m 46s\tremaining: 9m 6s\n\n13:\tlearn: 1.0457616\ttotal: 1m 55s\tremaining: 9m 4s\n\n14:\tlearn: 1.0422229\ttotal: 2m 2s\tremaining: 8m 49s\n\n15:\tlearn: 1.0391112\ttotal: 2m 11s\tremaining: 8m 47s\n\n16:\tlearn: 1.0356193\ttotal: 2m 18s\tremaining: 8m 34s\n\n17:\tlearn: 1.0321733\ttotal: 2m 28s\tremaining: 8m 31s\n\n18:\tlearn: 1.0288038\ttotal: 2m 35s\tremaining: 8m 18s\n\n19:\tlearn: 1.0256823\ttotal: 2m 45s\tremaining: 8m 14s\n\n20:\tlearn: 1.0224478\ttotal: 2m 52s\tremaining: 8m 3s\n\n21:\tlearn: 1.0194783\ttotal: 3m 2s\tremaining: 8m\n\n22:\tlearn: 1.0161738\ttotal: 3m 10s\tremaining: 7m 52s\n\n23:\tlearn: 1.0131003\ttotal: 3m 21s\tremaining: 7m 49s\n\n24:\tlearn: 1.0096142\ttotal: 3m 27s\tremaining: 7m 37s\n\n25:\tlearn: 1.0058499\ttotal: 3m 37s\tremaining: 7m 31s\n\n26:\tlearn: 1.0025147\ttotal: 3m 44s\tremaining: 7m 21s\n\n27:\tlearn: 0.9991216\ttotal: 3m 54s\tremaining: 7m 15s\n\n28:\tlearn: 0.9962951\ttotal: 4m 2s\tremaining: 7m 5s\n\n29:\tlearn: 0.9931367\ttotal: 4m 12s\tremaining: 7m\n\n30:\tlearn: 0.9901912\ttotal: 4m 19s\tremaining: 6m 49s\n\n31:\tlearn: 0.9871634\ttotal: 4m 28s\tremaining: 6m 43s\n\n32:\tlearn: 0.9841816\ttotal: 4m 35s\tremaining: 6m 32s\n\n33:\tlearn: 0.9810926\ttotal: 4m 45s\tremaining: 6m 26s\n\n34:\tlearn: 0.9782475\ttotal: 4m 52s\tremaining: 6m 15s\n\n35:\tlearn: 0.9751842\ttotal: 5m 1s\tremaining: 6m 8s\n\n36:\tlearn: 0.9724723\ttotal: 5m 8s\tremaining: 5m 58s\n\n37:\tlearn: 0.9695934\ttotal: 5m 18s\tremaining: 5m 52s\n\n38:\tlearn: 0.9667575\ttotal: 5m 25s\tremaining: 5m 42s\n\n39:\tlearn: 0.9633022\ttotal: 5m 35s\tremaining: 5m 35s\n\n40:\tlearn: 0.9600917\ttotal: 5m 42s\tremaining: 5m 25s\n\n41:\tlearn: 0.9577002\ttotal: 5m 51s\tremaining: 5m 18s\n\n42:\tlearn: 0.9546078\ttotal: 5m 58s\tremaining: 5m 8s\n\n43:\tlearn: 0.9514297\ttotal: 6m 8s\tremaining: 5m 1s\n\n44:\tlearn: 0.9487239\ttotal: 6m 14s\tremaining: 4m 51s\n\n45:\tlearn: 0.9457779\ttotal: 6m 24s\tremaining: 4m 44s\n\n46:\tlearn: 0.9426140\ttotal: 6m 31s\tremaining: 4m 34s\n\n47:\tlearn: 0.9396211\ttotal: 6m 40s\tremaining: 4m 27s\n\n48:\tlearn: 0.9365837\ttotal: 6m 47s\tremaining: 4m 17s\n\n49:\tlearn: 0.9332671\ttotal: 6m 57s\tremaining: 4m 10s\n\n50:\tlearn: 0.9305193\ttotal: 7m 4s\tremaining: 4m 1s\n\n51:\tlearn: 0.9276024\ttotal: 7m 13s\tremaining: 3m 53s\n\n52:\tlearn: 0.9249584\ttotal: 7m 20s\tremaining: 3m 44s\n\n53:\tlearn: 0.9218830\ttotal: 7m 30s\tremaining: 3m 36s\n\n54:\tlearn: 0.9188469\ttotal: 7m 36s\tremaining: 3m 27s\n\n55:\tlearn: 0.9159885\ttotal: 7m 46s\tremaining: 3m 20s\n\n56:\tlearn: 0.9135076\ttotal: 7m 53s\tremaining: 3m 11s\n\n57:\tlearn: 0.9109201\ttotal: 8m 2s\tremaining: 3m 3s\n\n58:\tlearn: 0.9086382\ttotal: 8m 9s\tremaining: 2m 54s\n\n59:\tlearn: 0.9059495\ttotal: 8m 19s\tremaining: 2m 46s\n\n60:\tlearn: 0.9034321\ttotal: 8m 26s\tremaining: 2m 37s\n\n61:\tlearn: 0.9008683\ttotal: 8m 35s\tremaining: 2m 29s\n\n62:\tlearn: 0.8983719\ttotal: 8m 43s\tremaining: 2m 21s\n\n63:\tlearn: 0.8957180\ttotal: 8m 52s\tremaining: 2m 13s\n\n64:\tlearn: 0.8928742\ttotal: 8m 59s\tremaining: 2m 4s\n\n65:\tlearn: 0.8906241\ttotal: 9m 9s\tremaining: 1m 56s\n\n66:\tlearn: 0.8880505\ttotal: 9m 16s\tremaining: 1m 47s\n\n67:\tlearn: 0.8851997\ttotal: 9m 25s\tremaining: 1m 39s\n\n68:\tlearn: 0.8828514\ttotal: 9m 32s\tremaining: 1m 31s\n\n69:\tlearn: 0.8803847\ttotal: 9m 42s\tremaining: 1m 23s\n\n70:\tlearn: 0.8775849\ttotal: 9m 49s\tremaining: 1m 14s\n\n71:\tlearn: 0.8752171\ttotal: 9m 59s\tremaining: 1m 6s\n\n72:\tlearn: 0.8725342\ttotal: 10m 6s\tremaining: 58.1s\n\n73:\tlearn: 0.8699697\ttotal: 10m 15s\tremaining: 49.9s\n\n74:\tlearn: 0.8678104\ttotal: 10m 23s\tremaining: 41.6s\n\n75:\tlearn: 0.8654527\ttotal: 10m 32s\tremaining: 33.3s\n\n76:\tlearn: 0.8630155\ttotal: 10m 40s\tremaining: 24.9s\n\n77:\tlearn: 0.8604605\ttotal: 10m 49s\tremaining: 16.7s\n\n78:\tlearn: 0.8584376\ttotal: 10m 56s\tremaining: 8.3s\n\n79:\tlearn: 0.8562305\ttotal: 11m 5s\tremaining: 0us\n\nBest params for cat:\n\n{'n_estimators': 80, 'max_depth': 10, 'loss_function': 'MultiClass', 'learning_rate': 0.01, 'l2_leaf_reg': 1}\n\nClassification report for cat:\n\n              precision    recall  f1-score   support\n\n\n\n        -1.0       0.53      0.07      0.13       125\n\n         0.0       0.65      0.67      0.66       257\n\n         1.0       0.50      0.73      0.59       218\n\n\n\n    accuracy                           0.57       600\n\n   macro avg       0.56      0.49      0.46       600\n\nweighted avg       0.57      0.57      0.53       600\n\n\n"}]},{"cell_type":"code","source":"#RF+KNN+MLP\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom joblib import parallel_backend\n\n'''\nThis code defines a stacking ensemble using RandomForestClassifier, KNeighborsClassifier, and MLPClassifier \nas base models, and MLPClassifier as the final estimator. It then uses a GridSearchCV object to \nsearch over a parameter grid for the best hyperparameters. Finally, it prints the classification report for the predictions made on the test set.\n'''\n\n# define base models\nmodel1 = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = MLPClassifier(hidden_layer_sizes=(32, 32), random_state=42)\n\n# define stacking ensemble\nestimators = [('rf', model1),\n              ('knn', model2),\n              ('mlp', model3)]\n\nclf = StackingClassifier(estimators=estimators, \n                         final_estimator=MLPClassifier(hidden_layer_sizes=(64, 64),\n                                                       random_state=42))\n\n\n# define parameter grid for randomized search\nparam_dist = {'rf__max_depth': [20,40,60],\n              'rf__max_features': ['sqrt'],\n              'rf__min_samples_leaf': [1, 2, 5, 10],\n              'rf__min_samples_split': [2, 5, 10, 15],\n              'knn__n_neighbors': [3, 5, 7],\n              'mlp__alpha': [0.001, 0.01, 0.1, 1],\n              'mlp__learning_rate_init': [0.001, 0.01, 0.1],\n              'final_estimator__alpha': [0.001, 0.01, 0.1, 1],\n              'final_estimator__learning_rate_init': [0.001, 0.01, 0.1]}\n\n# define RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=clf, \n                             param_distributions=param_dist, \n                             cv=3, \n                             n_jobs=-1, \n                             verbose=2)\n\n# fit the RandomizedSearchCV object to the data\nwith parallel_backend('multiprocessing'):\n    random_search.fit(x_train, y_train)\n\n# make predictions on test set\ny_pred = random_search.predict(x_test)\n\n# print classification report\nprint(classification_report(y_test, y_pred))\n","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"nMPQuCWuaPpu","outputId":"c7894cab-7362-48cb-9ef5-282036071118","execution":{"iopub.status.busy":"2023-05-15T17:27:56.649076Z","iopub.execute_input":"2023-05-15T17:27:56.649831Z","iopub.status.idle":"2023-05-15T17:39:52.434206Z","shell.execute_reply.started":"2023-05-15T17:27:56.649777Z","shell.execute_reply":"2023-05-15T17:39:52.432410Z"},"scrolled":true,"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nFitting 3 folds for each of 10 candidates, totalling 30 fits\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.01, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=10; total time=  37.6s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.01, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=10; total time=  37.7s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=10; total time=  39.3s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.01, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=10; total time=  44.0s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=10; total time=  46.5s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=10; total time=  44.8s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=2; total time=  37.9s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=2; total time=  39.0s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=2; total time=  31.0s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=5; total time=  35.5s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=5; total time=  37.8s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=5; total time=  39.7s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=5; total time=  40.2s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=5; total time=  41.6s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.1, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=5; total time=  40.8s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=10; total time=  51.3s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=10; total time=  54.8s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=10; total time= 1.1min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=5; total time=  42.9s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=5; total time=  35.0s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=5; total time=  35.1s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.1, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=5; total time=  41.6s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.1, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=5; total time=  41.6s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.1, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=5; total time=  43.5s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.01, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2; total time=  47.1s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.01, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2; total time=  43.1s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.01, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=2; total time=  43.4s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.01, mlp__learning_rate_init=0.001, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=2; total time= 1.2min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.01, mlp__learning_rate_init=0.001, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=2; total time= 1.2min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.01, mlp__learning_rate_init=0.001, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=2; total time=  41.4s\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n              precision    recall  f1-score   support\n\n        -1.0       0.56      0.31      0.40       143\n         0.0       0.64      0.83      0.72       247\n         1.0       0.61      0.59      0.60       210\n\n    accuracy                           0.62       600\n   macro avg       0.60      0.58      0.57       600\nweighted avg       0.61      0.62      0.60       600\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom joblib import parallel_backend\n\n'''\nThis code defines a stacking ensemble using LGBM Classifier, KNeighborsClassifier, and MLPClassifier \nas base models, and MLPClassifier as the final estimator. It then uses a GridSearchCV object to search \nover a parameter grid for the best hyperparameters. Finally, it prints the classification report for the predictions made on the test set.\n'''\nx_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny_test = y_test.astype(np.float32)\n\n# define base models\nmodel1 = lgbm.LGBMClassifier(n_estimators=100, random_state=42, \n                          colsample_bytree = 0.6,\n                          subsample = 0.8,\n                          min_child_samples = 1,\n                          objective = 'multiclass',\n                          num_leaves = 20,\n                          max_depth = 10,\n                          learning_rate = 0.5\n                         ) #RandomForestClassifier(n_estimators=100, random_state=42)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = MLPClassifier(hidden_layer_sizes=(32, 32), random_state=42)\n\n# define stacking ensemble\nestimators = [('lg', model1),\n              ('knn', model2),\n              ('mlp', model3)]\n\nclf = StackingClassifier(estimators=estimators, \n                         final_estimator=MLPClassifier(hidden_layer_sizes=(64, 64),\n                                                       random_state=42))\n\n\n# define parameter grid for randomized search\nparam_dist = {\n              'knn__n_neighbors': [3, 5, 7],\n              'mlp__alpha': [0.001, 0.01, 0.1, 1],\n              'mlp__learning_rate_init': [0.001, 0.01, 0.1],\n              'final_estimator__alpha': [0.001, 0.01, 0.1, 1],\n              'final_estimator__learning_rate_init': [0.001, 0.01, 0.1]}\n\n# define RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=clf, \n                             param_distributions=param_dist, \n                             cv=3, \n                             n_jobs=-1, \n                             verbose=2)\n\n# fit the RandomizedSearchCV object to the data\nwith parallel_backend('multiprocessing'):\n      random_search.fit(x_train, y_train)\n\n# make predictions on test set\ny_pred = random_search.predict(x_test)\n\n# print classification report\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T17:40:26.157022Z","iopub.execute_input":"2023-05-15T17:40:26.157465Z","iopub.status.idle":"2023-05-15T18:18:35.100036Z","shell.execute_reply.started":"2023-05-15T17:40:26.157409Z","shell.execute_reply":"2023-05-15T18:18:35.095519Z"},"scrolled":true,"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nFitting 3 folds for each of 10 candidates, totalling 30 fits\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 2.4min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 2.5min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=1, mlp__learning_rate_init=0.1; total time= 2.2min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 2.6min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=1, mlp__learning_rate_init=0.1; total time= 2.1min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=1, mlp__learning_rate_init=0.1; total time= 2.2min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time= 2.1min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time= 2.1min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time= 2.1min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time= 2.1min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time= 2.1min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time= 2.1min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 2.6min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 2.7min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 2.7min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 2.7min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 2.7min\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 2.7min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 2.7min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 2.7min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 2.7min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1; total time= 2.4min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1; total time= 2.1min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1; total time= 2.2min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=1, mlp__learning_rate_init=0.001; total time= 2.5min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=1, mlp__learning_rate_init=0.001; total time= 2.5min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=1, mlp__learning_rate_init=0.001; total time= 2.6min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 2.6min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 2.6min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 2.5min\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n              precision    recall  f1-score   support\n\n        -1.0       0.54      0.38      0.44       143\n         0.0       0.73      0.68      0.71       247\n         1.0       0.56      0.71      0.63       210\n\n    accuracy                           0.62       600\n   macro avg       0.61      0.59      0.59       600\nweighted avg       0.63      0.62      0.62       600\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# roBERTa","metadata":{"id":"hKqP_ifzgJOu"}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\nmodel = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest').to(device)\n\ndf_bert = tokenizer(df['lemma_sentence(with POS)'].values.tolist(), padding = True, truncation = True, return_tensors=\"pt\")\n\nprint(df_bert.keys())\n\n#move on device (GPU)\ntokenized_train = {k:torch.tensor(v).to(device) for k,v in df_bert.items()}","metadata":{"id":"JnXBGvPX_2TF","execution":{"iopub.status.busy":"2023-05-17T20:51:06.075356Z","iopub.execute_input":"2023-05-17T20:51:06.075770Z","iopub.status.idle":"2023-05-17T20:52:30.842364Z","shell.execute_reply.started":"2023-05-17T20:51:06.075730Z","shell.execute_reply":"2023-05-17T20:52:30.841203Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784a859af9b64763a139b48e44933209"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12a654ee34e04caeb7ec30d2d9f97598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"100bcb6bff1c4b529fb3b71112acec44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e620cfd5d7ab4efdb4c9f61c7980f0c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b066e044058447068b8c5890c03308ef"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask'])\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n    hidden_train = model(**tokenized_train) #dim : [batch_size(nr_sentences), tokens, emb_dim]\n\n#get only the [CLS] hidden states\ncls_train = hidden_train.last_hidden_state[:,0,:]","metadata":{"id":"AfEUj_rR_2TF","execution":{"iopub.status.busy":"2023-05-17T20:52:30.844003Z","iopub.execute_input":"2023-05-17T20:52:30.845064Z","iopub.status.idle":"2023-05-17T21:00:04.420941Z","shell.execute_reply.started":"2023-05-17T20:52:30.845021Z","shell.execute_reply":"2023-05-17T21:00:04.419311Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(cls_train.to(\"cpu\"), df['senti_textblob'],test_size = 0.2)\n#@title\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfrom sklearn.preprocessing import OneHotEncoder\ny_train_encoded = le.fit_transform(y_train)\ny_test_encoded = le.fit_transform(y_test)\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)","metadata":{"id":"I4Wbag2M_2TF","execution":{"iopub.status.busy":"2023-05-17T21:00:04.422541Z","iopub.execute_input":"2023-05-17T21:00:04.423435Z","iopub.status.idle":"2023-05-17T21:00:04.442662Z","shell.execute_reply.started":"2023-05-17T21:00:04.423363Z","shell.execute_reply":"2023-05-17T21:00:04.441466Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"torch.Size([2400, 768]) (2400,) torch.Size([600, 768]) (600,)\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train = np.array(x_train, dtype='float32')\ny_train = np.array(y_train, dtype='float32')\nx_test = np.array(x_test, dtype='float32')\ny_test = np.array(y_test, dtype='float32')","metadata":{"id":"EMa8B0_qZCDj","execution":{"iopub.status.busy":"2023-05-17T21:00:04.444397Z","iopub.execute_input":"2023-05-17T21:00:04.447340Z","iopub.status.idle":"2023-05-17T21:00:04.459149Z","shell.execute_reply.started":"2023-05-17T21:00:04.447272Z","shell.execute_reply":"2023-05-17T21:00:04.457061Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"### ALL MODELS #######\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom joblib import parallel_backend\n\nx_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny_test = y_test.astype(np.float32)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Scale the input data\nscaler = MinMaxScaler()\nx_train_scaled = scaler.fit_transform(x_train)\n\nmodels_list = [model_forest,model_nb,model_svc,model_gbm,model_lgbm,model_xgb,model_cat]\nmodels_name = ['rf','nb','svc','gbm','lgbm','xgb','cat']\nparam_grids = [param_grid_forest,param_grid_nb,param_grid_svc,param_grid_gbm,param_grid_lgbm,param_grid_xgb,param_grid_catboost]\n\nfor model,name,param_grid in zip(models_list, models_name, param_grids):\n    #best parameters for model (with BoW)\n    RandomGrid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=2, verbose=0, n_jobs=4, random_state=40)\n    with parallel_backend('multiprocessing'):\n        if(name!='xgb' and name!='nb'):\n            RandomGrid.fit(x_train, y_train)\n        elif(name=='nb'):\n            RandomGrid.fit(x_train_scaled, y_train)\n        else:\n            RandomGrid.fit(x_train, y_train_encoded)\n    print('Best params for ' + name+':')\n    print(RandomGrid.best_params_)\n\n    print('Classification report for ' + name+':')\n    if(name!='xgb' and name!='nb'):\n        print(classification_report(y_test, RandomGrid.predict(x_test)))\n    elif(name=='nb'):\n        print(classification_report(y_test, RandomGrid.predict(x_test)))\n    else:\n        print(classification_report(y_test_encoded, RandomGrid.predict(x_test)))\n","metadata":{"id":"ZGxwawJbZCIA","execution":{"iopub.status.busy":"2023-05-17T21:00:04.461376Z","iopub.execute_input":"2023-05-17T21:00:04.462257Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nBest params for rf:\n{'n_estimators': 40, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'auto', 'max_depth': 30}\nClassification report for rf:\n              precision    recall  f1-score   support\n\n        -1.0       0.53      0.48      0.51       135\n         0.0       0.65      0.75      0.70       256\n         1.0       0.64      0.57      0.60       209\n\n    accuracy                           0.62       600\n   macro avg       0.61      0.60      0.60       600\nweighted avg       0.62      0.62      0.62       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nBest params for nb:\n{'fit_prior': True, 'alpha': 0.01}\nClassification report for nb:\n              precision    recall  f1-score   support\n\n        -1.0       0.35      0.60      0.44       135\n         0.0       0.53      0.65      0.58       256\n         1.0       0.88      0.22      0.35       209\n\n    accuracy                           0.49       600\n   macro avg       0.59      0.49      0.46       600\nweighted avg       0.61      0.49      0.47       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nBest params for svc:\n{'kernel': 'poly', 'degree': 1, 'C': 100}\nClassification report for svc:\n              precision    recall  f1-score   support\n\n        -1.0       0.50      0.52      0.51       135\n         0.0       0.71      0.72      0.71       256\n         1.0       0.65      0.62      0.63       209\n\n    accuracy                           0.64       600\n   macro avg       0.62      0.62      0.62       600\nweighted avg       0.64      0.64      0.64       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nBest params for gbm:\n{'n_estimators': 90, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2', 'max_depth': 40, 'learning_rate': 0.1}\nClassification report for gbm:\n              precision    recall  f1-score   support\n\n        -1.0       0.52      0.47      0.49       135\n         0.0       0.66      0.77      0.71       256\n         1.0       0.65      0.56      0.61       209\n\n    accuracy                           0.63       600\n   macro avg       0.61      0.60      0.60       600\nweighted avg       0.62      0.63      0.62       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nBest params for lgbm:\n{'subsample': 0.9, 'objective': 'multiclass', 'num_leaves': 40, 'n_estimators': 100, 'min_child_samples': 1, 'max_depth': 15, 'learning_rate': 0.1, 'colsample_bytree': 0.6}\nClassification report for lgbm:\n              precision    recall  f1-score   support\n\n        -1.0       0.54      0.46      0.50       135\n         0.0       0.67      0.77      0.72       256\n         1.0       0.65      0.60      0.63       209\n\n    accuracy                           0.64       600\n   macro avg       0.62      0.61      0.61       600\nweighted avg       0.63      0.64      0.64       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n[21:15:51] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[21:15:51] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[21:15:51] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[21:15:51] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n\n\n[21:15:52] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[21:15:52] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n[21:16:03] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[21:16:03] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[21:16:09] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[21:16:09] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n[21:16:14] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[21:16:14] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n[21:18:26] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[21:18:28] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[21:19:37] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\nBest params for xgb:\n{'subsample': 0.6, 'objective': 'multi:softmax', 'n_estimators': 40, 'min_child_weight': 3, 'max_depth': 40, 'learning_rate': 0.3, 'gamma': 0.0, 'colsample_bytree': 0.8, 'booster': 'gblinear'}\nClassification report for xgb:\n              precision    recall  f1-score   support\n\n           0       0.51      0.44      0.47       135\n           1       0.68      0.73      0.70       256\n           2       0.61      0.61      0.61       209\n\n    accuracy                           0.62       600\n   macro avg       0.60      0.59      0.59       600\nweighted avg       0.62      0.62      0.62       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n0:\tlearn: 0.8897959\ttotal: 13.2s\tremaining: 13m 1s\n0:\tlearn: 0.9008841\ttotal: 13.3s\tremaining: 13m 2s\n0:\tlearn: 1.0935262\ttotal: 13.2s\tremaining: 17m 25s\n0:\tlearn: 1.0939133\ttotal: 13.7s\tremaining: 17m 58s\n1:\tlearn: 0.7663542\ttotal: 26.3s\tremaining: 12m 42s\n1:\tlearn: 1.0892351\ttotal: 26.6s\tremaining: 17m 16s\n1:\tlearn: 0.7815103\ttotal: 26.7s\tremaining: 12m 53s\n1:\tlearn: 1.0891732\ttotal: 27s\tremaining: 17m 34s\n2:\tlearn: 0.6734632\ttotal: 40.2s\tremaining: 12m 44s\n2:\tlearn: 1.0840089\ttotal: 40.4s\tremaining: 17m 16s\n2:\tlearn: 1.0844195\ttotal: 40.4s\tremaining: 17m 16s\n2:\tlearn: 0.6899979\ttotal: 40.7s\tremaining: 12m 54s\n3:\tlearn: 0.6060989\ttotal: 53s\tremaining: 12m 22s\n3:\tlearn: 1.0797441\ttotal: 53.1s\tremaining: 16m 48s\n3:\tlearn: 1.0804282\ttotal: 53.3s\tremaining: 16m 53s\n3:\tlearn: 0.6234656\ttotal: 53.6s\tremaining: 12m 30s\n4:\tlearn: 1.0753903\ttotal: 1m 6s\tremaining: 16m 31s\n4:\tlearn: 1.0757513\ttotal: 1m 6s\tremaining: 16m 34s\n4:\tlearn: 0.5300519\ttotal: 1m 6s\tremaining: 12m 10s\n4:\tlearn: 0.5489659\ttotal: 1m 6s\tremaining: 12m 16s\n5:\tlearn: 1.0705208\ttotal: 1m 18s\tremaining: 16m 13s\n5:\tlearn: 1.0709910\ttotal: 1m 19s\tremaining: 16m 14s\n5:\tlearn: 0.4653134\ttotal: 1m 19s\tremaining: 11m 52s\n5:\tlearn: 0.4853506\ttotal: 1m 19s\tremaining: 11m 57s\n6:\tlearn: 1.0655522\ttotal: 1m 31s\tremaining: 15m 55s\n6:\tlearn: 1.0664691\ttotal: 1m 31s\tremaining: 15m 57s\n6:\tlearn: 0.4232864\ttotal: 1m 31s\tremaining: 11m 35s\n6:\tlearn: 0.4466462\ttotal: 1m 32s\tremaining: 11m 39s\n7:\tlearn: 1.0613661\ttotal: 1m 44s\tremaining: 15m 39s\n7:\tlearn: 1.0607952\ttotal: 1m 44s\tremaining: 15m 43s\n7:\tlearn: 0.3773884\ttotal: 1m 45s\tremaining: 11m 23s\n7:\tlearn: 0.4087423\ttotal: 1m 45s\tremaining: 11m 25s\n8:\tlearn: 1.0569994\ttotal: 1m 57s\tremaining: 15m 24s\n8:\tlearn: 1.0561022\ttotal: 1m 57s\tremaining: 15m 28s\n8:\tlearn: 0.3416278\ttotal: 1m 57s\tremaining: 11m 8s\n8:\tlearn: 0.3604800\ttotal: 1m 58s\tremaining: 11m 11s\n9:\tlearn: 1.0516133\ttotal: 2m 10s\tremaining: 15m 16s\n9:\tlearn: 1.0525160\ttotal: 2m 10s\tremaining: 15m 16s\n9:\tlearn: 0.3102261\ttotal: 2m 12s\tremaining: 11m\n9:\tlearn: 0.3245354\ttotal: 2m 12s\tremaining: 11m 2s\n10:\tlearn: 1.0480117\ttotal: 2m 23s\tremaining: 15m 2s\n10:\tlearn: 1.0470264\ttotal: 2m 23s\tremaining: 15m 2s\n10:\tlearn: 0.2865090\ttotal: 2m 24s\tremaining: 10m 45s\n10:\tlearn: 0.3007431\ttotal: 2m 25s\tremaining: 10m 47s\n11:\tlearn: 1.0437401\ttotal: 2m 36s\tremaining: 14m 47s\n11:\tlearn: 1.0427573\ttotal: 2m 36s\tremaining: 14m 48s\n11:\tlearn: 0.2669860\ttotal: 2m 37s\tremaining: 10m 30s\n11:\tlearn: 0.2662691\ttotal: 2m 38s\tremaining: 10m 32s\n12:\tlearn: 1.0385348\ttotal: 2m 49s\tremaining: 14m 32s\n12:\tlearn: 1.0396479\ttotal: 2m 49s\tremaining: 14m 35s\n12:\tlearn: 0.2533219\ttotal: 2m 50s\tremaining: 10m 17s\n12:\tlearn: 0.2494321\ttotal: 2m 51s\tremaining: 10m 20s\n13:\tlearn: 1.0345741\ttotal: 3m 2s\tremaining: 14m 18s\n13:\tlearn: 1.0351321\ttotal: 3m 2s\tremaining: 14m 20s\n13:\tlearn: 0.2356348\ttotal: 3m 3s\tremaining: 10m 2s\n13:\tlearn: 0.2289641\ttotal: 3m 4s\tremaining: 10m 5s\n14:\tlearn: 1.0303248\ttotal: 3m 15s\tremaining: 14m 5s\n14:\tlearn: 1.0308035\ttotal: 3m 15s\tremaining: 14m 8s\n14:\tlearn: 0.2104721\ttotal: 3m 16s\tremaining: 9m 50s\n14:\tlearn: 0.2228274\ttotal: 3m 16s\tremaining: 9m 50s\n","output_type":"stream"}]},{"cell_type":"code","source":"#RF+KNN+MLP\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom joblib import parallel_backend\n\n'''\nThis code defines a stacking ensemble using RandomForestClassifier, KNeighborsClassifier, and MLPClassifier \nas base models, and MLPClassifier as the final estimator. It then uses a GridSearchCV object \nto search over a parameter grid for the best hyperparameters. Finally, it prints the classification \nreport for the predictions made on the test set.\n'''\n\n# define base models\nmodel1 = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = MLPClassifier(hidden_layer_sizes=(32, 32), random_state=42)\n\n# define stacking ensemble\nestimators = [('rf', model1),\n              ('knn', model2),\n              ('mlp', model3)]\n\nclf = StackingClassifier(estimators=estimators, \n                         final_estimator=MLPClassifier(hidden_layer_sizes=(64, 64),\n                                                       random_state=42))\n\n\n# define parameter grid for randomized search\nparam_dist = {'rf__max_depth': [20,40,60],\n              'rf__max_features': ['sqrt'],\n              'rf__min_samples_leaf': [1, 2, 5, 10],\n              'rf__min_samples_split': [2, 5, 10, 15],\n              'knn__n_neighbors': [3, 5, 7],\n              'mlp__alpha': [0.001, 0.01, 0.1, 1],\n              'mlp__learning_rate_init': [0.001, 0.01, 0.1],\n              'final_estimator__alpha': [0.001, 0.01, 0.1, 1],\n              'final_estimator__learning_rate_init': [0.001, 0.01, 0.1]}\n\n# define RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=clf, \n                             param_distributions=param_dist, \n                             cv=3, \n                             n_jobs=-1, \n                             verbose=2)\n\n# fit the RandomizedSearchCV object to the data\nwith parallel_backend('multiprocessing'):\n    random_search.fit(x_train, y_train)\n\n# make predictions on test set\ny_pred = random_search.predict(x_test)\n\n# print classification report\nprint(classification_report(y_test, y_pred))\n","metadata":{"id":"bP2W_K2B_2TG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom joblib import parallel_backend\n\n'''\nThis code defines a stacking ensemble using LGBM, KNeighborsClassifier, and MLPClassifier as base models, \nand MLPClassifier as the final estimator. It then uses a GridSearchCV object to search over a parameter grid for \nthe best hyperparameters. Finally, it prints the classification report for the predictions made on the test set.\n'''\nx_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny_test = y_test.astype(np.float32)\n\n# define base models\nmodel1 = lgbm.LGBMClassifier(n_estimators=100, random_state=42, \n                          colsample_bytree = 0.6,\n                          subsample = 0.8,\n                          min_child_samples = 1,\n                          objective = 'multiclass',\n                          num_leaves = 20,\n                          max_depth = 10,\n                          learning_rate = 0.5\n                         ) #RandomForestClassifier(n_estimators=100, random_state=42)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = MLPClassifier(hidden_layer_sizes=(32, 32), random_state=42)\n\n# define stacking ensemble\nestimators = [('lg', model1),\n              ('knn', model2),\n              ('mlp', model3)]\n\nclf = StackingClassifier(estimators=estimators, \n                         final_estimator=MLPClassifier(hidden_layer_sizes=(64, 64),\n                                                       random_state=42))\n\n\n# define parameter grid for randomized search\nparam_dist = {\n              'knn__n_neighbors': [3, 5, 7],\n              'mlp__alpha': [0.001, 0.01, 0.1, 1],\n              'mlp__learning_rate_init': [0.001, 0.01, 0.1],\n              'final_estimator__alpha': [0.001, 0.01, 0.1, 1],\n              'final_estimator__learning_rate_init': [0.001, 0.01, 0.1]}\n\n# define RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=clf, \n                             param_distributions=param_dist, \n                             cv=3, \n                             n_jobs=-1, \n                             verbose=2)\n\n# fit the RandomizedSearchCV object to the data\nwith parallel_backend('multiprocessing'):\n      random_search.fit(x_train, y_train)\n\n# make predictions on test set\ny_pred = random_search.predict(x_test)\n\n# print classification report\nprint(classification_report(y_test, y_pred))\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SBERT","metadata":{"id":"0jF61XJMgLmu"}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to(device)\n\ndf_bert = tokenizer(df['lemma_sentence(with POS)'].values.tolist(), padding = True, truncation = True, return_tensors=\"pt\")\n\nprint(df_bert.keys())\n\n#move on device (GPU)\ntokenized_train = {k:torch.tensor(v).to(device) for k,v in df_bert.items()}","metadata":{"id":"qzaynIHu_2TG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    hidden_train = model(**tokenized_train) #dim : [batch_size(nr_sentences), tokens, emb_dim]\n\n#get only the [CLS] hidden states\ncls_train = hidden_train.last_hidden_state[:,0,:]","metadata":{"id":"Ex3V_fHr_2TG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(cls_train.to(\"cpu\"), df['senti_textblob'],test_size = 0.2)\n#@title\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfrom sklearn.preprocessing import OneHotEncoder\ny_train_encoded = le.fit_transform(y_train)\ny_test_encoded = le.fit_transform(y_test)\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = np.array(x_train, dtype='float32')\ny_train = np.array(y_train, dtype='float32')\nx_test = np.array(x_test, dtype='float32')\ny_test = np.array(y_test, dtype='float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### ALL MODELS #######\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom joblib import parallel_backend\n\nx_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny_test = y_test.astype(np.float32)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Scale the input data\nscaler = MinMaxScaler()\nx_train_scaled = scaler.fit_transform(x_train)\n\nmodels_list = [model_forest,model_nb,model_svc,model_gbm,model_lgbm,model_xgb,model_cat]\nmodels_name = ['rf','nb','svc','gbm','lgbm','xgb','cat']\nparam_grids = [param_grid_forest,param_grid_nb,param_grid_svc,param_grid_gbm,param_grid_lgbm,param_grid_xgb,param_grid_catboost]\n\nfor model,name,param_grid in zip(models_list, models_name, param_grids):\n    #best parameters for model (with BoW)\n    RandomGrid = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=2, verbose=0, n_jobs=4, random_state=40)\n    with parallel_backend('multiprocessing'):\n        if(name!='xgb' and name!='nb'):\n            RandomGrid.fit(x_train, y_train)\n        elif(name=='nb'):\n            RandomGrid.fit(x_train_scaled, y_train)\n        else:\n            RandomGrid.fit(x_train, y_train_encoded)\n    print('Best params for ' + name+':')\n    print(RandomGrid.best_params_)\n\n    print('Classification report for ' + name+':')\n    if(name!='xgb' and name!='nb'):\n        print(classification_report(y_test, RandomGrid.predict(x_test)))\n    elif(name=='nb'):\n        print(classification_report(y_test, RandomGrid.predict(x_test)))\n    else:\n        print(classification_report(y_test_encoded, RandomGrid.predict(x_test)))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:10:52.547362Z","iopub.execute_input":"2023-05-17T19:10:52.547753Z","iopub.status.idle":"2023-05-17T19:36:17.938137Z","shell.execute_reply.started":"2023-05-17T19:10:52.547718Z","shell.execute_reply":"2023-05-17T19:36:17.936328Z"},"scrolled":true,"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nBest params for rf:\n{'n_estimators': 190, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 40}\nClassification report for rf:\n              precision    recall  f1-score   support\n\n        -1.0       0.73      0.05      0.10       147\n         0.0       0.60      0.62      0.61       241\n         1.0       0.49      0.78      0.60       212\n\n    accuracy                           0.54       600\n   macro avg       0.61      0.49      0.44       600\nweighted avg       0.59      0.54      0.48       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nBest params for nb:\n{'fit_prior': False, 'alpha': 10.0}\nClassification report for nb:\n              precision    recall  f1-score   support\n\n        -1.0       0.44      0.22      0.29       147\n         0.0       0.44      0.93      0.59       241\n         1.0       0.71      0.05      0.09       212\n\n    accuracy                           0.44       600\n   macro avg       0.53      0.40      0.32       600\nweighted avg       0.54      0.44      0.34       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nBest params for svc:\n{'kernel': 'rbf', 'degree': 2, 'C': 100}\nClassification report for svc:\n              precision    recall  f1-score   support\n\n        -1.0       0.48      0.48      0.48       147\n         0.0       0.65      0.61      0.63       241\n         1.0       0.57      0.61      0.59       212\n\n    accuracy                           0.58       600\n   macro avg       0.56      0.57      0.56       600\nweighted avg       0.58      0.58      0.58       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nBest params for gbm:\n{'n_estimators': 70, 'min_samples_split': 15, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'max_depth': 20, 'learning_rate': 0.1}\nClassification report for gbm:\n              precision    recall  f1-score   support\n\n        -1.0       0.54      0.15      0.23       147\n         0.0       0.60      0.66      0.63       241\n         1.0       0.51      0.72      0.60       212\n\n    accuracy                           0.55       600\n   macro avg       0.55      0.51      0.49       600\nweighted avg       0.55      0.55      0.52       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nBest params for lgbm:\n{'subsample': 0.9, 'objective': 'multiclass', 'num_leaves': 40, 'n_estimators': 100, 'min_child_samples': 1, 'max_depth': 15, 'learning_rate': 0.1, 'colsample_bytree': 0.6}\nClassification report for lgbm:\n              precision    recall  f1-score   support\n\n        -1.0       0.51      0.19      0.28       147\n         0.0       0.62      0.62      0.62       241\n         1.0       0.51      0.74      0.60       212\n\n    accuracy                           0.56       600\n   macro avg       0.55      0.52      0.50       600\nweighted avg       0.56      0.56      0.53       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n[19:17:53] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[19:17:53] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[19:17:53] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[19:17:53] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n\n\n[19:17:54] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[19:17:54] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n[19:18:00] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[19:18:00] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n[19:18:04] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n[19:18:04] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n\n[19:18:06] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[19:18:06] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[19:19:22] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[19:19:23] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[19:19:57] WARNING: ../src/learner.cc:627: \nParameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\nBest params for xgb:\n{'subsample': 0.8, 'objective': 'multi:softprob', 'n_estimators': 10, 'min_child_weight': 5, 'max_depth': 20, 'learning_rate': 1, 'gamma': 0.3, 'colsample_bytree': 0.9, 'booster': 'gblinear'}\nClassification report for xgb:\n              precision    recall  f1-score   support\n\n           0       0.49      0.44      0.46       147\n           1       0.65      0.62      0.64       241\n           2       0.54      0.61      0.58       212\n\n    accuracy                           0.57       600\n   macro avg       0.56      0.56      0.56       600\nweighted avg       0.57      0.57      0.57       600\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n0:\tlearn: 1.0945440\ttotal: 12.5s\tremaining: 16m 29s\n0:\tlearn: 1.0946767\ttotal: 12.6s\tremaining: 16m 31s\n0:\tlearn: 0.9221968\ttotal: 12.6s\tremaining: 12m 25s\n0:\tlearn: 0.9249924\ttotal: 12.6s\tremaining: 12m 24s\n1:\tlearn: 1.0908735\ttotal: 22.5s\tremaining: 14m 36s\n1:\tlearn: 0.8120040\ttotal: 22.8s\tremaining: 11m 2s\n1:\tlearn: 0.8153625\ttotal: 23.5s\tremaining: 11m 20s\n1:\tlearn: 1.0907993\ttotal: 24.1s\tremaining: 15m 41s\n2:\tlearn: 1.0870302\ttotal: 31.2s\tremaining: 13m 19s\n2:\tlearn: 0.6990786\ttotal: 31.5s\tremaining: 9m 58s\n2:\tlearn: 0.7042726\ttotal: 32s\tremaining: 10m 7s\n2:\tlearn: 1.0873667\ttotal: 32.5s\tremaining: 13m 54s\n3:\tlearn: 1.0831123\ttotal: 38.7s\tremaining: 12m 15s\n3:\tlearn: 0.6054914\ttotal: 39s\tremaining: 9m 6s\n3:\tlearn: 0.6167038\ttotal: 39.5s\tremaining: 9m 12s\n3:\tlearn: 1.0836058\ttotal: 39.9s\tremaining: 12m 38s\n4:\tlearn: 1.0788323\ttotal: 44.9s\tremaining: 11m 12s\n4:\tlearn: 0.5377882\ttotal: 45.2s\tremaining: 8m 16s\n4:\tlearn: 0.5277217\ttotal: 45.6s\tremaining: 8m 22s\n4:\tlearn: 1.0797962\ttotal: 46.1s\tremaining: 11m 32s\n5:\tlearn: 1.0747210\ttotal: 51.1s\tremaining: 10m 29s\n5:\tlearn: 0.4683348\ttotal: 51.4s\tremaining: 7m 42s\n5:\tlearn: 0.4621384\ttotal: 51.9s\tremaining: 7m 47s\n5:\tlearn: 1.0756727\ttotal: 52.3s\tremaining: 10m 45s\n6:\tlearn: 1.0709262\ttotal: 57.3s\tremaining: 9m 58s\n6:\tlearn: 0.4092621\ttotal: 57.6s\tremaining: 7m 15s\n6:\tlearn: 0.4108071\ttotal: 58.1s\tremaining: 7m 19s\n6:\tlearn: 1.0714897\ttotal: 58.6s\tremaining: 10m 11s\n7:\tlearn: 1.0669291\ttotal: 1m 3s\tremaining: 9m 31s\n7:\tlearn: 0.3762508\ttotal: 1m 3s\tremaining: 6m 54s\n7:\tlearn: 0.3683788\ttotal: 1m 4s\tremaining: 6m 57s\n7:\tlearn: 1.0676806\ttotal: 1m 4s\tremaining: 9m 42s\n8:\tlearn: 1.0631641\ttotal: 1m 10s\tremaining: 9m 12s\n8:\tlearn: 0.3431950\ttotal: 1m 10s\tremaining: 6m 39s\n8:\tlearn: 0.3319701\ttotal: 1m 10s\tremaining: 6m 39s\n8:\tlearn: 1.0637498\ttotal: 1m 11s\tremaining: 9m 21s\n9:\tlearn: 0.3097686\ttotal: 1m 16s\tremaining: 6m 22s\n9:\tlearn: 1.0598892\ttotal: 1m 16s\tremaining: 8m 56s\n9:\tlearn: 0.3065083\ttotal: 1m 16s\tremaining: 6m 23s\n9:\tlearn: 1.0600250\ttotal: 1m 17s\tremaining: 9m 1s\n10:\tlearn: 0.2800841\ttotal: 1m 22s\tremaining: 6m 9s\n10:\tlearn: 1.0567900\ttotal: 1m 23s\tremaining: 8m 40s\n10:\tlearn: 0.2801157\ttotal: 1m 23s\tremaining: 6m 10s\n10:\tlearn: 1.0567213\ttotal: 1m 23s\tremaining: 8m 45s\n11:\tlearn: 0.2519858\ttotal: 1m 29s\tremaining: 5m 56s\n11:\tlearn: 1.0529712\ttotal: 1m 29s\tremaining: 8m 25s\n11:\tlearn: 0.2529417\ttotal: 1m 29s\tremaining: 5m 57s\n11:\tlearn: 1.0529309\ttotal: 1m 30s\tremaining: 8m 30s\n12:\tlearn: 0.2348193\ttotal: 1m 35s\tremaining: 5m 44s\n12:\tlearn: 1.0496003\ttotal: 1m 35s\tremaining: 8m 11s\n12:\tlearn: 0.2324444\ttotal: 1m 35s\tremaining: 5m 45s\n12:\tlearn: 1.0494513\ttotal: 1m 36s\tremaining: 8m 15s\n13:\tlearn: 0.2166412\ttotal: 1m 41s\tremaining: 5m 33s\n13:\tlearn: 0.2104315\ttotal: 1m 41s\tremaining: 5m 34s\n13:\tlearn: 1.0458718\ttotal: 1m 41s\tremaining: 8m\n13:\tlearn: 1.0461941\ttotal: 1m 42s\tremaining: 8m 2s\n14:\tlearn: 0.1976444\ttotal: 1m 48s\tremaining: 5m 24s\n14:\tlearn: 1.0424566\ttotal: 1m 48s\tremaining: 7m 49s\n14:\tlearn: 0.1976852\ttotal: 1m 48s\tremaining: 5m 25s\n14:\tlearn: 1.0432887\ttotal: 1m 48s\tremaining: 7m 50s\n15:\tlearn: 0.1840666\ttotal: 1m 54s\tremaining: 5m 14s\n15:\tlearn: 0.1825103\ttotal: 1m 54s\tremaining: 5m 15s\n15:\tlearn: 1.0389487\ttotal: 1m 54s\tremaining: 7m 38s\n15:\tlearn: 1.0398873\ttotal: 1m 54s\tremaining: 7m 38s\n16:\tlearn: 0.1678098\ttotal: 2m\tremaining: 5m 4s\n16:\tlearn: 0.1698527\ttotal: 2m\tremaining: 5m 5s\n16:\tlearn: 1.0353551\ttotal: 2m\tremaining: 7m 28s\n16:\tlearn: 1.0367702\ttotal: 2m\tremaining: 7m 27s\n17:\tlearn: 0.1532443\ttotal: 2m 6s\tremaining: 4m 55s\n17:\tlearn: 0.1554279\ttotal: 2m 6s\tremaining: 4m 56s\n17:\tlearn: 1.0316990\ttotal: 2m 7s\tremaining: 7m 17s\n17:\tlearn: 1.0330372\ttotal: 2m 7s\tremaining: 7m 17s\n18:\tlearn: 0.1427788\ttotal: 2m 12s\tremaining: 4m 46s\n18:\tlearn: 0.1413781\ttotal: 2m 13s\tremaining: 4m 47s\n18:\tlearn: 1.0289205\ttotal: 2m 13s\tremaining: 7m 8s\n18:\tlearn: 1.0278857\ttotal: 2m 13s\tremaining: 7m 8s\n19:\tlearn: 0.1333634\ttotal: 2m 19s\tremaining: 4m 38s\n19:\tlearn: 0.1303682\ttotal: 2m 19s\tremaining: 4m 39s\n19:\tlearn: 1.0248393\ttotal: 2m 19s\tremaining: 6m 59s\n19:\tlearn: 1.0240616\ttotal: 2m 20s\tremaining: 7m\n20:\tlearn: 0.1251720\ttotal: 2m 25s\tremaining: 4m 29s\n20:\tlearn: 0.1204304\ttotal: 2m 26s\tremaining: 4m 31s\n20:\tlearn: 1.0207707\ttotal: 2m 26s\tremaining: 6m 50s\n20:\tlearn: 1.0208201\ttotal: 2m 26s\tremaining: 6m 51s\n21:\tlearn: 0.1164941\ttotal: 2m 31s\tremaining: 4m 21s\n21:\tlearn: 0.1130341\ttotal: 2m 32s\tremaining: 4m 22s\n21:\tlearn: 1.0177169\ttotal: 2m 32s\tremaining: 6m 42s\n21:\tlearn: 1.0172784\ttotal: 2m 32s\tremaining: 6m 41s\n22:\tlearn: 0.1080379\ttotal: 2m 37s\tremaining: 4m 13s\n22:\tlearn: 0.1055830\ttotal: 2m 38s\tremaining: 4m 14s\n22:\tlearn: 1.0140346\ttotal: 2m 38s\tremaining: 6m 33s\n22:\tlearn: 1.0145518\ttotal: 2m 38s\tremaining: 6m 33s\n23:\tlearn: 0.1012540\ttotal: 2m 43s\tremaining: 4m 5s\n23:\tlearn: 0.0990807\ttotal: 2m 44s\tremaining: 4m 6s\n23:\tlearn: 1.0102818\ttotal: 2m 44s\tremaining: 6m 24s\n23:\tlearn: 1.0111953\ttotal: 2m 45s\tremaining: 6m 25s\n24:\tlearn: 0.0948846\ttotal: 2m 49s\tremaining: 3m 57s\n24:\tlearn: 0.0934308\ttotal: 2m 51s\tremaining: 3m 59s\n24:\tlearn: 1.0065894\ttotal: 2m 51s\tremaining: 6m 16s\n24:\tlearn: 1.0076089\ttotal: 2m 51s\tremaining: 6m 17s\n25:\tlearn: 0.0890472\ttotal: 2m 56s\tremaining: 3m 50s\n25:\tlearn: 0.0876676\ttotal: 2m 57s\tremaining: 3m 52s\n25:\tlearn: 1.0027747\ttotal: 2m 57s\tremaining: 6m 8s\n25:\tlearn: 1.0040135\ttotal: 2m 57s\tremaining: 6m 9s\n26:\tlearn: 0.0837051\ttotal: 3m 2s\tremaining: 3m 42s\n26:\tlearn: 0.0825081\ttotal: 3m 3s\tremaining: 3m 44s\n26:\tlearn: 0.9993661\ttotal: 3m 3s\tremaining: 6m\n26:\tlearn: 1.0003135\ttotal: 3m 4s\tremaining: 6m 1s\n27:\tlearn: 0.0787365\ttotal: 3m 8s\tremaining: 3m 35s\n27:\tlearn: 0.9963900\ttotal: 3m 9s\tremaining: 5m 52s\n27:\tlearn: 0.0777487\ttotal: 3m 9s\tremaining: 3m 37s\n27:\tlearn: 0.9965999\ttotal: 3m 10s\tremaining: 5m 53s\n28:\tlearn: 0.0746289\ttotal: 3m 14s\tremaining: 3m 27s\n28:\tlearn: 0.9932523\ttotal: 3m 16s\tremaining: 5m 44s\n28:\tlearn: 0.0737638\ttotal: 3m 16s\tremaining: 3m 29s\n28:\tlearn: 0.9936892\ttotal: 3m 16s\tremaining: 5m 45s\n29:\tlearn: 0.0707530\ttotal: 3m 21s\tremaining: 3m 21s\n29:\tlearn: 0.0700032\ttotal: 3m 22s\tremaining: 3m 22s\n29:\tlearn: 0.9902312\ttotal: 3m 22s\tremaining: 5m 38s\n29:\tlearn: 0.9903021\ttotal: 3m 23s\tremaining: 5m 38s\n30:\tlearn: 0.0674363\ttotal: 3m 27s\tremaining: 3m 13s\n30:\tlearn: 0.0662325\ttotal: 3m 28s\tremaining: 3m 15s\n30:\tlearn: 0.9873172\ttotal: 3m 29s\tremaining: 5m 30s\n30:\tlearn: 0.9867487\ttotal: 3m 29s\tremaining: 5m 30s\n31:\tlearn: 0.0642825\ttotal: 3m 33s\tremaining: 3m 6s\n31:\tlearn: 0.0630726\ttotal: 3m 35s\tremaining: 3m 8s\n31:\tlearn: 0.9837115\ttotal: 3m 35s\tremaining: 5m 22s\n31:\tlearn: 0.9842377\ttotal: 3m 35s\tremaining: 5m 23s\n32:\tlearn: 0.0614350\ttotal: 3m 39s\tremaining: 2m 59s\n32:\tlearn: 0.0602920\ttotal: 3m 41s\tremaining: 3m 1s\n32:\tlearn: 0.9805777\ttotal: 3m 41s\tremaining: 5m 15s\n32:\tlearn: 0.9807273\ttotal: 3m 41s\tremaining: 5m 15s\n33:\tlearn: 0.0586521\ttotal: 3m 45s\tremaining: 2m 52s\n33:\tlearn: 0.0573729\ttotal: 3m 47s\tremaining: 2m 53s\n33:\tlearn: 0.9777547\ttotal: 3m 47s\tremaining: 5m 7s\n33:\tlearn: 0.9777244\ttotal: 3m 47s\tremaining: 5m 8s\n34:\tlearn: 0.0559194\ttotal: 3m 51s\tremaining: 2m 45s\n34:\tlearn: 0.0548869\ttotal: 3m 54s\tremaining: 2m 47s\n34:\tlearn: 0.9746406\ttotal: 3m 54s\tremaining: 5m 1s\n34:\tlearn: 0.9746258\ttotal: 3m 54s\tremaining: 5m 1s\n35:\tlearn: 0.0534878\ttotal: 3m 57s\tremaining: 2m 38s\n35:\tlearn: 0.0527236\ttotal: 4m\tremaining: 2m 40s\n35:\tlearn: 0.9709345\ttotal: 4m\tremaining: 4m 54s\n35:\tlearn: 0.9713311\ttotal: 4m\tremaining: 4m 54s\n36:\tlearn: 0.0514171\ttotal: 4m 4s\tremaining: 2m 31s\n36:\tlearn: 0.0505276\ttotal: 4m 6s\tremaining: 2m 33s\n36:\tlearn: 0.9678133\ttotal: 4m 7s\tremaining: 4m 47s\n36:\tlearn: 0.9681343\ttotal: 4m 7s\tremaining: 4m 47s\n37:\tlearn: 0.0494136\ttotal: 4m 10s\tremaining: 2m 24s\n37:\tlearn: 0.0484696\ttotal: 4m 12s\tremaining: 2m 26s\n37:\tlearn: 0.9646931\ttotal: 4m 13s\tremaining: 4m 39s\n37:\tlearn: 0.9649627\ttotal: 4m 13s\tremaining: 4m 39s\n38:\tlearn: 0.0474958\ttotal: 4m 16s\tremaining: 2m 18s\n38:\tlearn: 0.0466140\ttotal: 4m 18s\tremaining: 2m 19s\n38:\tlearn: 0.9613764\ttotal: 4m 19s\tremaining: 4m 32s\n38:\tlearn: 0.9618480\ttotal: 4m 19s\tremaining: 4m 32s\n39:\tlearn: 0.0456608\ttotal: 4m 22s\tremaining: 2m 11s\n39:\tlearn: 0.0449006\ttotal: 4m 25s\tremaining: 2m 12s\n39:\tlearn: 0.9586876\ttotal: 4m 26s\tremaining: 4m 26s\n39:\tlearn: 0.9588757\ttotal: 4m 26s\tremaining: 4m 26s\n40:\tlearn: 0.0440310\ttotal: 4m 28s\tremaining: 2m 4s\n40:\tlearn: 0.0433414\ttotal: 4m 31s\tremaining: 2m 5s\n40:\tlearn: 0.9559025\ttotal: 4m 32s\tremaining: 4m 19s\n40:\tlearn: 0.9559213\ttotal: 4m 32s\tremaining: 4m 19s\n41:\tlearn: 0.0425240\ttotal: 4m 35s\tremaining: 1m 57s\n41:\tlearn: 0.0417098\ttotal: 4m 38s\tremaining: 1m 59s\n41:\tlearn: 0.9525719\ttotal: 4m 38s\tremaining: 4m 12s\n41:\tlearn: 0.9527098\ttotal: 4m 38s\tremaining: 4m 11s\n42:\tlearn: 0.0410221\ttotal: 4m 41s\tremaining: 1m 51s\n42:\tlearn: 0.0402754\ttotal: 4m 44s\tremaining: 1m 52s\n42:\tlearn: 0.9500578\ttotal: 4m 44s\tremaining: 4m 4s\n42:\tlearn: 0.9494303\ttotal: 4m 44s\tremaining: 4m 5s\n43:\tlearn: 0.0397918\ttotal: 4m 47s\tremaining: 1m 44s\n43:\tlearn: 0.0388809\ttotal: 4m 50s\tremaining: 1m 45s\n43:\tlearn: 0.9470702\ttotal: 4m 50s\tremaining: 3m 58s\n43:\tlearn: 0.9470386\ttotal: 4m 50s\tremaining: 3m 58s\n44:\tlearn: 0.0385200\ttotal: 4m 53s\tremaining: 1m 37s\n44:\tlearn: 0.0376982\ttotal: 4m 57s\tremaining: 1m 39s\n44:\tlearn: 0.9435775\ttotal: 4m 56s\tremaining: 3m 50s\n44:\tlearn: 0.9435442\ttotal: 4m 57s\tremaining: 3m 51s\n45:\tlearn: 0.0373478\ttotal: 5m\tremaining: 1m 31s\n45:\tlearn: 0.0365779\ttotal: 5m 3s\tremaining: 1m 32s\n45:\tlearn: 0.9403380\ttotal: 5m 3s\tremaining: 3m 44s\n45:\tlearn: 0.9403063\ttotal: 5m 3s\tremaining: 3m 44s\n46:\tlearn: 0.0361067\ttotal: 5m 6s\tremaining: 1m 24s\n46:\tlearn: 0.9373131\ttotal: 5m 9s\tremaining: 3m 37s\n46:\tlearn: 0.0355037\ttotal: 5m 9s\tremaining: 1m 25s\n46:\tlearn: 0.9373544\ttotal: 5m 10s\tremaining: 3m 37s\n47:\tlearn: 0.0349952\ttotal: 5m 12s\tremaining: 1m 18s\n47:\tlearn: 0.9341267\ttotal: 5m 15s\tremaining: 3m 30s\n47:\tlearn: 0.0343898\ttotal: 5m 15s\tremaining: 1m 18s\n47:\tlearn: 0.9346446\ttotal: 5m 16s\tremaining: 3m 30s\n48:\tlearn: 0.0339682\ttotal: 5m 18s\tremaining: 1m 11s\n48:\tlearn: 0.9313268\ttotal: 5m 21s\tremaining: 3m 23s\n48:\tlearn: 0.0333727\ttotal: 5m 22s\tremaining: 1m 12s\n48:\tlearn: 0.9319652\ttotal: 5m 22s\tremaining: 3m 24s\n49:\tlearn: 0.0329007\ttotal: 5m 25s\tremaining: 1m 5s\n49:\tlearn: 0.0323271\ttotal: 5m 28s\tremaining: 1m 5s\n49:\tlearn: 0.9280218\ttotal: 5m 28s\tremaining: 3m 17s\n49:\tlearn: 0.9293294\ttotal: 5m 29s\tremaining: 3m 17s\n50:\tlearn: 0.0320031\ttotal: 5m 31s\tremaining: 58.6s\n50:\tlearn: 0.0313951\ttotal: 5m 34s\tremaining: 59s\n50:\tlearn: 0.9251714\ttotal: 5m 34s\tremaining: 3m 10s\n50:\tlearn: 0.9260674\ttotal: 5m 35s\tremaining: 3m 10s\n51:\tlearn: 0.0311093\ttotal: 5m 38s\tremaining: 52s\n51:\tlearn: 0.0305852\ttotal: 5m 40s\tremaining: 52.4s\n51:\tlearn: 0.9219258\ttotal: 5m 41s\tremaining: 3m 3s\n51:\tlearn: 0.9230515\ttotal: 5m 41s\tremaining: 3m 4s\n52:\tlearn: 0.0302522\ttotal: 5m 44s\tremaining: 45.5s\n52:\tlearn: 0.0297129\ttotal: 5m 46s\tremaining: 45.8s\n52:\tlearn: 0.9190538\ttotal: 5m 47s\tremaining: 2m 56s\n52:\tlearn: 0.9200549\ttotal: 5m 48s\tremaining: 2m 57s\n53:\tlearn: 0.0294310\ttotal: 5m 50s\tremaining: 38.9s\n53:\tlearn: 0.0289963\ttotal: 5m 52s\tremaining: 39.2s\n53:\tlearn: 0.9161017\ttotal: 5m 53s\tremaining: 2m 50s\n53:\tlearn: 0.9166691\ttotal: 5m 54s\tremaining: 2m 50s\n54:\tlearn: 0.0287281\ttotal: 5m 56s\tremaining: 32.4s\n54:\tlearn: 0.0282829\ttotal: 5m 59s\tremaining: 32.7s\n54:\tlearn: 0.9130583\ttotal: 6m\tremaining: 2m 43s\n54:\tlearn: 0.9132906\ttotal: 6m\tremaining: 2m 44s\n55:\tlearn: 0.0279017\ttotal: 6m 2s\tremaining: 25.9s\n55:\tlearn: 0.0276394\ttotal: 6m 5s\tremaining: 26.1s\n55:\tlearn: 0.9100870\ttotal: 6m 6s\tremaining: 2m 37s\n55:\tlearn: 0.9103555\ttotal: 6m 7s\tremaining: 2m 37s\n56:\tlearn: 0.0271751\ttotal: 6m 9s\tremaining: 19.4s\n56:\tlearn: 0.0269796\ttotal: 6m 11s\tremaining: 19.6s\n56:\tlearn: 0.9076250\ttotal: 6m 12s\tremaining: 2m 30s\n56:\tlearn: 0.9075127\ttotal: 6m 13s\tremaining: 2m 30s\n57:\tlearn: 0.0265331\ttotal: 6m 15s\tremaining: 12.9s\n57:\tlearn: 0.0263106\ttotal: 6m 18s\tremaining: 13s\n57:\tlearn: 0.9047980\ttotal: 6m 18s\tremaining: 2m 23s\n57:\tlearn: 0.9048420\ttotal: 6m 19s\tremaining: 2m 24s\n58:\tlearn: 0.0258650\ttotal: 6m 21s\tremaining: 6.47s\n58:\tlearn: 0.0257001\ttotal: 6m 24s\tremaining: 6.51s\n58:\tlearn: 0.9020740\ttotal: 6m 24s\tremaining: 2m 17s\n58:\tlearn: 0.9015510\ttotal: 6m 25s\tremaining: 2m 17s\n59:\tlearn: 0.0252438\ttotal: 6m 27s\tremaining: 0us\n59:\tlearn: 0.0250199\ttotal: 6m 30s\tremaining: 0us\n59:\tlearn: 0.8991236\ttotal: 6m 31s\tremaining: 2m 10s\n59:\tlearn: 0.8991492\ttotal: 6m 32s\tremaining: 2m 10s\n0:\tlearn: 1.0977359\ttotal: 6.33s\tremaining: 13m 37s\n0:\tlearn: 1.0976273\ttotal: 6.25s\tremaining: 13m 25s\n60:\tlearn: 0.8961909\ttotal: 6m 37s\tremaining: 2m 3s\n60:\tlearn: 0.8959690\ttotal: 6m 38s\tremaining: 2m 4s\n1:\tlearn: 1.0968111\ttotal: 12.5s\tremaining: 13m 17s\n1:\tlearn: 1.0965957\ttotal: 12.5s\tremaining: 13m 20s\n61:\tlearn: 0.8936394\ttotal: 6m 43s\tremaining: 1m 57s\n61:\tlearn: 0.8932596\ttotal: 6m 44s\tremaining: 1m 57s\n2:\tlearn: 1.0959732\ttotal: 18.7s\tremaining: 13m 11s\n2:\tlearn: 1.0957064\ttotal: 18.7s\tremaining: 13m 12s\n62:\tlearn: 0.8905979\ttotal: 6m 49s\tremaining: 1m 50s\n62:\tlearn: 0.8906690\ttotal: 6m 50s\tremaining: 1m 50s\n3:\tlearn: 1.0950184\ttotal: 25s\tremaining: 13m 7s\n3:\tlearn: 1.0947803\ttotal: 25s\tremaining: 13m 6s\n63:\tlearn: 0.8877672\ttotal: 6m 56s\tremaining: 1m 44s\n63:\tlearn: 0.8881191\ttotal: 6m 56s\tremaining: 1m 44s\n4:\tlearn: 1.0940948\ttotal: 31.3s\tremaining: 13m 1s\n4:\tlearn: 1.0938639\ttotal: 31.8s\tremaining: 13m 15s\n64:\tlearn: 0.8854711\ttotal: 7m 2s\tremaining: 1m 37s\n64:\tlearn: 0.8851570\ttotal: 7m 2s\tremaining: 1m 37s\n5:\tlearn: 1.0932098\ttotal: 38.2s\tremaining: 13m 8s\n5:\tlearn: 1.0928639\ttotal: 38.1s\tremaining: 13m 6s\n65:\tlearn: 0.8825319\ttotal: 7m 8s\tremaining: 1m 30s\n65:\tlearn: 0.8822914\ttotal: 7m 9s\tremaining: 1m 31s\n6:\tlearn: 1.0923002\ttotal: 44.4s\tremaining: 12m 59s\n6:\tlearn: 1.0919963\ttotal: 44.3s\tremaining: 12m 58s\n66:\tlearn: 0.8797363\ttotal: 7m 15s\tremaining: 1m 24s\n66:\tlearn: 0.8800326\ttotal: 7m 15s\tremaining: 1m 24s\n7:\tlearn: 1.0914334\ttotal: 50.6s\tremaining: 12m 51s\n67:\tlearn: 0.8773399\ttotal: 7m 21s\tremaining: 1m 17s\n7:\tlearn: 1.0910417\ttotal: 50.8s\tremaining: 12m 54s\n67:\tlearn: 0.8773467\ttotal: 7m 21s\tremaining: 1m 17s\n8:\tlearn: 1.0905857\ttotal: 56.9s\tremaining: 12m 44s\n8:\tlearn: 1.0901548\ttotal: 57s\tremaining: 12m 45s\n68:\tlearn: 0.8745127\ttotal: 7m 27s\tremaining: 1m 11s\n68:\tlearn: 0.8742353\ttotal: 7m 28s\tremaining: 1m 11s\n9:\tlearn: 1.0897078\ttotal: 1m 3s\tremaining: 12m 38s\n9:\tlearn: 1.0892054\ttotal: 1m 3s\tremaining: 12m 40s\n69:\tlearn: 0.8715449\ttotal: 7m 34s\tremaining: 1m 4s\n69:\tlearn: 0.8714976\ttotal: 7m 34s\tremaining: 1m 4s\n10:\tlearn: 1.0888695\ttotal: 1m 9s\tremaining: 12m 30s\n10:\tlearn: 1.0882784\ttotal: 1m 9s\tremaining: 12m 34s\n70:\tlearn: 0.8690316\ttotal: 7m 40s\tremaining: 58.4s\n70:\tlearn: 0.8691021\ttotal: 7m 40s\tremaining: 58.4s\n11:\tlearn: 1.0879856\ttotal: 1m 15s\tremaining: 12m 23s\n11:\tlearn: 1.0873212\ttotal: 1m 15s\tremaining: 12m 26s\n71:\tlearn: 0.8666909\ttotal: 7m 47s\tremaining: 51.9s\n71:\tlearn: 0.8669345\ttotal: 7m 47s\tremaining: 51.9s\n12:\tlearn: 1.0868573\ttotal: 1m 21s\tremaining: 12m 16s\n12:\tlearn: 1.0863412\ttotal: 1m 22s\tremaining: 12m 19s\n72:\tlearn: 0.8639469\ttotal: 7m 53s\tremaining: 45.4s\n72:\tlearn: 0.8644832\ttotal: 7m 53s\tremaining: 45.4s\n13:\tlearn: 1.0859485\ttotal: 1m 27s\tremaining: 12m 8s\n13:\tlearn: 1.0853660\ttotal: 1m 28s\tremaining: 12m 12s\n73:\tlearn: 0.8609547\ttotal: 7m 59s\tremaining: 38.9s\n73:\tlearn: 0.8615900\ttotal: 7m 59s\tremaining: 38.9s\n14:\tlearn: 1.0849908\ttotal: 1m 34s\tremaining: 12m 1s\n74:\tlearn: 0.8581990\ttotal: 8m 5s\tremaining: 32.4s\n14:\tlearn: 1.0844368\ttotal: 1m 35s\tremaining: 12m 8s\n74:\tlearn: 0.8587079\ttotal: 8m 6s\tremaining: 32.4s\n15:\tlearn: 1.0841702\ttotal: 1m 40s\tremaining: 11m 57s\n75:\tlearn: 0.8552086\ttotal: 8m 12s\tremaining: 25.9s\n15:\tlearn: 1.0833189\ttotal: 1m 41s\tremaining: 12m 2s\n75:\tlearn: 0.8564424\ttotal: 8m 12s\tremaining: 25.9s\n16:\tlearn: 1.0833193\ttotal: 1m 46s\tremaining: 11m 50s\n76:\tlearn: 0.8529961\ttotal: 8m 18s\tremaining: 19.4s\n16:\tlearn: 1.0824086\ttotal: 1m 47s\tremaining: 11m 54s\n76:\tlearn: 0.8538771\ttotal: 8m 18s\tremaining: 19.4s\n17:\tlearn: 1.0824660\ttotal: 1m 53s\tremaining: 11m 43s\n77:\tlearn: 0.8500359\ttotal: 8m 24s\tremaining: 12.9s\n17:\tlearn: 1.0814730\ttotal: 1m 53s\tremaining: 11m 47s\n77:\tlearn: 0.8514279\ttotal: 8m 24s\tremaining: 12.9s\n18:\tlearn: 1.0815768\ttotal: 1m 59s\tremaining: 11m 36s\n78:\tlearn: 0.8472467\ttotal: 8m 30s\tremaining: 6.46s\n18:\tlearn: 1.0806339\ttotal: 1m 59s\tremaining: 11m 40s\n78:\tlearn: 0.8485221\ttotal: 8m 31s\tremaining: 6.47s\n19:\tlearn: 1.0806732\ttotal: 2m 5s\tremaining: 11m 29s\n79:\tlearn: 0.8443349\ttotal: 8m 36s\tremaining: 0us\n19:\tlearn: 1.0796596\ttotal: 2m 6s\tremaining: 11m 33s\n79:\tlearn: 0.8457442\ttotal: 8m 37s\tremaining: 0us\n20:\tlearn: 1.0797029\ttotal: 2m 10s\tremaining: 11m 16s\n20:\tlearn: 1.0787049\ttotal: 2m 9s\tremaining: 11m 12s\n21:\tlearn: 1.0788166\ttotal: 2m 13s\tremaining: 10m 54s\n21:\tlearn: 1.0778322\ttotal: 2m 12s\tremaining: 10m 50s\n22:\tlearn: 1.0778901\ttotal: 2m 16s\tremaining: 10m 34s\n22:\tlearn: 1.0768549\ttotal: 2m 15s\tremaining: 10m 30s\n23:\tlearn: 1.0770089\ttotal: 2m 19s\tremaining: 10m 15s\n23:\tlearn: 1.0760176\ttotal: 2m 18s\tremaining: 10m 12s\n24:\tlearn: 1.0761592\ttotal: 2m 22s\tremaining: 9m 58s\n24:\tlearn: 1.0751584\ttotal: 2m 21s\tremaining: 9m 54s\n25:\tlearn: 1.0752815\ttotal: 2m 25s\tremaining: 9m 41s\n25:\tlearn: 1.0742729\ttotal: 2m 24s\tremaining: 9m 38s\n26:\tlearn: 1.0743341\ttotal: 2m 28s\tremaining: 9m 26s\n26:\tlearn: 1.0734363\ttotal: 2m 27s\tremaining: 9m 23s\n27:\tlearn: 1.0734642\ttotal: 2m 31s\tremaining: 9m 11s\n27:\tlearn: 1.0724586\ttotal: 2m 30s\tremaining: 9m 9s\n28:\tlearn: 1.0726228\ttotal: 2m 34s\tremaining: 8m 57s\n28:\tlearn: 1.0716471\ttotal: 2m 33s\tremaining: 8m 55s\n29:\tlearn: 1.0717252\ttotal: 2m 37s\tremaining: 8m 44s\n29:\tlearn: 1.0708025\ttotal: 2m 36s\tremaining: 8m 42s\n30:\tlearn: 1.0708852\ttotal: 2m 40s\tremaining: 8m 33s\n30:\tlearn: 1.0699294\ttotal: 2m 40s\tremaining: 8m 31s\n31:\tlearn: 1.0700674\ttotal: 2m 43s\tremaining: 8m 22s\n31:\tlearn: 1.0690320\ttotal: 2m 43s\tremaining: 8m 19s\n32:\tlearn: 1.0691547\ttotal: 2m 46s\tremaining: 8m 10s\n32:\tlearn: 1.0682069\ttotal: 2m 46s\tremaining: 8m 8s\n33:\tlearn: 1.0683286\ttotal: 2m 49s\tremaining: 7m 59s\n33:\tlearn: 1.0673387\ttotal: 2m 49s\tremaining: 7m 58s\n34:\tlearn: 1.0674946\ttotal: 2m 53s\tremaining: 7m 49s\n34:\tlearn: 1.0664158\ttotal: 2m 52s\tremaining: 7m 47s\n35:\tlearn: 1.0665909\ttotal: 2m 56s\tremaining: 7m 39s\n35:\tlearn: 1.0655993\ttotal: 2m 55s\tremaining: 7m 38s\n36:\tlearn: 1.0657238\ttotal: 2m 59s\tremaining: 7m 30s\n36:\tlearn: 1.0646150\ttotal: 2m 58s\tremaining: 7m 28s\n37:\tlearn: 1.0649320\ttotal: 3m 2s\tremaining: 7m 20s\n37:\tlearn: 1.0637564\ttotal: 3m 1s\tremaining: 7m 19s\n38:\tlearn: 1.0640630\ttotal: 3m 4s\tremaining: 7m 11s\n38:\tlearn: 1.0629071\ttotal: 3m 4s\tremaining: 7m 10s\n39:\tlearn: 1.0631912\ttotal: 3m 7s\tremaining: 7m 2s\n39:\tlearn: 1.0620660\ttotal: 3m 7s\tremaining: 7m 1s\n40:\tlearn: 1.0622234\ttotal: 3m 10s\tremaining: 6m 54s\n40:\tlearn: 1.0611399\ttotal: 3m 10s\tremaining: 6m 54s\n41:\tlearn: 1.0614190\ttotal: 3m 14s\tremaining: 6m 46s\n41:\tlearn: 1.0601999\ttotal: 3m 14s\tremaining: 6m 46s\n42:\tlearn: 1.0605245\ttotal: 3m 17s\tremaining: 6m 38s\n42:\tlearn: 1.0593985\ttotal: 3m 17s\tremaining: 6m 38s\n43:\tlearn: 1.0595922\ttotal: 3m 20s\tremaining: 6m 31s\n43:\tlearn: 1.0585661\ttotal: 3m 20s\tremaining: 6m 31s\n44:\tlearn: 1.0587444\ttotal: 3m 23s\tremaining: 6m 23s\n44:\tlearn: 1.0576297\ttotal: 3m 23s\tremaining: 6m 23s\n45:\tlearn: 1.0578538\ttotal: 3m 26s\tremaining: 6m 16s\n45:\tlearn: 1.0568319\ttotal: 3m 26s\tremaining: 6m 16s\n46:\tlearn: 1.0569987\ttotal: 3m 29s\tremaining: 6m 9s\n46:\tlearn: 1.0559229\ttotal: 3m 29s\tremaining: 6m 9s\n47:\tlearn: 1.0561528\ttotal: 3m 32s\tremaining: 6m 2s\n47:\tlearn: 1.0551254\ttotal: 3m 32s\tremaining: 6m 2s\n48:\tlearn: 1.0552740\ttotal: 3m 35s\tremaining: 5m 55s\n48:\tlearn: 1.0541980\ttotal: 3m 35s\tremaining: 5m 55s\n49:\tlearn: 1.0544403\ttotal: 3m 38s\tremaining: 5m 48s\n49:\tlearn: 1.0532638\ttotal: 3m 38s\tremaining: 5m 49s\n50:\tlearn: 1.0535972\ttotal: 3m 40s\tremaining: 5m 42s\n50:\tlearn: 1.0523749\ttotal: 3m 41s\tremaining: 5m 42s\n51:\tlearn: 1.0525031\ttotal: 3m 44s\tremaining: 5m 36s\n51:\tlearn: 1.0514553\ttotal: 3m 44s\tremaining: 5m 37s\n52:\tlearn: 1.0517161\ttotal: 3m 47s\tremaining: 5m 30s\n52:\tlearn: 1.0506518\ttotal: 3m 47s\tremaining: 5m 31s\n53:\tlearn: 1.0509684\ttotal: 3m 50s\tremaining: 5m 24s\n53:\tlearn: 1.0497881\ttotal: 3m 50s\tremaining: 5m 25s\n54:\tlearn: 1.0501019\ttotal: 3m 53s\tremaining: 5m 18s\n54:\tlearn: 1.0489113\ttotal: 3m 53s\tremaining: 5m 19s\n55:\tlearn: 1.0491372\ttotal: 3m 56s\tremaining: 5m 12s\n55:\tlearn: 1.0479447\ttotal: 3m 56s\tremaining: 5m 13s\n56:\tlearn: 1.0483286\ttotal: 3m 59s\tremaining: 5m 6s\n56:\tlearn: 1.0471380\ttotal: 3m 59s\tremaining: 5m 7s\n57:\tlearn: 1.0475853\ttotal: 4m 2s\tremaining: 5m 1s\n57:\tlearn: 1.0463555\ttotal: 4m 2s\tremaining: 5m 1s\n58:\tlearn: 1.0466495\ttotal: 4m 5s\tremaining: 4m 55s\n58:\tlearn: 1.0455360\ttotal: 4m 5s\tremaining: 4m 55s\n59:\tlearn: 1.0458478\ttotal: 4m 8s\tremaining: 4m 50s\n59:\tlearn: 1.0447174\ttotal: 4m 8s\tremaining: 4m 50s\n60:\tlearn: 1.0449804\ttotal: 4m 11s\tremaining: 4m 44s\n60:\tlearn: 1.0438577\ttotal: 4m 11s\tremaining: 4m 44s\n61:\tlearn: 1.0442174\ttotal: 4m 14s\tremaining: 4m 39s\n61:\tlearn: 1.0430197\ttotal: 4m 15s\tremaining: 4m 39s\n62:\tlearn: 1.0434154\ttotal: 4m 18s\tremaining: 4m 34s\n62:\tlearn: 1.0421248\ttotal: 4m 18s\tremaining: 4m 34s\n63:\tlearn: 1.0425575\ttotal: 4m 21s\tremaining: 4m 29s\n63:\tlearn: 1.0412023\ttotal: 4m 21s\tremaining: 4m 29s\n64:\tlearn: 1.0417534\ttotal: 4m 24s\tremaining: 4m 24s\n64:\tlearn: 1.0403668\ttotal: 4m 24s\tremaining: 4m 24s\n65:\tlearn: 1.0409164\ttotal: 4m 27s\tremaining: 4m 18s\n65:\tlearn: 1.0395805\ttotal: 4m 27s\tremaining: 4m 19s\n66:\tlearn: 1.0401324\ttotal: 4m 30s\tremaining: 4m 13s\n66:\tlearn: 1.0387828\ttotal: 4m 30s\tremaining: 4m 14s\n67:\tlearn: 1.0394047\ttotal: 4m 33s\tremaining: 4m 8s\n67:\tlearn: 1.0379994\ttotal: 4m 33s\tremaining: 4m 9s\n68:\tlearn: 1.0385679\ttotal: 4m 36s\tremaining: 4m 4s\n68:\tlearn: 1.0371823\ttotal: 4m 36s\tremaining: 4m 4s\n69:\tlearn: 1.0377928\ttotal: 4m 39s\tremaining: 3m 59s\n69:\tlearn: 1.0364365\ttotal: 4m 39s\tremaining: 3m 59s\n70:\tlearn: 1.0369682\ttotal: 4m 42s\tremaining: 3m 54s\n70:\tlearn: 1.0355963\ttotal: 4m 42s\tremaining: 3m 54s\n71:\tlearn: 1.0361940\ttotal: 4m 45s\tremaining: 3m 49s\n71:\tlearn: 1.0347964\ttotal: 4m 45s\tremaining: 3m 49s\n72:\tlearn: 1.0354158\ttotal: 4m 48s\tremaining: 3m 45s\n72:\tlearn: 1.0340148\ttotal: 4m 48s\tremaining: 3m 45s\n73:\tlearn: 1.0346368\ttotal: 4m 51s\tremaining: 3m 40s\n73:\tlearn: 1.0332073\ttotal: 4m 51s\tremaining: 3m 40s\n74:\tlearn: 1.0338170\ttotal: 4m 54s\tremaining: 3m 36s\n74:\tlearn: 1.0323904\ttotal: 4m 54s\tremaining: 3m 36s\n75:\tlearn: 1.0329994\ttotal: 4m 57s\tremaining: 3m 31s\n75:\tlearn: 1.0315506\ttotal: 4m 57s\tremaining: 3m 31s\n76:\tlearn: 1.0322834\ttotal: 5m\tremaining: 3m 26s\n76:\tlearn: 1.0307860\ttotal: 5m\tremaining: 3m 27s\n77:\tlearn: 1.0315122\ttotal: 5m 3s\tremaining: 3m 22s\n77:\tlearn: 1.0298974\ttotal: 5m 3s\tremaining: 3m 22s\n78:\tlearn: 1.0307107\ttotal: 5m 6s\tremaining: 3m 18s\n78:\tlearn: 1.0291073\ttotal: 5m 6s\tremaining: 3m 18s\n79:\tlearn: 1.0298914\ttotal: 5m 9s\tremaining: 3m 13s\n79:\tlearn: 1.0283482\ttotal: 5m 9s\tremaining: 3m 13s\n80:\tlearn: 1.0291041\ttotal: 5m 12s\tremaining: 3m 9s\n80:\tlearn: 1.0274531\ttotal: 5m 12s\tremaining: 3m 9s\n81:\tlearn: 1.0283152\ttotal: 5m 15s\tremaining: 3m 4s\n81:\tlearn: 1.0266315\ttotal: 5m 15s\tremaining: 3m 4s\n82:\tlearn: 1.0275737\ttotal: 5m 18s\tremaining: 3m\n82:\tlearn: 1.0258564\ttotal: 5m 19s\tremaining: 3m\n83:\tlearn: 1.0267699\ttotal: 5m 21s\tremaining: 2m 56s\n83:\tlearn: 1.0249415\ttotal: 5m 22s\tremaining: 2m 56s\n84:\tlearn: 1.0259761\ttotal: 5m 24s\tremaining: 2m 51s\n84:\tlearn: 1.0241601\ttotal: 5m 25s\tremaining: 2m 52s\n85:\tlearn: 1.0252015\ttotal: 5m 27s\tremaining: 2m 47s\n85:\tlearn: 1.0233673\ttotal: 5m 28s\tremaining: 2m 48s\n86:\tlearn: 1.0243208\ttotal: 5m 30s\tremaining: 2m 43s\n86:\tlearn: 1.0225535\ttotal: 5m 31s\tremaining: 2m 43s\n87:\tlearn: 1.0235097\ttotal: 5m 33s\tremaining: 2m 39s\n87:\tlearn: 1.0217673\ttotal: 5m 34s\tremaining: 2m 39s\n88:\tlearn: 1.0226650\ttotal: 5m 36s\tremaining: 2m 35s\n88:\tlearn: 1.0209945\ttotal: 5m 37s\tremaining: 2m 35s\n89:\tlearn: 1.0219029\ttotal: 5m 39s\tremaining: 2m 30s\n89:\tlearn: 1.0202015\ttotal: 5m 40s\tremaining: 2m 31s\n90:\tlearn: 1.0210280\ttotal: 5m 42s\tremaining: 2m 26s\n90:\tlearn: 1.0194192\ttotal: 5m 43s\tremaining: 2m 27s\n91:\tlearn: 1.0202103\ttotal: 5m 45s\tremaining: 2m 22s\n91:\tlearn: 1.0186046\ttotal: 5m 46s\tremaining: 2m 23s\n92:\tlearn: 1.0194189\ttotal: 5m 48s\tremaining: 2m 18s\n92:\tlearn: 1.0178537\ttotal: 5m 49s\tremaining: 2m 19s\n93:\tlearn: 1.0186371\ttotal: 5m 51s\tremaining: 2m 14s\n93:\tlearn: 1.0170950\ttotal: 5m 53s\tremaining: 2m 15s\n94:\tlearn: 1.0178379\ttotal: 5m 55s\tremaining: 2m 10s\n94:\tlearn: 1.0163461\ttotal: 5m 56s\tremaining: 2m 11s\n95:\tlearn: 1.0170425\ttotal: 5m 58s\tremaining: 2m 6s\n95:\tlearn: 1.0156111\ttotal: 5m 59s\tremaining: 2m 7s\n96:\tlearn: 1.0162487\ttotal: 6m 1s\tremaining: 2m 2s\n96:\tlearn: 1.0148308\ttotal: 6m 2s\tremaining: 2m 3s\n97:\tlearn: 1.0154232\ttotal: 6m 4s\tremaining: 1m 58s\n97:\tlearn: 1.0140227\ttotal: 6m 5s\tremaining: 1m 59s\n98:\tlearn: 1.0146677\ttotal: 6m 7s\tremaining: 1m 55s\n98:\tlearn: 1.0132195\ttotal: 6m 8s\tremaining: 1m 55s\n99:\tlearn: 1.0138652\ttotal: 6m 10s\tremaining: 1m 51s\n99:\tlearn: 1.0124573\ttotal: 6m 11s\tremaining: 1m 51s\n100:\tlearn: 1.0130953\ttotal: 6m 13s\tremaining: 1m 47s\n100:\tlearn: 1.0117170\ttotal: 6m 14s\tremaining: 1m 47s\n101:\tlearn: 1.0123653\ttotal: 6m 16s\tremaining: 1m 43s\n101:\tlearn: 1.0109656\ttotal: 6m 17s\tremaining: 1m 43s\n102:\tlearn: 1.0115330\ttotal: 6m 19s\tremaining: 1m 39s\n102:\tlearn: 1.0101683\ttotal: 6m 20s\tremaining: 1m 39s\n103:\tlearn: 1.0107726\ttotal: 6m 22s\tremaining: 1m 35s\n103:\tlearn: 1.0094428\ttotal: 6m 23s\tremaining: 1m 35s\n104:\tlearn: 1.0100124\ttotal: 6m 25s\tremaining: 1m 31s\n104:\tlearn: 1.0086762\ttotal: 6m 26s\tremaining: 1m 32s\n105:\tlearn: 1.0092791\ttotal: 6m 28s\tremaining: 1m 28s\n105:\tlearn: 1.0079553\ttotal: 6m 29s\tremaining: 1m 28s\n106:\tlearn: 1.0084951\ttotal: 6m 31s\tremaining: 1m 24s\n106:\tlearn: 1.0070620\ttotal: 6m 32s\tremaining: 1m 24s\n107:\tlearn: 1.0077842\ttotal: 6m 34s\tremaining: 1m 20s\n107:\tlearn: 1.0062673\ttotal: 6m 35s\tremaining: 1m 20s\n108:\tlearn: 1.0070732\ttotal: 6m 37s\tremaining: 1m 16s\n108:\tlearn: 1.0055528\ttotal: 6m 38s\tremaining: 1m 16s\n109:\tlearn: 1.0062561\ttotal: 6m 40s\tremaining: 1m 12s\n109:\tlearn: 1.0048016\ttotal: 6m 41s\tremaining: 1m 13s\n110:\tlearn: 1.0055147\ttotal: 6m 43s\tremaining: 1m 9s\n110:\tlearn: 1.0040567\ttotal: 6m 44s\tremaining: 1m 9s\n111:\tlearn: 1.0046979\ttotal: 6m 47s\tremaining: 1m 5s\n111:\tlearn: 1.0032940\ttotal: 6m 47s\tremaining: 1m 5s\n112:\tlearn: 1.0039448\ttotal: 6m 50s\tremaining: 1m 1s\n112:\tlearn: 1.0025272\ttotal: 6m 51s\tremaining: 1m 1s\n113:\tlearn: 1.0031380\ttotal: 6m 53s\tremaining: 58s\n113:\tlearn: 1.0017125\ttotal: 6m 54s\tremaining: 58.2s\n114:\tlearn: 1.0023509\ttotal: 6m 56s\tremaining: 54.3s\n114:\tlearn: 1.0008287\ttotal: 6m 57s\tremaining: 54.5s\n115:\tlearn: 1.0015453\ttotal: 6m 59s\tremaining: 50.6s\n115:\tlearn: 1.0000480\ttotal: 7m\tremaining: 50.8s\n116:\tlearn: 1.0007804\ttotal: 7m 2s\tremaining: 47s\n116:\tlearn: 0.9992760\ttotal: 7m 3s\tremaining: 47.1s\n117:\tlearn: 0.9999977\ttotal: 7m 5s\tremaining: 43.3s\n117:\tlearn: 0.9986195\ttotal: 7m 6s\tremaining: 43.4s\n118:\tlearn: 0.9992327\ttotal: 7m 8s\tremaining: 39.6s\n118:\tlearn: 0.9977978\ttotal: 7m 9s\tremaining: 39.7s\n119:\tlearn: 0.9984726\ttotal: 7m 11s\tremaining: 36s\n119:\tlearn: 0.9970554\ttotal: 7m 12s\tremaining: 36.1s\n120:\tlearn: 0.9977033\ttotal: 7m 15s\tremaining: 32.4s\n120:\tlearn: 0.9962919\ttotal: 7m 15s\tremaining: 32.4s\n121:\tlearn: 0.9969963\ttotal: 7m 18s\tremaining: 28.7s\n121:\tlearn: 0.9955661\ttotal: 7m 18s\tremaining: 28.8s\n122:\tlearn: 0.9961759\ttotal: 7m 21s\tremaining: 25.1s\n122:\tlearn: 0.9948183\ttotal: 7m 21s\tremaining: 25.1s\n123:\tlearn: 0.9953903\ttotal: 7m 24s\tremaining: 21.5s\n123:\tlearn: 0.9941426\ttotal: 7m 24s\tremaining: 21.5s\n124:\tlearn: 0.9946587\ttotal: 7m 27s\tremaining: 17.9s\n124:\tlearn: 0.9934286\ttotal: 7m 28s\tremaining: 17.9s\n125:\tlearn: 0.9938645\ttotal: 7m 30s\tremaining: 14.3s\n125:\tlearn: 0.9926482\ttotal: 7m 30s\tremaining: 14.3s\n126:\tlearn: 0.9930407\ttotal: 7m 33s\tremaining: 10.7s\n126:\tlearn: 0.9917945\ttotal: 7m 33s\tremaining: 10.7s\n127:\tlearn: 0.9922964\ttotal: 7m 36s\tremaining: 7.14s\n127:\tlearn: 0.9910332\ttotal: 7m 37s\tremaining: 7.14s\n128:\tlearn: 0.9915291\ttotal: 7m 39s\tremaining: 3.56s\n128:\tlearn: 0.9903249\ttotal: 7m 40s\tremaining: 3.56s\n129:\tlearn: 0.9907519\ttotal: 7m 42s\tremaining: 0us\n129:\tlearn: 0.9896013\ttotal: 7m 42s\tremaining: 0us\n0:\tlearn: 1.0945665\ttotal: 1.58s\tremaining: 2m 4s\n1:\tlearn: 1.0908505\ttotal: 3.07s\tremaining: 1m 59s\n2:\tlearn: 1.0875128\ttotal: 4.61s\tremaining: 1m 58s\n3:\tlearn: 1.0838355\ttotal: 6.11s\tremaining: 1m 56s\n4:\tlearn: 1.0802398\ttotal: 7.61s\tremaining: 1m 54s\n5:\tlearn: 1.0770320\ttotal: 9.12s\tremaining: 1m 52s\n6:\tlearn: 1.0740191\ttotal: 10.6s\tremaining: 1m 50s\n7:\tlearn: 1.0711428\ttotal: 12.2s\tremaining: 1m 49s\n8:\tlearn: 1.0679274\ttotal: 13.7s\tremaining: 1m 47s\n9:\tlearn: 1.0641241\ttotal: 15.2s\tremaining: 1m 46s\n10:\tlearn: 1.0611494\ttotal: 17.1s\tremaining: 1m 47s\n11:\tlearn: 1.0579340\ttotal: 18.6s\tremaining: 1m 45s\n12:\tlearn: 1.0547819\ttotal: 20.1s\tremaining: 1m 43s\n13:\tlearn: 1.0517671\ttotal: 21.6s\tremaining: 1m 42s\n14:\tlearn: 1.0489305\ttotal: 23.1s\tremaining: 1m 40s\n15:\tlearn: 1.0457807\ttotal: 24.6s\tremaining: 1m 38s\n16:\tlearn: 1.0424428\ttotal: 26.2s\tremaining: 1m 37s\n17:\tlearn: 1.0391708\ttotal: 27.8s\tremaining: 1m 35s\n18:\tlearn: 1.0363356\ttotal: 29.3s\tremaining: 1m 33s\n19:\tlearn: 1.0332268\ttotal: 30.8s\tremaining: 1m 32s\n20:\tlearn: 1.0304715\ttotal: 32.3s\tremaining: 1m 30s\n21:\tlearn: 1.0276654\ttotal: 33.8s\tremaining: 1m 29s\n22:\tlearn: 1.0244877\ttotal: 35.3s\tremaining: 1m 27s\n23:\tlearn: 1.0212672\ttotal: 36.8s\tremaining: 1m 25s\n24:\tlearn: 1.0184092\ttotal: 38.3s\tremaining: 1m 24s\n25:\tlearn: 1.0156594\ttotal: 39.8s\tremaining: 1m 22s\n26:\tlearn: 1.0131307\ttotal: 41.3s\tremaining: 1m 21s\n27:\tlearn: 1.0100435\ttotal: 42.8s\tremaining: 1m 19s\n28:\tlearn: 1.0066535\ttotal: 44.3s\tremaining: 1m 17s\n29:\tlearn: 1.0040620\ttotal: 45.8s\tremaining: 1m 16s\n30:\tlearn: 1.0010620\ttotal: 47.3s\tremaining: 1m 14s\n31:\tlearn: 0.9978408\ttotal: 49.2s\tremaining: 1m 13s\n32:\tlearn: 0.9955388\ttotal: 50.7s\tremaining: 1m 12s\n33:\tlearn: 0.9925959\ttotal: 52.2s\tremaining: 1m 10s\n34:\tlearn: 0.9897467\ttotal: 53.7s\tremaining: 1m 9s\n35:\tlearn: 0.9866990\ttotal: 55.2s\tremaining: 1m 7s\n36:\tlearn: 0.9834116\ttotal: 56.7s\tremaining: 1m 5s\n37:\tlearn: 0.9806530\ttotal: 58.2s\tremaining: 1m 4s\n38:\tlearn: 0.9780283\ttotal: 59.8s\tremaining: 1m 2s\n39:\tlearn: 0.9751289\ttotal: 1m 1s\tremaining: 1m 1s\n40:\tlearn: 0.9730563\ttotal: 1m 2s\tremaining: 59.7s\n41:\tlearn: 0.9704377\ttotal: 1m 4s\tremaining: 58.2s\n42:\tlearn: 0.9676389\ttotal: 1m 5s\tremaining: 56.6s\n43:\tlearn: 0.9650289\ttotal: 1m 7s\tremaining: 55.1s\n44:\tlearn: 0.9624314\ttotal: 1m 8s\tremaining: 53.6s\n45:\tlearn: 0.9596933\ttotal: 1m 10s\tremaining: 52.1s\n46:\tlearn: 0.9576477\ttotal: 1m 11s\tremaining: 50.5s\n47:\tlearn: 0.9547844\ttotal: 1m 13s\tremaining: 49s\n48:\tlearn: 0.9522767\ttotal: 1m 14s\tremaining: 47.4s\n49:\tlearn: 0.9491832\ttotal: 1m 16s\tremaining: 45.9s\n50:\tlearn: 0.9465956\ttotal: 1m 18s\tremaining: 44.4s\n51:\tlearn: 0.9436532\ttotal: 1m 19s\tremaining: 42.9s\n52:\tlearn: 0.9408645\ttotal: 1m 21s\tremaining: 41.5s\n53:\tlearn: 0.9380094\ttotal: 1m 22s\tremaining: 39.9s\n54:\tlearn: 0.9352738\ttotal: 1m 24s\tremaining: 38.4s\n55:\tlearn: 0.9328038\ttotal: 1m 25s\tremaining: 36.8s\n56:\tlearn: 0.9305858\ttotal: 1m 27s\tremaining: 35.3s\n57:\tlearn: 0.9286992\ttotal: 1m 28s\tremaining: 33.7s\n58:\tlearn: 0.9263145\ttotal: 1m 30s\tremaining: 32.2s\n59:\tlearn: 0.9243954\ttotal: 1m 31s\tremaining: 30.6s\n60:\tlearn: 0.9221118\ttotal: 1m 33s\tremaining: 29.1s\n61:\tlearn: 0.9199167\ttotal: 1m 34s\tremaining: 27.6s\n62:\tlearn: 0.9171528\ttotal: 1m 36s\tremaining: 26s\n63:\tlearn: 0.9146575\ttotal: 1m 37s\tremaining: 24.5s\n64:\tlearn: 0.9120058\ttotal: 1m 39s\tremaining: 22.9s\n65:\tlearn: 0.9096104\ttotal: 1m 40s\tremaining: 21.4s\n66:\tlearn: 0.9073592\ttotal: 1m 42s\tremaining: 19.9s\n67:\tlearn: 0.9052805\ttotal: 1m 43s\tremaining: 18.3s\n68:\tlearn: 0.9031056\ttotal: 1m 45s\tremaining: 16.8s\n69:\tlearn: 0.9005247\ttotal: 1m 46s\tremaining: 15.3s\n70:\tlearn: 0.8981321\ttotal: 1m 48s\tremaining: 13.7s\n71:\tlearn: 0.8959608\ttotal: 1m 49s\tremaining: 12.2s\n72:\tlearn: 0.8938885\ttotal: 1m 51s\tremaining: 10.7s\n73:\tlearn: 0.8914146\ttotal: 1m 53s\tremaining: 9.18s\n74:\tlearn: 0.8890808\ttotal: 1m 54s\tremaining: 7.65s\n75:\tlearn: 0.8865721\ttotal: 1m 56s\tremaining: 6.12s\n76:\tlearn: 0.8847276\ttotal: 1m 57s\tremaining: 4.59s\n77:\tlearn: 0.8821194\ttotal: 1m 59s\tremaining: 3.06s\n78:\tlearn: 0.8796801\ttotal: 2m\tremaining: 1.53s\n79:\tlearn: 0.8773990\ttotal: 2m 2s\tremaining: 0us\nBest params for cat:\n{'n_estimators': 80, 'max_depth': 10, 'loss_function': 'MultiClass', 'learning_rate': 0.01, 'l2_leaf_reg': 1}\nClassification report for cat:\n              precision    recall  f1-score   support\n\n        -1.0       1.00      0.04      0.08       147\n         0.0       0.60      0.59      0.60       241\n         1.0       0.47      0.79      0.59       212\n\n    accuracy                           0.53       600\n   macro avg       0.69      0.48      0.42       600\nweighted avg       0.65      0.53      0.47       600\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#RF+KNN+MLP\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom joblib import parallel_backend\n\n'''\nThis code defines a stacking ensemble using RandomForestClassifier, KNeighborsClassifier, and \nMLPClassifier as base models, and MLPClassifier as the final estimator. It then uses a GridSearchCV \nobject to search over a parameter grid for the best hyperparameters. \nFinally, it prints the classification report for the predictions made on the test set.\n'''\n\n# define base models\nmodel1 = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = MLPClassifier(hidden_layer_sizes=(32, 32), random_state=42)\n\n# define stacking ensemble\nestimators = [('rf', model1),\n              ('knn', model2),\n              ('mlp', model3)]\n\nclf = StackingClassifier(estimators=estimators, \n                         final_estimator=MLPClassifier(hidden_layer_sizes=(64, 64),\n                                                       random_state=42))\n\n\n# define parameter grid for randomized search\nparam_dist = {'rf__max_depth': [20,40,60],\n              'rf__max_features': ['sqrt'],\n              'rf__min_samples_leaf': [1, 2, 5, 10],\n              'rf__min_samples_split': [2, 5, 10, 15],\n              'knn__n_neighbors': [3, 5, 7],\n              'mlp__alpha': [0.001, 0.01, 0.1, 1],\n              'mlp__learning_rate_init': [0.001, 0.01, 0.1],\n              'final_estimator__alpha': [0.001, 0.01, 0.1, 1],\n              'final_estimator__learning_rate_init': [0.001, 0.01, 0.1]}\n\n# define RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=clf, \n                             param_distributions=param_dist, \n                             cv=3, \n                             n_jobs=-1, \n                             verbose=2)\n\n# fit the RandomizedSearchCV object to the data\nwith parallel_backend('multiprocessing'):\n    random_search.fit(x_train, y_train)\n\n# make predictions on test set\ny_pred = random_search.predict(x_test)\n\n# print classification report\nprint(classification_report(y_test, y_pred))\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-05-17T19:36:17.940997Z","iopub.execute_input":"2023-05-17T19:36:17.941446Z","iopub.status.idle":"2023-05-17T19:57:06.923552Z","shell.execute_reply.started":"2023-05-17T19:36:17.941401Z","shell.execute_reply":"2023-05-17T19:57:06.920945Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 10 candidates, totalling 30 fits\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=5; total time=  28.1s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=5; total time=  30.4s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=5; total time=  30.3s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=2; total time=  29.4s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=2; total time=  29.4s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.1, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=2; total time=  30.6s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.001, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=10; total time=  54.6s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.001, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=10; total time=  52.7s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.001, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=10, rf__min_samples_split=10; total time=  53.6s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.01, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=10; total time=  31.5s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.01, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=10; total time=  30.2s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.001, mlp__learning_rate_init=0.01, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=10; total time=  27.3s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=15; total time=  30.9s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=15; total time=  29.7s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=15; total time=  28.9s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=7, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=10; total time=  31.2s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=7, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=10; total time=  29.7s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=7, mlp__alpha=0.01, mlp__learning_rate_init=0.01, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=10; total time=  29.2s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.001, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=5; total time=  48.4s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.001, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=5; total time=  46.1s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.001, mlp__learning_rate_init=0.001, rf__max_depth=40, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=5; total time=  48.3s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.001, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  50.7s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.001, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  47.5s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=0.001, mlp__learning_rate_init=0.001, rf__max_depth=60, rf__max_features=sqrt, rf__min_samples_leaf=1, rf__min_samples_split=5; total time=  48.9s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=15; total time=  37.3s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=15; total time=  39.0s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=1, mlp__learning_rate_init=0.01, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=2, rf__min_samples_split=15; total time=  37.0s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.001, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=2; total time=  53.3s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.001, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=2; total time=  52.5s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.001, rf__max_depth=20, rf__max_features=sqrt, rf__min_samples_leaf=5, rf__min_samples_split=2; total time=  53.8s\n              precision    recall  f1-score   support\n\n        -1.0       0.50      0.29      0.36       147\n         0.0       0.64      0.62      0.63       241\n         1.0       0.54      0.72      0.61       212\n\n    accuracy                           0.57       600\n   macro avg       0.56      0.54      0.54       600\nweighted avg       0.57      0.57      0.56       600\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom joblib import parallel_backend\n\n'''\nThis code defines a stacking ensemble using LGBM, KNeighborsClassifier, and MLPClassifier \nas base models, and MLPClassifier as the final estimator. It then uses a GridSearchCV object \nto search over a parameter grid for the best hyperparameters. Finally, it prints \nthe classification report for the predictions made on the test set.\n'''\nx_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny_test = y_test.astype(np.float32)\n\n# define base models\nmodel1 = lgbm.LGBMClassifier(n_estimators=100, random_state=42, \n                          colsample_bytree = 0.6,\n                          subsample = 0.8,\n                          min_child_samples = 1,\n                          objective = 'multiclass',\n                          num_leaves = 20,\n                          max_depth = 10,\n                          learning_rate = 0.5\n                         ) #RandomForestClassifier(n_estimators=100, random_state=42)\nmodel2 = KNeighborsClassifier(n_neighbors=5)\nmodel3 = MLPClassifier(hidden_layer_sizes=(32, 32), random_state=42)\n\n# define stacking ensemble\nestimators = [('lg', model1),\n              ('knn', model2),\n              ('mlp', model3)]\n\nclf = StackingClassifier(estimators=estimators, \n                         final_estimator=MLPClassifier(hidden_layer_sizes=(64, 64),\n                                                       random_state=42))\n\n\n# define parameter grid for randomized search\nparam_dist = {\n              'knn__n_neighbors': [3, 5, 7],\n              'mlp__alpha': [0.001, 0.01, 0.1, 1],\n              'mlp__learning_rate_init': [0.001, 0.01, 0.1],\n              'final_estimator__alpha': [0.001, 0.01, 0.1, 1],\n              'final_estimator__learning_rate_init': [0.001, 0.01, 0.1]}\n\n# define RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(estimator=clf, \n                             param_distributions=param_dist, \n                             cv=3, \n                             n_jobs=-1, \n                             verbose=2)\n\n# fit the RandomizedSearchCV object to the data\nwith parallel_backend('multiprocessing'):\n      random_search.fit(x_train, y_train)\n\n# make predictions on test set\ny_pred = random_search.predict(x_test)\n\n# print classification report\nprint(classification_report(y_test, y_pred))\n","metadata":{"id":"g6Fa8sZK_2TG","scrolled":true,"execution":{"iopub.status.busy":"2023-05-17T19:57:06.931620Z","iopub.execute_input":"2023-05-17T19:57:06.934880Z","iopub.status.idle":"2023-05-17T20:26:02.911969Z","shell.execute_reply.started":"2023-05-17T19:57:06.934829Z","shell.execute_reply":"2023-05-17T20:26:02.909679Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 10 candidates, totalling 30 fits\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1; total time=  43.9s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1; total time=  40.0s\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.01, mlp__learning_rate_init=0.1; total time=  43.0s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.001; total time= 1.3min\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.001; total time= 1.3min\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.001; total time= 1.3min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 1.2min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 1.1min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.1, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 1.1min\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time=  50.5s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time=  47.7s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=7, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time=  47.5s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.1; total time=  47.0s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.1; total time=  44.5s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.1; total time=  43.6s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.1; total time=  48.2s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.1; total time=  43.9s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=7, mlp__alpha=1, mlp__learning_rate_init=0.1; total time=  43.6s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.1; total time=  38.9s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.1; total time=  37.3s\n[CV] END final_estimator__alpha=0.01, final_estimator__learning_rate_init=0.1, knn__n_neighbors=3, mlp__alpha=0.1, mlp__learning_rate_init=0.1; total time=  39.6s\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 1.2min\n[CV] END final_estimator__alpha=0.1, final_estimator__learning_rate_init=0.1, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.001; total time= 1.1min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 1.2min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 1.1min\n[CV] END final_estimator__alpha=1, final_estimator__learning_rate_init=0.001, knn__n_neighbors=3, mlp__alpha=0.001, mlp__learning_rate_init=0.001; total time= 1.1min\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time=  57.7s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time=  56.0s\n[CV] END final_estimator__alpha=0.001, final_estimator__learning_rate_init=0.01, knn__n_neighbors=5, mlp__alpha=0.1, mlp__learning_rate_init=0.01; total time=  53.9s\n              precision    recall  f1-score   support\n\n        -1.0       0.53      0.16      0.25       147\n         0.0       0.64      0.54      0.58       241\n         1.0       0.47      0.78      0.58       212\n\n    accuracy                           0.53       600\n   macro avg       0.55      0.49      0.47       600\nweighted avg       0.55      0.53      0.50       600\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"JCeQ37Z__2TG"},"execution_count":null,"outputs":[]}]}