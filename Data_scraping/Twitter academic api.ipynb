{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reuse data extraction modules from https://github.com/puyanoii/Twitter_Search\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from ast import literal_eval\n",
    "\n",
    "def unnest_json(dataframe, column):\n",
    "    dataframe_new = json_normalize(dataframe[column].apply(literal_eval))\n",
    "    return dataframe_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tweets saved in the file\n",
      "200 tweets saved in the file\n",
      "300 tweets saved in the file\n",
      "400 tweets saved in the file\n",
      "500 tweets saved in the file\n",
      "600 tweets saved in the file\n",
      "700 tweets saved in the file\n",
      "800 tweets saved in the file\n",
      "900 tweets saved in the file\n",
      "1000 tweets saved in the file\n",
      "1100 tweets saved in the file\n",
      "1200 tweets saved in the file\n",
      "1300 tweets saved in the file\n",
      "1400 tweets saved in the file\n",
      "1500 tweets saved in the file\n",
      "1600 tweets saved in the file\n",
      "1700 tweets saved in the file\n",
      "1800 tweets saved in the file\n",
      "1900 tweets saved in the file\n",
      "2000 tweets saved in the file\n",
      "2100 tweets saved in the file\n",
      "2200 tweets saved in the file\n",
      "2300 tweets saved in the file\n",
      "2400 tweets saved in the file\n",
      "2500 tweets saved in the file\n",
      "2600 tweets saved in the file\n",
      "2700 tweets saved in the file\n",
      "2800 tweets saved in the file\n",
      "2900 tweets saved in the file\n",
      "3000 tweets saved in the file\n",
      "3100 tweets saved in the file\n",
      "3200 tweets saved in the file\n",
      "3300 tweets saved in the file\n",
      "3400 tweets saved in the file\n",
      "3500 tweets saved in the file\n",
      "3600 tweets saved in the file\n",
      "3700 tweets saved in the file\n",
      "3800 tweets saved in the file\n",
      "3900 tweets saved in the file\n",
      "4000 tweets saved in the file\n",
      "4100 tweets saved in the file\n",
      "4200 tweets saved in the file\n",
      "4300 tweets saved in the file\n",
      "4400 tweets saved in the file\n",
      "4500 tweets saved in the file\n",
      "4600 tweets saved in the file\n",
      "4700 tweets saved in the file\n",
      "4800 tweets saved in the file\n",
      "4900 tweets saved in the file\n",
      "5000 tweets saved in the file\n",
      "5100 tweets saved in the file\n",
      "5200 tweets saved in the file\n",
      "5300 tweets saved in the file\n",
      "5400 tweets saved in the file\n",
      "5500 tweets saved in the file\n",
      "5600 tweets saved in the file\n",
      "5700 tweets saved in the file\n",
      "5800 tweets saved in the file\n",
      "5900 tweets saved in the file\n",
      "6000 tweets saved in the file\n",
      "6100 tweets saved in the file\n",
      "6200 tweets saved in the file\n",
      "6300 tweets saved in the file\n",
      "6400 tweets saved in the file\n",
      "6500 tweets saved in the file\n",
      "6600 tweets saved in the file\n",
      "6700 tweets saved in the file\n",
      "6800 tweets saved in the file\n",
      "6900 tweets saved in the file\n",
      "7000 tweets saved in the file\n",
      "7100 tweets saved in the file\n",
      "7200 tweets saved in the file\n",
      "7300 tweets saved in the file\n",
      "7400 tweets saved in the file\n",
      "7500 tweets saved in the file\n",
      "7600 tweets saved in the file\n",
      "7700 tweets saved in the file\n",
      "7800 tweets saved in the file\n",
      "7900 tweets saved in the file\n",
      "8000 tweets saved in the file\n",
      "8100 tweets saved in the file\n",
      "8200 tweets saved in the file\n",
      "8300 tweets saved in the file\n",
      "8400 tweets saved in the file\n",
      "8500 tweets saved in the file\n",
      "8600 tweets saved in the file\n",
      "8700 tweets saved in the file\n",
      "8800 tweets saved in the file\n",
      "8900 tweets saved in the file\n",
      "9000 tweets saved in the file\n",
      "9100 tweets saved in the file\n",
      "9200 tweets saved in the file\n",
      "9300 tweets saved in the file\n",
      "9400 tweets saved in the file\n",
      "9500 tweets saved in the file\n",
      "9600 tweets saved in the file\n",
      "9700 tweets saved in the file\n",
      "9800 tweets saved in the file\n",
      "9900 tweets saved in the file\n",
      "Finished searching for tweets!Total Tweet IDs saved: 9900\n"
     ]
    }
   ],
   "source": [
    "#London step 2 & 3\n",
    "# Accessing the Academic API \n",
    "def connect_to_endpoint(bearer_token, query, next_token=None):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    params = {\n",
    "        'expansions' : \"author_id,referenced_tweets.id,geo.place_id,in_reply_to_user_id,referenced_tweets.id.author_id\",\n",
    "        'tweet.fields' : \"attachments,author_id,context_annotations,created_at,entities,public_metrics\",\n",
    "        'user.fields' : \"created_at,username,verified,description,entities,id,location,name,public_metrics,url\",\n",
    "        'place.fields' : \"contained_within,country,country_code,full_name,geo,id,name,place_type\"}\n",
    "#The endpoint\n",
    "    if (next_token is not None):\n",
    "        #specify the timeline\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?max_results=100&query={}&start_time=2021-03-08T00:00:00Z&end_time=2021-07-18T23:59:59.000Z&next_token={}\".format(query, next_token)\n",
    "    else:\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?max_results=100&start_time=2021-03-08T00:00:00Z&end_time=2021-07-18T23:59:59.000Z&query={}\".format(query)\n",
    "    response = requests.request(\"GET\", url, params=params, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "count = 0\n",
    "flag = True\n",
    "bearer_token = \"my tokens\"\n",
    "data_collect = pd.DataFrame()\n",
    "while flag:\n",
    "    # Replace the count below with the number of Tweets you want to stop at. \n",
    "    # Note: running without the count check will result in getting more Tweets\n",
    "    # that will count towards the Tweet cap\n",
    "    if count >= 2000:\n",
    "        break\n",
    "    json_response = connect_to_endpoint(bearer_token, '(\"covid\" OR \"corona\" OR \"COVID-19\") lang:en -is:retweet -is:reply has:geo place:London') #query\n",
    "    result_count = json_response['meta']['result_count']\n",
    "    while 'next_token' in json_response['meta']:\n",
    "        next_token = json_response['meta']['next_token']\n",
    "        #print(next_token)\n",
    "        if result_count is not None and result_count > 0 and next_token is not None:\n",
    "            df_tweet = pd.DataFrame(json_response['data'])\n",
    "            df_user = pd.DataFrame(json_response['includes']['users'])\n",
    "            df_places = pd.DataFrame(json_response['includes']['places'])\n",
    "            df_full = pd.merge(df_tweet, df_user, how = 'left', left_on = 'author_id', right_on = 'id', suffixes=('_tweet', '_user'))\n",
    "            df_full['geo'] = df_full['geo'].fillna({i: {} for i in df_full.index})\n",
    "            df_full['geo_id'] =  df_full['geo'].apply(lambda x: x.get('place_id'))\n",
    "            df_full = pd.merge(df_full, df_places, how = 'outer', left_on = 'geo_id', right_on = 'id', suffixes=('_meta', '_geo'))\n",
    "            data_collect = data_collect.append(df_full)\n",
    "            count += result_count\n",
    "            print(\"{} tweets saved in the file\".format(count))\n",
    "           #json_response = connect_to_endpoint(bearer_token, '(\"covid\" OR \"corona\") lang:en -is:retweet -is:reply has:geo place_country:GB', next_token)\n",
    "            json_response = connect_to_endpoint(bearer_token, '(\"covid\" OR \"corona\" OR \"COVID-19\") lang:en -is:retweet -is:reply has:geo place:London', next_token)#change city name here, and coutry name is GB\n",
    "    else:\n",
    "        flag = False\n",
    "print(\"Finished searching for tweets!Total Tweet IDs saved: {}\".format(count))\n",
    "data_collect.to_csv(\"London(step 2 & 3)/London-step2&3.csv\", encoding=\"utf-8\",index=False, escapechar=\"\\r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
