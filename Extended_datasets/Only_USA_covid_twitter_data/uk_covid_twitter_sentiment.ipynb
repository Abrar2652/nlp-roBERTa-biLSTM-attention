{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtbOK3YG_2Sb"
      },
      "source": [
        "# Md Abrar Jahin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzLqz4BN_2Sp"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x53emXkn__8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5ba687-0e48-4341-f43b-a683aba4db26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDTjjIGY_2Sq"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "import pandas as pd\n",
        "import emoji\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpMOZQjB_2Sq"
      },
      "outputs": [],
      "source": [
        "df_one = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Step_one.csv')\n",
        "df_two = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Step_two.csv')\n",
        "df_three = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Step_three.csv')\n",
        "\n",
        "print(df_one.info()) #29934\n",
        "# print(df_two.info()) #24702\n",
        "# print(df_three.info()) #22742"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeRdElUz_2Sq"
      },
      "outputs": [],
      "source": [
        "#merge all the datasets\n",
        "all_df=[df_one, df_two, df_three]\n",
        "all_df=pd.concat(all_df)\n",
        "all_df = all_df.reset_index(drop=True)\n",
        "print(len(all_df))\n",
        "empty_tweet=all_df['tweet'].isna().value_counts()\n",
        "print(empty_tweet) #no empty tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL2Y6MRt_2Sq"
      },
      "outputs": [],
      "source": [
        "all_df['clean_tweet']= all_df['tweet'].copy()\n",
        "all_df=all_df.drop('id', axis=1)\n",
        "all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyLl62fK_2Sr"
      },
      "outputs": [],
      "source": [
        "#handle emoji\n",
        "def convert_emoji(text):\n",
        "    text=[emoji.demojize(tw) for tw in text]  #run slowly\n",
        "    new_df= pd.DataFrame(text, columns=['tweet'])\n",
        "    return new_df\n",
        "new_df=convert_emoji(all_df['clean_tweet'])\n",
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9_J8Vqm_2Sr"
      },
      "outputs": [],
      "source": [
        "all_df[['clean_tweet']]=new_df[['tweet']].copy()  \n",
        "all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjYetv_3_2Sr"
      },
      "outputs": [],
      "source": [
        "#simple data cleaning\n",
        "#reference:https://github.com/ugis22/analysing_twitter/blob/master/Jupyter%20Notebook%20files/Analysis%20of%20Twitter.ipynb\n",
        "def preprocessing_one(df):\n",
        "    #lower all characters\n",
        "    df['clean_tweet'] = df['clean_tweet'].str.lower()\n",
        "    #remove all the mentions: @username\n",
        "    df['clean_tweet'] = df['clean_tweet'].replace(r'@\\w+', '', regex=True)\n",
        "    #remove all the links in the original tweets (start with \"www\" and \"http\" and \"https\")\n",
        "    df['clean_tweet'] = df['clean_tweet'].replace(r'http\\S+|rhttps\\S+|rwww\\S+', '', regex=True)   \n",
        "    return df\n",
        "#Notice: Remove punctuation and special characters after handling contraction words\n",
        "preprocessing_one(all_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJl1vXEm_2Ss",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC-Efhe-_2St"
      },
      "outputs": [],
      "source": [
        "#Handling repeated characters\n",
        "#reference: https://github.com/ugis22/analysing_twitter/blob/master/Jupyter%20Notebook%20files/Analysis%20of%20Twitter.ipynb\n",
        "#https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python\n",
        "from nltk.corpus import wordnet\n",
        "#re.sub(pattern, repl, string, count): pattern(Eligible pattern)，repl(replace to...), string\n",
        "def repeated_char(word):\n",
        "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "    repl_word = r'\\1\\2\\3'\n",
        "    if wordnet.synsets(word):\n",
        "        return word  #test for words existence\n",
        "    #repl_new= repeat_pattern.sub(repl_word, word)\n",
        "    repl_new = re.sub(repeat_pattern, repl_word, word)\n",
        "    if repl_new != word:\n",
        "        return repeated_char(repl_new)\n",
        "    else:\n",
        "        return repl_new\n",
        "word1='loooove'\n",
        "print(repeated_char(word1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQbaaUWz_2St"
      },
      "outputs": [],
      "source": [
        "def check_repeated (tweet):\n",
        "    repeat_pattern = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
        "    word_set = [''.join(i) for i in re.findall(repeat_pattern, tweet)]  #find all the words with repeated characters\n",
        "    for word in word_set:\n",
        "        if not wordnet.synsets(word):\n",
        "            tweet=re.sub(word, repeated_char(word), tweet)\n",
        "    return tweet\n",
        "#test the function\n",
        "tweet1='I looove you, soooo much'\n",
        "print(check_repeated (tweet1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "py3uxbTE_2Su"
      },
      "outputs": [],
      "source": [
        "#replace words contraction\n",
        "#reference:https://github.com/kiran-bal/Disaster_tweets_classifier/blob/2e6d648f5ef9cbe67024ad5cf032582fc4dc3a75/version2/notebooks/Disaster_tweet_classifier.ipynb\n",
        "#re.sub(pattern, repl, string, count=0, flags=0): count=0: all matched will be replaced\n",
        "contraction_dict=[(r'I\\'m', 'I am'),(r'i\\'m', 'i am'),(r'ain\\'t', 'am not'),(r'(\\w+)\\'s', '\\g<1> is'),(r'(\\w+)\\'re', '\\g<1> are'),(r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                  (r'can\\'t', 'cannot'),(r'won\\'t', 'will not'), (r'wont', 'will not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)\\'d', '\\g<1> would'), (r'(\\w+)\\'ve', '\\g<1> have'),\n",
        "                 (r'I\\’m', 'I am'),(r'i\\’m', 'i am'),(r'ain\\’t', 'am not'),(r'(\\w+)\\’s', '\\g<1> is'),(r'(\\w+)\\’re', '\\g<1> are'),(r'(\\w+)n\\’t', '\\g<1> not'),\n",
        "                  (r'can\\’t', 'cannot'),(r'won\\’t', 'will not'), (r'(\\w+)\\’ll', '\\g<1> will'), (r'(\\w+)\\’d', '\\g<1> would'), (r'(\\w+)\\’ve', '\\g<1> have')]\n",
        "#Notice: The quotation of some texts are not in English\n",
        "def handle_contraction(text):\n",
        "    patterns_set=[(re.compile(pattern), repl) for (pattern, repl) in contraction_dict]\n",
        "    for (pattern, repl) in patterns_set:\n",
        "        text=re.sub(pattern, repl, text)\n",
        "    return text\n",
        "#test\n",
        "print(all_df['clean_tweet'][4])\n",
        "print(handle_contraction(all_df['clean_tweet'][4]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoCY6kw6_2Su"
      },
      "outputs": [],
      "source": [
        "def preprocessing_two(df):\n",
        "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: check_repeated(x)) #remove repeated charaters\n",
        "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: handle_contraction(x)) #handle constraction\n",
        "    return df\n",
        "all_df=preprocessing_two(all_df)\n",
        "all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGcNtcSW_2Su"
      },
      "outputs": [],
      "source": [
        "#only English character\n",
        "def replace_non_alphabetical(df):\n",
        "    df['clean_tweet']=df['clean_tweet'].replace('[^a-zA-Z]',' ', regex=True)\n",
        "    df['clean_tweet'] = df['clean_tweet'].replace('\\s+', ' ', regex=True)\n",
        "    return df\n",
        "all_df=replace_non_alphabetical(all_df)\n",
        "all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8j4LLnJ_2Su"
      },
      "outputs": [],
      "source": [
        "#remove less than two-character words, but keep \"no\" if len(w)>2 or w==\"no\"\n",
        "def short_words(df):\n",
        "    df['clean_tweet'] = df['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2 or w==\"no\"]))\n",
        "    return df\n",
        "all_df=short_words(all_df)\n",
        "all_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vG0bIpE_2Su"
      },
      "outputs": [],
      "source": [
        "#remove stopwords\n",
        "#can't remove words like \"not\" or \"no\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzthGGTV_2Sv"
      },
      "outputs": [],
      "source": [
        "#create own stopwords\n",
        "my_stopwords = ['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', \"c'mon\", \"c's\", 'came', 'can', \"can't\", 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', \"don't\", 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', \"hadn't\", 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he's\", 'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'normally', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', \"shouldn't\", 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', \"t's\", 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that's\", 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \"there's\", 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', \"wasn't\", 'way', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'welcome', 'well', 'went', 'were', \"weren't\", 'what', \"what's\", 'whatever', 'when', 'whence', 'whenever', 'where', \"where's\", 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', \"who's\", 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', \"won't\", 'wonder', 'would', 'would', \"wouldn't\", 'x', 'y', 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', '']\n",
        "print(my_stopwords)\n",
        "\n",
        "def remove_stopwords(df):\n",
        "    df['clean_tweet']=df['clean_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in my_stopwords]))\n",
        "    return df\n",
        "all_df=remove_stopwords(all_df)\n",
        "all_df\n",
        "# all_df.to_csv('clean_datasets/all_clean_1_test.csv',index = False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gChkYZo_2Sv"
      },
      "outputs": [],
      "source": [
        "#remove empty tweets\n",
        "def remove_empty(df):\n",
        "    df=df[df['clean_tweet']!='']\n",
        "    return df\n",
        "all_df=remove_empty(all_df)\n",
        "all_df=all_df.reset_index(drop=True)\n",
        "all_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYp081ig_2Sv"
      },
      "outputs": [],
      "source": [
        "#split the clean datasets into three steps\n",
        "all_df.to_csv('all_clean_1.csv',index = False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7qSiD-C_2Sx"
      },
      "outputs": [],
      "source": [
        "#clean_df = pd.read_csv('/kaggle/input/uk-twitter-covid19-dataset/all_clean_1.csv')\n",
        "clean_df=all_df\n",
        "clean_df['created_at'] = clean_df['created_at'].str.replace(r'[^\\x00-\\x7F]+', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5_YjCR7_2Sx"
      },
      "outputs": [],
      "source": [
        "clean_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlry-lZe_2Sy"
      },
      "outputs": [],
      "source": [
        "clean_df['created_at'] = pd.to_datetime(clean_df['created_at'] , utc=True).dt.date\n",
        "#clean_df.to_csv('clean_datasets/all_clean_1.csv',index = False, encoding='utf_8_sig')\n",
        "clean_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jTKCzSH_2Sy"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "clean_step_one = clean_df[(clean_df['created_at']) < datetime.date(2021,3,8)].reset_index(drop=True)\n",
        "print(\"step_one:\",len(clean_step_one))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghraqMpp_2Sy"
      },
      "outputs": [],
      "source": [
        "clean_step_two = clean_df[((clean_df['created_at']) >= datetime.date(2021,3,8)) & ((clean_df['created_at']) < datetime.date(2021,5,17))].reset_index(drop=True)\n",
        "print(\"step_two:\",len(clean_step_two))\n",
        "# clean_step_two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5phK5yah_2Sy"
      },
      "outputs": [],
      "source": [
        "clean_step_three= clean_df[((clean_df['created_at']) >= datetime.date(2021,5,17)) & ((clean_df['created_at']) <= datetime.date(2021,7,18))].reset_index(drop=True)\n",
        "print(\"step_three:\",len(clean_step_three))\n",
        "# clean_step_three"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylx_-Fv6_2Sy"
      },
      "outputs": [],
      "source": [
        "#save as csv\n",
        "clean_step_one.to_csv('clean_step1.csv',index = False, encoding='utf_8_sig')\n",
        "clean_step_two.to_csv('clean_step2.csv',index = False, encoding='utf_8_sig')\n",
        "clean_step_three.to_csv('clean_step3.csv',index = False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHySibym_2Sz"
      },
      "outputs": [],
      "source": [
        "len(clean_step_one)+len(clean_step_two)+len(clean_step_three)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwwMP1Pc_2Sz"
      },
      "outputs": [],
      "source": [
        "clean_all=[clean_step_one,clean_step_two,clean_step_three]\n",
        "clean_all=pd.concat(clean_all)\n",
        "clean_all_dfs=clean_all.reset_index(drop=True)\n",
        "# new_dfs.to_csv(\"unsupervised_datasets/lexicon_all.csv\")\n",
        "clean_all.to_csv('clean_all.csv',index = False, encoding='utf_8_sig') #final clean datasets\n",
        "clean_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdAjZ2Rw_2Sz"
      },
      "outputs": [],
      "source": [
        "#The number of tweets collected in each city\n",
        "#London\n",
        "London_df = clean_all[clean_all['near'].str.contains('London|london')]\n",
        "London_df = London_df.reset_index(drop=True)\n",
        "print(\"Total tweets in London:\",len(London_df))\n",
        "#Birmingham\n",
        "Birmingham_df = clean_all[clean_all['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_df = Birmingham_df.reset_index(drop=True)\n",
        "print(\"Total tweets in Birmingham:\",len(Birmingham_df))\n",
        "#Bristol\n",
        "Bristol_df = clean_all[clean_all['near'].str.contains('Bristol|Bristol')]\n",
        "Bristol_df = Bristol_df.reset_index(drop=True)\n",
        "print(\"Total tweets in Bristol:\",len(Bristol_df))\n",
        "#Leeds\n",
        "Leeds_df = clean_all[clean_all['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_df = Leeds_df.reset_index(drop=True)\n",
        "print(\"Total tweets in Leeds:\",len(Leeds_df))\n",
        "#Liverpool\n",
        "Liverpool_df = clean_all[clean_all['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_df = Liverpool_df.reset_index(drop=True)\n",
        "print(\"Total tweets in Liverpool:\",len(Liverpool_df))\n",
        "#Manchester\n",
        "Manchester_df = clean_all[clean_all['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_df = Manchester_df.reset_index(drop=True)\n",
        "print(\"Total tweets in Manchester:\",len(Manchester_df))\n",
        "#Newcastle\n",
        "Newcastle_df = clean_all[clean_all['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_df = Newcastle_df.reset_index(drop=True)\n",
        "print(\"Total tweets in Newcastle:\",len(Newcastle_df))\n",
        "#Nottingham\n",
        "Nottingham_df = clean_all[clean_all['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_df = Nottingham_df.reset_index(drop=True)\n",
        "print(\"Total tweets in Nottingham:\",len(Nottingham_df))\n",
        "#Sheffield\n",
        "Sheffield_df = clean_all[clean_all['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_df = Sheffield_df.reset_index(drop=True)\n",
        "print(\"Total tweets in Sheffield:\",len(Sheffield_df))\n",
        "#Southampton\n",
        "Southampton_df = clean_all[clean_all['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_df = Southampton_df.reset_index(drop=True)\n",
        "print(\"Total tweets in Southampton:\",len(Southampton_df))\n",
        "print(len(London_df)+len(Birmingham_df)+len(Bristol_df)+len(Leeds_df)+len(Liverpool_df)+len(Manchester_df)+len(Newcastle_df)+len(Nottingham_df)+len(Sheffield_df)+len(Southampton_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zM1KqnR_2Sz"
      },
      "outputs": [],
      "source": [
        "cities = {\"city\": [\"London\", \"Manchester\", \"Birmingham\", \"Liverpool\", \"Leeds\", \"Bristol\", \"Sheffield\", \"Nottingham\",\"Southampton\", \"Newcastle\"],\n",
        "           \"total number\":[len(London_df),  len(Manchester_df), len(Birmingham_df), len(Liverpool_df), len(Leeds_df), len(Bristol_df), len(Sheffield_df),len(Nottingham_df),len(Southampton_df), len(Newcastle_df)]}\n",
        "city_frame  = pd.DataFrame(data = cities)\n",
        "city_frame.to_csv('all_cities.csv',index = False)\n",
        "city_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ9RseGp_2Sz"
      },
      "outputs": [],
      "source": [
        "np.arange(len(city_frame['total number'])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oil3i7A_2Sz"
      },
      "outputs": [],
      "source": [
        "#reference: https://stackoverflow.com/questions/60212294/how-to-add-labels-to-a-horizontal-bar-chart-in-matplotlib\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "fig, ax = plt.subplots()   \n",
        "\n",
        "width = 0.75 # bar's width \n",
        "x_loc = np.arange(len(city_frame['total number']))  # the x locations\n",
        "ax.barh(x_loc, city_frame['total number'], width, color=\"slateblue\")\n",
        "ax.set_yticks(x_loc+width/2)\n",
        "ax.set_yticklabels(city_frame['city'], minor=False)\n",
        "\n",
        "_, xmax = plt.xlim()\n",
        "plt.xlim(0, xmax+300)\n",
        "for i, v in enumerate(city_frame['total number']):\n",
        "    ax.text(v + 100, i, str(v), color='black', fontsize=10, ha='left', va='center')\n",
        "    \n",
        "plt.title('The distribution of tweets number')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('City')\n",
        "\n",
        "# plt.show()\n",
        "plt.savefig(\"tweets distribution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Px6PiXz_2S0"
      },
      "outputs": [],
      "source": [
        "#The number of tweets collected in each city\n",
        "#step 1\n",
        "#London\n",
        "London_df = clean_step_one[clean_step_one['near'].str.contains('London|london')]\n",
        "London_df = London_df.reset_index(drop=True)\n",
        "print(\"Step1 tweets in London:\",len(London_df))\n",
        "#Birmingham\n",
        "Birmingham_df = clean_step_one[clean_step_one['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_df = Birmingham_df.reset_index(drop=True)\n",
        "print(\"Step1 tweets in Birmingham:\",len(Birmingham_df))\n",
        "#Bristol\n",
        "Bristol_df = clean_step_one[clean_step_one['near'].str.contains('Bristol|Bristol')]\n",
        "Bristol_df = Bristol_df.reset_index(drop=True)\n",
        "print(\"Step1 tweets in Bristol:\",len(Bristol_df))\n",
        "#Leeds\n",
        "Leeds_df = clean_step_one[clean_step_one['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_df = Leeds_df.reset_index(drop=True)\n",
        "print(\"Step1 tweets in Leeds:\",len(Leeds_df))\n",
        "#Liverpool\n",
        "Liverpool_df = clean_step_one[clean_step_one['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_df = Liverpool_df.reset_index(drop=True)\n",
        "print(\"Step1 tweets in Liverpool:\",len(Liverpool_df))\n",
        "#Manchester\n",
        "Manchester_df = clean_step_one[clean_step_one['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_df = Manchester_df.reset_index(drop=True)\n",
        "print(\"Step1 tweets in Manchester:\",len(Manchester_df))\n",
        "#Newcastle\n",
        "Newcastle_df = clean_step_one[clean_step_one['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_df = Newcastle_df.reset_index(drop=True)\n",
        "print(\"Step1 tweets in Newcastle:\",len(Newcastle_df))\n",
        "#Nottingham\n",
        "Nottingham_df = clean_step_one[clean_step_one['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_df = Nottingham_df.reset_index(drop=True)\n",
        "print(\"Step1 tweets in Nottingham:\",len(Nottingham_df))\n",
        "#Sheffield\n",
        "Sheffield_df = clean_step_one[clean_step_one['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_df = Sheffield_df.reset_index(drop=True)\n",
        "print(\"Step1 tweets in Sheffield:\",len(Sheffield_df))\n",
        "#Southampton\n",
        "Southampton_df = clean_step_one[clean_step_one['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_df = Southampton_df.reset_index(drop=True)\n",
        "print(\"Step1 tweets in Southampton:\",len(Southampton_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RGLP5EI_2S0"
      },
      "outputs": [],
      "source": [
        "#step 2\n",
        "#The number of tweets collected in each city\n",
        "#London\n",
        "London_df = clean_step_two[clean_step_two['near'].str.contains('London|london')]\n",
        "London_df = London_df.reset_index(drop=True)\n",
        "print(\"Step2 tweets in London:\",len(London_df))\n",
        "#Birmingham\n",
        "Birmingham_df = clean_step_two[clean_step_two['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_df = Birmingham_df.reset_index(drop=True)\n",
        "print(\"Step2 tweets in Birmingham:\",len(Birmingham_df))\n",
        "#Bristol\n",
        "Bristol_df = clean_step_two[clean_step_two['near'].str.contains('Bristol|Bristol')]\n",
        "Bristol_df = Bristol_df.reset_index(drop=True)\n",
        "print(\"Step2 tweets in Bristol:\",len(Bristol_df))\n",
        "#Leeds\n",
        "Leeds_df = clean_step_two[clean_step_two['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_df = Leeds_df.reset_index(drop=True)\n",
        "print(\"Step2 tweets in Leeds:\",len(Leeds_df))\n",
        "#Liverpool\n",
        "Liverpool_df = clean_step_two[clean_step_two['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_df = Liverpool_df.reset_index(drop=True)\n",
        "print(\"Step2 tweets in Liverpool:\",len(Liverpool_df))\n",
        "#Manchester\n",
        "Manchester_df = clean_step_two[clean_step_two['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_df = Manchester_df.reset_index(drop=True)\n",
        "print(\"Step2 tweets in Manchester:\",len(Manchester_df))\n",
        "#Newcastle\n",
        "Newcastle_df = clean_step_two[clean_step_two['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_df = Newcastle_df.reset_index(drop=True)\n",
        "print(\"Step2 tweets in Newcastle:\",len(Newcastle_df))\n",
        "#Nottingham\n",
        "Nottingham_df = clean_step_two[clean_step_two['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_df = Nottingham_df.reset_index(drop=True)\n",
        "print(\"Step2 tweets in Nottingham:\",len(Nottingham_df))\n",
        "#Sheffield\n",
        "Sheffield_df = clean_step_two[clean_step_two['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_df = Sheffield_df.reset_index(drop=True)\n",
        "print(\"Step2 tweets in Sheffield:\",len(Sheffield_df))\n",
        "#Southampton\n",
        "Southampton_df = clean_step_two[clean_step_two['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_df = Southampton_df.reset_index(drop=True)\n",
        "print(\"Step2 tweets in Southampton:\",len(Southampton_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D2vOa2P_2S0"
      },
      "outputs": [],
      "source": [
        "#step 3\n",
        "#The number of tweets collected in each city\n",
        "#London\n",
        "London_df = clean_step_three[clean_step_three['near'].str.contains('London|london')]\n",
        "London_df = London_df.reset_index(drop=True)\n",
        "print(\"Step3 tweets in London:\",len(London_df))\n",
        "#Birmingham\n",
        "Birmingham_df = clean_step_three[clean_step_three['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_df = Birmingham_df.reset_index(drop=True)\n",
        "print(\"Step3 tweets in Birmingham:\",len(Birmingham_df))\n",
        "#Bristol\n",
        "Bristol_df = clean_step_three[clean_step_three['near'].str.contains('Bristol|Bristol')]\n",
        "Bristol_df = Bristol_df.reset_index(drop=True)\n",
        "print(\"Step3 tweets in Bristol:\",len(Bristol_df))\n",
        "#Leeds\n",
        "Leeds_df = clean_step_three[clean_step_three['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_df = Leeds_df.reset_index(drop=True)\n",
        "print(\"Step3 tweets in Leeds:\",len(Leeds_df))\n",
        "#Liverpool\n",
        "Liverpool_df = clean_step_three[clean_step_three['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_df = Liverpool_df.reset_index(drop=True)\n",
        "print(\"Step3 tweets in Liverpool:\",len(Liverpool_df))\n",
        "#Manchester\n",
        "Manchester_df = clean_step_three[clean_step_three['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_df = Manchester_df.reset_index(drop=True)\n",
        "print(\"Step3 tweets in Manchester:\",len(Manchester_df))\n",
        "#Newcastle\n",
        "Newcastle_df = clean_step_three[clean_step_three['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_df = Newcastle_df.reset_index(drop=True)\n",
        "print(\"Step3 tweets in Newcastle:\",len(Newcastle_df))\n",
        "#Nottingham\n",
        "Nottingham_df = clean_step_three[clean_step_three['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_df = Nottingham_df.reset_index(drop=True)\n",
        "print(\"Step3 tweets in Nottingham:\",len(Nottingham_df))\n",
        "#Sheffield\n",
        "Sheffield_df = clean_step_three[clean_step_three['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_df = Sheffield_df.reset_index(drop=True)\n",
        "print(\"Step3 tweets in Sheffield:\",len(Sheffield_df))\n",
        "#Southampton\n",
        "Southampton_df = clean_step_three[clean_step_three['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_df = Southampton_df.reset_index(drop=True)\n",
        "print(\"Step3 tweets in Southampton:\",len(Southampton_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVV1PSGJ_2S0"
      },
      "outputs": [],
      "source": [
        "#stack bar chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "    \n",
        "Steps_df = pd.DataFrame([['Stage1', 12870, 3010, 1572, 1857, 2372, 3796, 239, 1315, 1604, 1287], \n",
        "                   ['Stage2', 12945, 1905, 1003, 1214, 1652, 2838, 249, 942, 1185, 755], \n",
        "                   ['Stage3', 11863, 1793, 1187, 1170, 1506, 2559, 364, 735, 931, 612]],\n",
        "                  columns=['Stages', 'London','Birmingham','Bristol','Leeds','Liverpool','Manchester','Newcastle','Nottingham','Sheffield','Southampton'])\n",
        "Steps_df.to_csv('three_steps_cities.csv',index = False, encoding='utf_8_sig')\n",
        "Steps_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdT4Oepb_2S1"
      },
      "outputs": [],
      "source": [
        "# plt.xticks(rotation=0, ha='center')\n",
        "Steps_df.plot(x='Stages', kind='bar', stacked=True,\n",
        "        title='Stacked Bar Graph by Different Steps')\n",
        "plt.xticks(rotation=0, ha='center')\n",
        "plt.savefig(\"stacked bar graph\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vULv4txH_2S1"
      },
      "source": [
        "## Further Cleaning and lexicon-based method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIIs5MTA_2S1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwYXoS4M_2S1"
      },
      "outputs": [],
      "source": [
        "df=clean_all\n",
        "#df=pd.read_csv('/kaggle/input/uk-twitter-covid19-dataset/clean_all.csv')\n",
        "df.head()\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-L--QRg_2S1"
      },
      "outputs": [],
      "source": [
        "#tokenization\n",
        "def tweet_token(text):\n",
        "    words_set=text.str.split()\n",
        "    tokens=[word for word in words_set]\n",
        "    return tokens\n",
        "\n",
        "df['tokens']=tweet_token(df['clean_tweet'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ali85Ffx_2S1"
      },
      "outputs": [],
      "source": [
        "#lemmatize and stemming\n",
        "import nltk\n",
        "word_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "word_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return [word_lemmatizer.lemmatize(word) for word in word_tokenizer.tokenize(text)]\n",
        "df['lemma']=df['clean_tweet'].apply(lemmatize_text)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zQ1ECyx_2S2"
      },
      "outputs": [],
      "source": [
        "#lemma_sentence\n",
        "df['lemma_sentence'] = df['lemma'].apply(lambda x: ' '.join(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkOya3di_2S2"
      },
      "outputs": [],
      "source": [
        "#POS for clean tweets\n",
        "#reference:\n",
        "#https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "#https://stackoverflow.com/questions/51267166/lemmatization-pandas-python\n",
        "nltk.download('punkt')\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stem_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def convert_wordnet_tag(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def pos_tag_set(tweet):\n",
        "    tagged_words = nltk.pos_tag(nltk.word_tokenize(tweet))\n",
        "    new_tag=[]\n",
        "    for word, tag in tagged_words:\n",
        "        new_tag.append(tuple([word, convert_wordnet_tag(tag)]))\n",
        "    return new_tag\n",
        "df['pos_tag']=df['clean_tweet'].apply(pos_tag_set)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRBIEY0N_2S2"
      },
      "outputs": [],
      "source": [
        "#create lemma sentence with pos-tags\n",
        "def handle_lemma(pos_tweet):\n",
        "    lemma_set = \" \"\n",
        "    for word, pos in pos_tweet:\n",
        "        if not pos: \n",
        "            lemma = word\n",
        "            lemma_set = lemma_set + \" \" + lemma\n",
        "        else:  \n",
        "            lemma = stem_lemmatizer.lemmatize(word, pos=pos)\n",
        "            lemma_set = lemma_set + \" \" + lemma\n",
        "    return lemma_set\n",
        "df['pos_tag'].apply(handle_lemma)\n",
        "    \n",
        "df['lemma_sentence(with POS)'] = df['pos_tag'].apply(handle_lemma)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DJBxVDr_2S3"
      },
      "outputs": [],
      "source": [
        "#save as csv\n",
        "df.to_csv('further_clean_all.csv',index = False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQl7IOU_2S3"
      },
      "outputs": [],
      "source": [
        "further_clean_all = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWZXywfx_2S3"
      },
      "outputs": [],
      "source": [
        "#split into 3 steps\n",
        "import datetime\n",
        "df['created_at'] = pd.to_datetime(df['created_at'] , utc=True).dt.date\n",
        "further_step_one = df[(df['created_at']) < datetime.date(2021,3,8)].reset_index(drop=True)\n",
        "print(\"step_one:\",len(further_step_one))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUuxldEL_2S3"
      },
      "outputs": [],
      "source": [
        "further_step_two = df[((df['created_at']) >= datetime.date(2021,3,8)) & ((df['created_at']) < datetime.date(2021,5,17))].reset_index(drop=True)\n",
        "print(\"step_two:\",len(further_step_two))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FwGAKal_2S3"
      },
      "outputs": [],
      "source": [
        "further_step_three= df[((df['created_at']) >= datetime.date(2021,5,17)) & ((df['created_at']) <= datetime.date(2021,7,18))].reset_index(drop=True)\n",
        "print(\"step_three:\",len(further_step_three))\n",
        "len(further_step_one)+len(further_step_two)+len(further_step_three)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6797wbS_2S4"
      },
      "outputs": [],
      "source": [
        "further_step_one.to_csv('further_clean_step1.csv',index = False, encoding='utf_8_sig')\n",
        "further_step_two.to_csv('further_clean_step2.csv',index = False, encoding='utf_8_sig')\n",
        "further_step_three.to_csv('further_clean_step3.csv',index = False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jPiym-M_2S4"
      },
      "outputs": [],
      "source": [
        "#sentiwordnet analysis\n",
        "#reference: https://github.com/harika-bonthu/Lexicon-based-SentimentAnalysis/blob/main/lexicon_based_sentiment_analysis.ipynb\n",
        "nltk.download('sentiwordnet')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "#和之前的方法结果 有差距\n",
        "\n",
        "def sentiwordnetanalysis(pos_data):\n",
        "    sentiment = 0\n",
        "    tokens_count = 0\n",
        "    for word, pos in pos_data:\n",
        "        if not pos:\n",
        "            continue\n",
        "        lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
        "        if not lemma:\n",
        "            continue\n",
        "        \n",
        "        synsets = wordnet.synsets(lemma, pos=pos)\n",
        "        if not synsets:\n",
        "            continue\n",
        "\n",
        "        synset = synsets[0] #only take the most common meaning\n",
        "        swn_synset = swn.senti_synset(synset.name())\n",
        "        sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
        "        tokens_count += 1\n",
        "    if not tokens_count:\n",
        "        return 0\n",
        "    else:\n",
        "        return sentiment\n",
        "\n",
        "df['sentiword_analysis']=df['pos_tag'].apply(sentiwordnetanalysis)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_jXN3AE_2S4"
      },
      "outputs": [],
      "source": [
        "#VADER--low speed--use further clean datasets\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "def vaderSentiment_method(df):\n",
        "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "    snt_score = sentiment_analyzer.polarity_scores(df['lemma_sentence(with POS)'])\n",
        "    return snt_score['compound'] \n",
        "\n",
        "df['vader_score'] = df.apply(vaderSentiment_method, axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BHQC6vW_2S4"
      },
      "outputs": [],
      "source": [
        "#Textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "def Polarity_score(tweet):\n",
        "    return TextBlob(tweet).sentiment.polarity\n",
        "\n",
        "def Subjectivity_score(tweet):\n",
        "    return TextBlob(tweet).sentiment.subjectivity\n",
        "\n",
        "df['textblob_polarity'] = df['lemma_sentence(with POS)'].apply(Polarity_score) \n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSPgLobz_2S5"
      },
      "outputs": [],
      "source": [
        "##save as csv\n",
        "df.to_csv('unsupervised_lexicon_all.csv',index = False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QMESaru_2S5"
      },
      "outputs": [],
      "source": [
        "#split into 3 steps\n",
        "import datetime\n",
        "df['created_at'] = pd.to_datetime(df['created_at'] , utc=True).dt.date\n",
        "lexicon_step_one = df[(df['created_at']) < datetime.date(2021,3,8)].reset_index(drop=True)\n",
        "print(\"step_one:\",len(lexicon_step_one))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_11pQSFp_2S5"
      },
      "outputs": [],
      "source": [
        "lexicon_step_two = df[((df['created_at']) >= datetime.date(2021,3,8)) & ((df['created_at']) < datetime.date(2021,5,17))].reset_index(drop=True)\n",
        "print(\"step_two:\",len(lexicon_step_two))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA8vh3cD_2S5"
      },
      "outputs": [],
      "source": [
        "lexicon_step_three= df[((df['created_at']) >= datetime.date(2021,5,17)) & ((df['created_at']) <= datetime.date(2021,7,18))].reset_index(drop=True)\n",
        "print(\"step_three:\",len(lexicon_step_three))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOtIseHj_2S5"
      },
      "outputs": [],
      "source": [
        "#merge three datasets\n",
        "dfs=[lexicon_step_one,lexicon_step_two,lexicon_step_three]\n",
        "dfs=pd.concat(dfs)\n",
        "dfs=dfs.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9dNQk3c_2S5"
      },
      "outputs": [],
      "source": [
        "lexicon_step_one.to_csv('unsupervised_lexicon_step1.csv',index = False, encoding='utf_8_sig')\n",
        "lexicon_step_two.to_csv('unsupervised_lexicon_step2.csv',index = False, encoding='utf_8_sig')\n",
        "lexicon_step_three.to_csv('unsupervised_lexicon_step3.csv',index = False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXtNB06E_2S6"
      },
      "source": [
        "## Initial data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP6qAsFy_2S6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bugHgfJ_2S6"
      },
      "outputs": [],
      "source": [
        "#df=pd.read_csv('/content/further_clean_all.csv')\n",
        "df = further_clean_all\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COyQK9LA_2S6"
      },
      "outputs": [],
      "source": [
        "#word cloud map\n",
        "#https://www.datacamp.com/community/tutorials/wordcloud-python\n",
        "def word_cloud(words):\n",
        "    wordcloud = WordCloud(width=800, height=600, random_state=21, relative_scaling=0.5, background_color=\"white\").generate(words)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "word_set = ' '.join([tweet_word for tweet_word in df['lemma_sentence(with POS)']])\n",
        "word_cloud(word_set)\n",
        "plt.savefig('word_cloud.jpg')\n",
        "#vaccine is a hot topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2BUMbbq_2S6"
      },
      "outputs": [],
      "source": [
        "df['lemma'].value_counts()[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hMCF6ND_2S6"
      },
      "outputs": [],
      "source": [
        "#Word frequency\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from collections import Counter\n",
        "texts = df['lemma_sentence(with POS)']\n",
        "word_counts = Counter(word_tokenize('\\n'.join(texts)))\n",
        "word_top=word_counts.most_common(n=20)\n",
        "print(word_top)\n",
        "# count_sum="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_megzNQ_2S6"
      },
      "outputs": [],
      "source": [
        "#(5)\n",
        "count_all = df['lemma_sentence(with POS)'].str.len().sum()\n",
        "print(count_all)\n",
        "\n",
        "words=[count[0] for count in word_top]\n",
        "frac_value=[int(count[1])/count_all for count in  word_top]\n",
        "words=words[: :-1]\n",
        "frac_value=sorted(frac_value)\n",
        "\n",
        "#plot line chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(words,frac_value)\n",
        "plt.xticks(rotation=40, fontsize=13)\n",
        "plt.xlabel('Words',fontsize=15)\n",
        "plt.ylabel('Word Fraction',fontsize=15)\n",
        "plt.title('Word Frequency Chart',fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDc1QnJf_2S7"
      },
      "source": [
        "## Data analysis for lexicon-based methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGnbmCg0_2S7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk4EBVaI_2S7"
      },
      "outputs": [],
      "source": [
        "#df1=pd.read_csv('/kaggle/input/uk-twitter-covid19-dataset/unsupervised_lexicon_step1.csv')\n",
        "#df2=pd.read_csv('/kaggle/input/uk-twitter-covid19-dataset/unsupervised_lexicon_step2.csv')\n",
        "#df3=pd.read_csv('/kaggle/input/uk-twitter-covid19-dataset/unsupervised_lexicon_step3.csv')\n",
        "#dfs=pd.read_csv('/kaggle/input/uk-twitter-covid19-dataset/unsupervised_lexicon_all.csv')\n",
        "df1=lexicon_step_one\n",
        "df2=lexicon_step_two\n",
        "df3=lexicon_step_three\n",
        "dfs=df\n",
        "print(len(dfs))\n",
        "print(len(df1)+len(df2)+len(df3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNfy-Twb_2S7"
      },
      "outputs": [],
      "source": [
        "#sentiment for step1\n",
        "#for textblob and sentiword\n",
        "def senti_label(score):\n",
        "    if score < 0:\n",
        "        return -1\n",
        "    elif score == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz1XSVBm_2S7"
      },
      "outputs": [],
      "source": [
        "#for VADER\n",
        "def senti_label_2(score):\n",
        "    if score <= -0.05:\n",
        "        return -1\n",
        "    elif score >= 0.05:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9AnrcP4_2S7"
      },
      "outputs": [],
      "source": [
        "#Step1\n",
        "#using senti_label\n",
        "df1['senti_textblob'] = df1['textblob_polarity'].apply(senti_label)\n",
        "df1['senti_wordnet'] = df1['sentiword_analysis'].apply(senti_label)\n",
        "df1.to_csv('supervised_lexicon_step1.csv')\n",
        "#counting\n",
        "blob_1=df1.groupby('senti_textblob').count()\n",
        "print(\"textblob_step1:\",blob_1[\"textblob_polarity\"])\n",
        "\n",
        "wordnet_1=df1.groupby('senti_wordnet').count()\n",
        "print(\"wordnet_step1:\",wordnet_1[\"sentiword_analysis\"])\n",
        "\n",
        "#using senti_label_2\n",
        "df1['senti_vader'] = df1['vader_score'].apply(senti_label_2)\n",
        "df1\n",
        "vader_1=df1.groupby('senti_vader').count()\n",
        "print('vader_step1:',vader_1[\"vader_score\"])\n",
        "df1\n",
        "df1.to_csv(\"supervised_lexicon_step1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnyuEMTu_2S7"
      },
      "outputs": [],
      "source": [
        "supervised_lexicon_step1 = df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfBk-HjW_2S7"
      },
      "outputs": [],
      "source": [
        "#Step2\n",
        "#using senti_label\n",
        "df2['senti_textblob'] = df2['textblob_polarity'].apply(senti_label)\n",
        "df2['senti_wordnet'] = df2['sentiword_analysis'].apply(senti_label)\n",
        "df2\n",
        "#counting\n",
        "blob_2=df2.groupby('senti_textblob').count()\n",
        "print(\"textblob_step2:\",blob_2[\"textblob_polarity\"])\n",
        "\n",
        "wordnet_2=df2.groupby('senti_wordnet').count()\n",
        "print(\"wordnet_step2:\",wordnet_2[\"sentiword_analysis\"])\n",
        "\n",
        "#using senti_label_2\n",
        "df2['senti_vader'] = df2['vader_score'].apply(senti_label_2)\n",
        "df2\n",
        "vader_2=df2.groupby('senti_vader').count()\n",
        "print('vader_step2:',vader_2[\"vader_score\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P4nKaP2_2S8"
      },
      "outputs": [],
      "source": [
        "#Step3\n",
        "#using senti_label\n",
        "df3['senti_textblob'] = df3['textblob_polarity'].apply(senti_label)\n",
        "df3['senti_wordnet'] = df3['sentiword_analysis'].apply(senti_label)\n",
        "df3\n",
        "#counting\n",
        "blob_3=df3.groupby('senti_textblob').count()\n",
        "print(\"textblob_step3:\",blob_3[\"textblob_polarity\"])\n",
        "\n",
        "wordnet_3=df3.groupby('senti_wordnet').count()\n",
        "print(\"wordnet_step3:\",wordnet_3[\"sentiword_analysis\"])\n",
        "\n",
        "#using senti_label_2\n",
        "df3['senti_vader'] = df3['vader_score'].apply(senti_label_2)\n",
        "vader_3=df3.groupby('senti_vader').count()\n",
        "print('vader_step3:',vader_3[\"vader_score\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtiOht8K_2S8"
      },
      "outputs": [],
      "source": [
        "#new dfs\n",
        "new_dfs=[df1,df2,df3]\n",
        "new_dfs=pd.concat(new_dfs)\n",
        "new_dfs=new_dfs.reset_index(drop=True)\n",
        "len(new_dfs)\n",
        "# new_dfs.to_csv(\"unsupervised_datasets/lexicon_all.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yupr9xIV_2S8"
      },
      "outputs": [],
      "source": [
        "dfs['senti_textblob'] = dfs['textblob_polarity'].apply(senti_label)\n",
        "dfs['senti_wordnet'] = dfs['sentiword_analysis'].apply(senti_label)\n",
        "dfs\n",
        "#counting\n",
        "blob_all=dfs.groupby('senti_textblob').count()\n",
        "print(\"textblob_all:\",blob_all[\"textblob_polarity\"])\n",
        "\n",
        "wordnet_all=dfs.groupby('senti_wordnet').count()\n",
        "print(\"wordnet_all:\",wordnet_all[\"sentiword_analysis\"])\n",
        "\n",
        "#using senti_label_2\n",
        "dfs['senti_vader'] = dfs['vader_score'].apply(senti_label_2)\n",
        "vader_all=dfs.groupby('senti_vader').count()\n",
        "print('vader_all:',vader_all[\"vader_score\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uitwUU3N_2S8"
      },
      "outputs": [],
      "source": [
        "vader_all=dfs.groupby('senti_vader').count()\n",
        "print(vader_all[\"vader_score\"])\n",
        "wordnet_all=dfs.groupby('senti_wordnet').count()\n",
        "print(wordnet_all[\"sentiword_analysis\"])\n",
        "blob_all=dfs.groupby('senti_textblob').count()\n",
        "print(blob_all[\"textblob_polarity\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdTDw6jv_2S8"
      },
      "outputs": [],
      "source": [
        "#extreme positive\n",
        "slight_pos_wordnet = new_dfs[new_dfs['sentiword_analysis'].between(0.5,1)]\n",
        "print(\"wordnet:\",len(slight_pos_wordnet))\n",
        "slight_pos_blob = new_dfs[new_dfs['textblob_polarity'].between(0.5,1)]\n",
        "print(\"textbolb\",len(slight_pos_blob))\n",
        "slight_pos_vader = new_dfs[new_dfs['vader_score'].between(0.5,1)]\n",
        "print(\"vader\",len(slight_pos_vader))\n",
        "#extreme negative \n",
        "slight_pos_wordnet = new_dfs[new_dfs['sentiword_analysis'].between(-1,-0.5)]\n",
        "print(\"wordnet:\",len(slight_pos_wordnet))\n",
        "slight_pos_blob = new_dfs[new_dfs['textblob_polarity'].between(-1,-0.5)]\n",
        "print(\"textblob\",len(slight_pos_blob))\n",
        "slight_pos_vader = new_dfs[new_dfs['vader_score'].between(-1,-0.5)]\n",
        "print(\"vader\",len(slight_pos_vader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjofDNAC_2S8"
      },
      "outputs": [],
      "source": [
        "#for each city: we use mean value to show their sentiment orientation of each city\n",
        "#for england, each step with three methods,counting the number of negative, positive and neutral\n",
        "#step1--Textblob\n",
        "#London\n",
        "London_1 = df1[df1['near'].str.contains('London|london')]\n",
        "London_1 = London_1.reset_index(drop=True)\n",
        "#rows (include 1)/ all rows\n",
        "london_pos_1=len(London_1[London_1['senti_textblob']==1])/len(London_1)\n",
        "london_neg_1=len(London_1[London_1['senti_textblob']==-1])/len(London_1)\n",
        "print(\"london_pos_1\",round(london_pos_1,3))\n",
        "print(\"london_neg_1\",round(london_neg_1,3))\n",
        "\n",
        "#Birmingham\n",
        "Birmingham_1 = df1[df1['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_1 = Birmingham_1.reset_index(drop=True)\n",
        "#rows include 1 or -1/ all rows\n",
        "Birmingham_pos_1=len(Birmingham_1[Birmingham_1['senti_textblob']==1])/len(Birmingham_1)\n",
        "Birmingham_neg_1=len(Birmingham_1[Birmingham_1['senti_textblob']==-1])/len(Birmingham_1)\n",
        "print(\"Birmingham_pos_1\",round(Birmingham_pos_1,3))\n",
        "print(\"Birmingham_neg_1\",round(Birmingham_neg_1,3))\n",
        "\n",
        "#Bristol\n",
        "Bristol_1 = df1[df1['near'].str.contains('Bristol|bristol')]\n",
        "Bristol_1 = Bristol_1.reset_index(drop=True)\n",
        "Bristol_pos_1=len(Bristol_1[Bristol_1['senti_textblob']==1])/len(Bristol_1)\n",
        "Bristol_neg_1=len(Bristol_1[Bristol_1['senti_textblob']==-1])/len(Bristol_1)\n",
        "print(\"Bristol_pos_1\",round(Bristol_pos_1,3))\n",
        "print(\"Bristol_neg_1\",round(Bristol_neg_1,3))\n",
        "\n",
        "# #Leeds\n",
        "Leeds_1 = df1[df1['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_1 = Leeds_1.reset_index(drop=True)\n",
        "Leeds_pos_1=len(Leeds_1[Leeds_1['senti_textblob']==1])/len(Leeds_1)\n",
        "Leeds_neg_1=len(Leeds_1[Leeds_1['senti_textblob']==-1])/len(Leeds_1)\n",
        "print(\"Leeds_pos_1\",round(Leeds_pos_1,3))\n",
        "print(\"Leeds_neg_1\",round(Leeds_neg_1,3))\n",
        "\n",
        "# #Liverpool\n",
        "Liverpool_1 = df1[df1['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_1 = Liverpool_1.reset_index(drop=True)\n",
        "Liverpool_pos_1=len(Liverpool_1[Liverpool_1['senti_textblob']==1])/len(Liverpool_1)\n",
        "Liverpool_neg_1=len(Liverpool_1[Liverpool_1['senti_textblob']==-1])/len(Liverpool_1)\n",
        "print(\"Liverpool_pos_1\",round(Liverpool_pos_1,3))\n",
        "print(\"Liverpool_neg_1\",round(Liverpool_neg_1,3))\n",
        "\n",
        "#Manchester\n",
        "Manchester_1 = df1[df1['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_1 = Manchester_1.reset_index(drop=True)\n",
        "Manchester_pos_1=len(Manchester_1[Manchester_1['senti_textblob']==1])/len(Manchester_1)\n",
        "Manchester_neg_1=len(Manchester_1[Manchester_1['senti_textblob']==-1])/len(Manchester_1)\n",
        "print(\"Manchester_pos_1\",round(Manchester_pos_1,3))\n",
        "print(\"Manchester_neg_1\",round(Manchester_neg_1,3))\n",
        "\n",
        "#Newcastle\n",
        "Newcastle_1 = df1[df1['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_1 = Newcastle_1.reset_index(drop=True)\n",
        "Newcastle_pos_1=len(Newcastle_1[Newcastle_1['senti_textblob']==1])/len(Newcastle_1)\n",
        "Newcastle_neg_1=len(Newcastle_1[Newcastle_1['senti_textblob']==-1])/len(Newcastle_1)\n",
        "print(\"Newcastle_pos_1\",round(Newcastle_pos_1,3))\n",
        "print(\"Newcastle_neg_1\",round(Newcastle_neg_1,3))\n",
        "\n",
        "#Nottingham\n",
        "Nottingham_1 = df1[df1['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_1 = Nottingham_1.reset_index(drop=True)\n",
        "Nottingham_pos_1=len(Nottingham_1[Nottingham_1['senti_textblob']==1])/len(Nottingham_1)\n",
        "Nottingham_neg_1=len(Nottingham_1[Nottingham_1['senti_textblob']==-1])/len(Nottingham_1)\n",
        "print(\"Nottingham_pos_1\",round(Nottingham_pos_1,3))\n",
        "print(\"Nottingham_neg_1\",round(Nottingham_neg_1,3))\n",
        "\n",
        "#Sheffield\n",
        "Sheffield_1 = df1[df1['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_1 = Sheffield_1.reset_index(drop=True)\n",
        "Sheffield_pos_1=len(Sheffield_1[Sheffield_1['senti_textblob']==1])/len(Sheffield_1)\n",
        "Sheffield_neg_1=len(Sheffield_1[Sheffield_1['senti_textblob']==-1])/len(Sheffield_1)\n",
        "print(\"Sheffield_pos_1\",round(Sheffield_pos_1,3))\n",
        "print(\"Sheffield_neg_1\",round(Sheffield_neg_1,3))\n",
        "\n",
        "#Southampton\n",
        "Southampton_1 = df1[df1['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_1 = Southampton_1.reset_index(drop=True)\n",
        "Southampton_pos_1=len(Southampton_1[Southampton_1['senti_textblob']==1])/len(Southampton_1)\n",
        "Southampton_neg_1=len(Southampton_1[Southampton_1['senti_textblob']==-1])/len(Southampton_1)\n",
        "print(\"Southampton_pos_1\",round(Southampton_pos_1,3))\n",
        "print(\"Southampton_neg_1\",round(Southampton_neg_1,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwmA6yJa_2S9"
      },
      "outputs": [],
      "source": [
        "#step2--Textblob\n",
        "#London\n",
        "London_2 = df2[df2['near'].str.contains('London|london')]\n",
        "London_2 = London_2.reset_index(drop=True)\n",
        "#rows include 1/ all rows\n",
        "london_pos_2=len(London_2[London_2['senti_textblob']==1])/len(London_2)\n",
        "london_neg_2=len(London_2[London_2['senti_textblob']==-1])/len(London_2)\n",
        "print(\"london_pos_2\",round(london_pos_2,3))\n",
        "print(\"london_neg_2\",round(london_neg_2,3))\n",
        "\n",
        "#Birmingham\n",
        "Birmingham_2 = df2[df2['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_2 = Birmingham_2.reset_index(drop=True)\n",
        "#rows include 1 or -1/ all rows\n",
        "Birmingham_pos_2=len(Birmingham_2[Birmingham_2['senti_textblob']==1])/len(Birmingham_2)\n",
        "Birmingham_neg_2=len(Birmingham_2[Birmingham_2['senti_textblob']==-1])/len(Birmingham_2)\n",
        "print(\"Birmingham_pos_2\",round(Birmingham_pos_2,3))\n",
        "print(\"Birmingham_neg_2\",round(Birmingham_neg_2,3))\n",
        "\n",
        "#Bristol\n",
        "Bristol_2 = df2[df2['near'].str.contains('Bristol|bristol')]\n",
        "Bristol_2 = Bristol_2.reset_index(drop=True)\n",
        "Bristol_pos_2=len(Bristol_2[Bristol_2['senti_textblob']==1])/len(Bristol_2)\n",
        "Bristol_neg_2=len(Bristol_2[Bristol_2['senti_textblob']==-1])/len(Bristol_2)\n",
        "print(\"Bristol_pos_2\",round(Bristol_pos_2,3))\n",
        "print(\"Bristol_neg_2\",round(Bristol_neg_2,3))\n",
        "\n",
        "# #Leeds\n",
        "Leeds_2 = df2[df2['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_2 = Leeds_2.reset_index(drop=True)\n",
        "Leeds_pos_2=len(Leeds_2[Leeds_2['senti_textblob']==1])/len(Leeds_2)\n",
        "Leeds_neg_2=len(Leeds_2[Leeds_2['senti_textblob']==-1])/len(Leeds_2)\n",
        "print(\"Leeds_pos_2\",round(Leeds_pos_2,3))\n",
        "print(\"Leeds_neg_2\",round(Leeds_neg_2,3))\n",
        "\n",
        "# #Liverpool\n",
        "Liverpool_2 = df2[df2['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_2 = Liverpool_2.reset_index(drop=True)\n",
        "Liverpool_pos_2=len(Liverpool_2[Liverpool_2['senti_textblob']==1])/len(Liverpool_2)\n",
        "Liverpool_neg_2=len(Liverpool_2[Liverpool_2['senti_textblob']==-1])/len(Liverpool_2)\n",
        "print(\"Liverpool_pos_2\",round(Liverpool_pos_2,3))\n",
        "print(\"Liverpool_neg_2\",round(Liverpool_neg_2,3))\n",
        "\n",
        "#Manchester\n",
        "Manchester_2 = df2[df2['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_2 = Manchester_2.reset_index(drop=True)\n",
        "Manchester_pos_2=len(Manchester_2[Manchester_2['senti_textblob']==1])/len(Manchester_2)\n",
        "Manchester_neg_2=len(Manchester_2[Manchester_2['senti_textblob']==-1])/len(Manchester_2)\n",
        "print(\"Manchester_pos_2\",round(Manchester_pos_2,3))\n",
        "print(\"Manchester_neg_2\",round(Manchester_neg_2,3))\n",
        "\n",
        "#Newcastle\n",
        "Newcastle_2 = df2[df2['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_2 = Newcastle_2.reset_index(drop=True)\n",
        "Newcastle_pos_2=len(Newcastle_2[Newcastle_2['senti_textblob']==1])/len(Newcastle_2)\n",
        "Newcastle_neg_2=len(Newcastle_2[Newcastle_2['senti_textblob']==-1])/len(Newcastle_2)\n",
        "print(\"Newcastle_pos_2\",round(Newcastle_pos_2,3))\n",
        "print(\"Newcastle_neg_2\",round(Newcastle_neg_2,3))\n",
        "\n",
        "#Nottingham\n",
        "Nottingham_2 = df2[df2['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_2 = Nottingham_2.reset_index(drop=True)\n",
        "Nottingham_pos_2=len(Nottingham_2[Nottingham_2['senti_textblob']==1])/len(Nottingham_2)\n",
        "Nottingham_neg_2=len(Nottingham_2[Nottingham_2['senti_textblob']==-1])/len(Nottingham_2)\n",
        "print(\"Nottingham_pos_2\",round(Nottingham_pos_2,3))\n",
        "print(\"Nottingham_neg_2\",round(Nottingham_neg_2,3))\n",
        "\n",
        "#Sheffield\n",
        "Sheffield_2 = df2[df2['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_2 = Sheffield_2.reset_index(drop=True)\n",
        "Sheffield_pos_2=len(Sheffield_2[Sheffield_2['senti_textblob']==1])/len(Sheffield_2)\n",
        "Sheffield_neg_2=len(Sheffield_2[Sheffield_2['senti_textblob']==-1])/len(Sheffield_2)\n",
        "print(\"Sheffield_pos_2\",round(Sheffield_pos_2,3))\n",
        "print(\"Sheffield_neg_2\",round(Sheffield_neg_2,3))\n",
        "\n",
        "#Southampton\n",
        "Southampton_2 = df2[df2['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_2 = Southampton_2.reset_index(drop=True)\n",
        "Southampton_pos_2=len(Southampton_2[Southampton_2['senti_textblob']==1])/len(Southampton_2)\n",
        "Southampton_neg_2=len(Southampton_2[Southampton_2['senti_textblob']==-1])/len(Southampton_2)\n",
        "print(\"Southampton_pos_2\",round(Southampton_pos_2,3))\n",
        "print(\"Southampton_neg_2\",round(Southampton_neg_2,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEajvJN1_2S9"
      },
      "outputs": [],
      "source": [
        "df3.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7kujGwg_2S9"
      },
      "outputs": [],
      "source": [
        "#step3--Textblob\n",
        "#London\n",
        "London_3 = df3[df3['near'].str.contains('London|london')]\n",
        "London_3 = London_3.reset_index(drop=True)\n",
        "#rows include 1/ all rows\n",
        "london_pos_3=len(London_3[London_3['senti_textblob']==1])/len(London_3)\n",
        "#len(London_1[London_1['senti_textblob']==0])/len(London_1)\n",
        "london_neg_3=len(London_3[London_3['senti_textblob']==-1])/len(London_3)\n",
        "print(\"london_pos_3\",round(london_pos_3,3))\n",
        "print(\"london_neg_3\",round(london_neg_3,3))\n",
        "\n",
        "#Birmingham\n",
        "Birmingham_3 = df3[df3['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_3 = Birmingham_3.reset_index(drop=True)\n",
        "#rows include 1 or -1/ all rows\n",
        "Birmingham_pos_3=len(Birmingham_3[Birmingham_3['senti_textblob']==1])/len(Birmingham_3)\n",
        "Birmingham_neg_3=len(Birmingham_3[Birmingham_3['senti_textblob']==-1])/len(Birmingham_3)\n",
        "print(\"Birmingham_pos_3\",round(Birmingham_pos_3,3))\n",
        "print(\"Birmingham_neg_3\",round(Birmingham_neg_3,3))\n",
        "\n",
        "#Bristol\n",
        "Bristol_3 = df3[df3['near'].str.contains('Bristol|bristol')]\n",
        "Bristol_3 = Bristol_3.reset_index(drop=True)\n",
        "Bristol_pos_3=len(Bristol_3[Bristol_3['senti_textblob']==1])/len(Bristol_3)\n",
        "Bristol_neg_3=len(Bristol_3[Bristol_3['senti_textblob']==-1])/len(Bristol_3)\n",
        "print(\"Bristol_pos_3\",round(Bristol_pos_3,3))\n",
        "print(\"Bristol_neg_3\",round(Bristol_neg_3,3))\n",
        "\n",
        "# #Leeds\n",
        "Leeds_3 = df3[df3['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_3 = Leeds_3.reset_index(drop=True)\n",
        "Leeds_pos_3=len(Leeds_3[Leeds_3['senti_textblob']==1])/len(Leeds_3)\n",
        "Leeds_neg_3=len(Leeds_3[Leeds_3['senti_textblob']==-1])/len(Leeds_3)\n",
        "print(\"Leeds_pos_3\",round(Leeds_pos_3,3))\n",
        "print(\"Leeds_neg_3\",round(Leeds_neg_3,3))\n",
        "\n",
        "# #Liverpool\n",
        "Liverpool_3 = df3[df3['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_3 = Liverpool_3.reset_index(drop=True)\n",
        "Liverpool_pos_3=len(Liverpool_3[Liverpool_3['senti_textblob']==1])/len(Liverpool_3)\n",
        "Liverpool_neg_3=len(Liverpool_3[Liverpool_3['senti_textblob']==-1])/len(Liverpool_3)\n",
        "print(\"Liverpool_pos_3\",round(Liverpool_pos_3,3))\n",
        "print(\"Liverpool_neg_3\",round(Liverpool_neg_3,3))\n",
        "\n",
        "#Manchester\n",
        "Manchester_3 = df3[df3['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_3 = Manchester_3.reset_index(drop=True)\n",
        "Manchester_pos_3=len(Manchester_3[Manchester_3['senti_textblob']==1])/len(Manchester_3)\n",
        "Manchester_neg_3=len(Manchester_3[Manchester_3['senti_textblob']==-1])/len(Manchester_3)\n",
        "print(\"Manchester_pos_3\",round(Manchester_pos_3,2))\n",
        "print(\"Manchester_neg_3\",round(Manchester_neg_3,2))\n",
        "\n",
        "#Newcastle\n",
        "Newcastle_3 = df3[df3['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_3 = Newcastle_3.reset_index(drop=True)\n",
        "Newcastle_pos_3=len(Newcastle_3[Newcastle_3['senti_textblob']==1])/len(Newcastle_3)\n",
        "Newcastle_neg_3=len(Newcastle_3[Newcastle_3['senti_textblob']==-1])/len(Newcastle_3)\n",
        "print(\"Newcastle_pos_3\",round(Newcastle_pos_3,3))\n",
        "print(\"Newcastle_neg_3\",round(Newcastle_neg_3,3))\n",
        "\n",
        "#Nottingham\n",
        "Nottingham_3 = df3[df3['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_3 = Nottingham_3.reset_index(drop=True)\n",
        "Nottingham_pos_3=len(Nottingham_3[Nottingham_3['senti_textblob']==1])/len(Nottingham_3)\n",
        "Nottingham_neg_3=len(Nottingham_3[Nottingham_3['senti_textblob']==-1])/len(Nottingham_3)\n",
        "print(\"Nottingham_pos_3\",round(Nottingham_pos_3,3))\n",
        "print(\"Nottingham_neg_3\",round(Nottingham_neg_3,3))\n",
        "\n",
        "#Sheffield\n",
        "Sheffield_3 = df3[df3['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_3 = Sheffield_3.reset_index(drop=True)\n",
        "Sheffield_pos_3=len(Sheffield_3[Sheffield_3['senti_textblob']==1])/len(Sheffield_3)\n",
        "Sheffield_neg_3=len(Sheffield_3[Sheffield_3['senti_textblob']==-1])/len(Sheffield_3)\n",
        "print(\"Sheffield_pos_3\",round(Sheffield_pos_3,3))\n",
        "print(\"Sheffield_neg_3\",round(Sheffield_neg_3,3))\n",
        "\n",
        "#Southampton\n",
        "Southampton_3 = df3[df3['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_3 = Southampton_3.reset_index(drop=True)\n",
        "Southampton_pos_3=len(Southampton_3[Southampton_3['senti_textblob']==1])/len(Southampton_3)\n",
        "Southampton_neg_3=len(Southampton_3[Southampton_3['senti_textblob']==-1])/len(Southampton_3)\n",
        "print(\"Southampton_pos_3\",round(Southampton_pos_3,3))\n",
        "print(\"Southampton_neg_3\",round(Southampton_neg_3,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_HZdxW5_2S9"
      },
      "outputs": [],
      "source": [
        "#step1--VADER\n",
        "#London\n",
        "London_1 = df1[df1['near'].str.contains('London|london')]\n",
        "London_1 = London_1.reset_index(drop=True)\n",
        "#rows include 1/ all rows\n",
        "london_pos_1=len(London_1[London_1['senti_vader']==1])/len(London_1)\n",
        "#london_neu=len(London_1[London_1['senti_textblob']==0])/len(London_1)\n",
        "london_neg_1=len(London_1[London_1['senti_vader']==-1])/len(London_1)\n",
        "print(\"london_pos_1\",round(london_pos_1,3))\n",
        "print(\"london_neg_1\",round(london_neg_1,3))\n",
        "# print(\"london_neg_1\",round(london_neu,3))\n",
        "#Birmingham\n",
        "Birmingham_1 = df1[df1['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_1 = Birmingham_1.reset_index(drop=True)\n",
        "#rows include 1 or -1/ all rows\n",
        "Birmingham_pos_1=len(Birmingham_1[Birmingham_1['senti_vader']==1])/len(Birmingham_1)\n",
        "Birmingham_neg_1=len(Birmingham_1[Birmingham_1['senti_vader']==-1])/len(Birmingham_1)\n",
        "print(\"Birmingham_pos_1\",round(Birmingham_pos_1,3))\n",
        "print(\"Birmingham_neg_1\",round(Birmingham_neg_1,3))\n",
        "\n",
        "#Bristol\n",
        "Bristol_1 = df1[df1['near'].str.contains('Bristol|bristol')]\n",
        "Bristol_1 = Bristol_1.reset_index(drop=True)\n",
        "Bristol_pos_1=len(Bristol_1[Bristol_1['senti_vader']==1])/len(Bristol_1)\n",
        "Bristol_neg_1=len(Bristol_1[Bristol_1['senti_vader']==-1])/len(Bristol_1)\n",
        "print(\"Bristol_pos_1\",round(Bristol_pos_1,3))\n",
        "print(\"Bristol_neg_1\",round(Bristol_neg_1,3))\n",
        "\n",
        "# #Leeds\n",
        "Leeds_1 = df1[df1['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_1 = Leeds_1.reset_index(drop=True)\n",
        "Leeds_pos_1=len(Leeds_1[Leeds_1['senti_vader']==1])/len(Leeds_1)\n",
        "Leeds_neg_1=len(Leeds_1[Leeds_1['senti_vader']==-1])/len(Leeds_1)\n",
        "print(\"Leeds_pos_1\",round(Leeds_pos_1,3))\n",
        "print(\"Leeds_neg_1\",round(Leeds_neg_1,3))\n",
        "\n",
        "#Liverpool\n",
        "Liverpool_1 = df1[df1['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_1 = Liverpool_1.reset_index(drop=True)\n",
        "Liverpool_pos_1=len(Liverpool_1[Liverpool_1['senti_vader']==1])/len(Liverpool_1)\n",
        "Liverpool_neg_1=len(Liverpool_1[Liverpool_1['senti_vader']==-1])/len(Liverpool_1)\n",
        "print(\"Liverpool_pos_1\",round(Liverpool_pos_1,3))\n",
        "print(\"Liverpool_neg_1\",round(Liverpool_neg_1,3))\n",
        "\n",
        "#Manchester\n",
        "Manchester_1 = df1[df1['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_1 = Manchester_1.reset_index(drop=True)\n",
        "Manchester_pos_1=len(Manchester_1[Manchester_1['senti_vader']==1])/len(Manchester_1)\n",
        "Manchester_neg_1=len(Manchester_1[Manchester_1['senti_vader']==-1])/len(Manchester_1)\n",
        "print(\"Manchester_pos_1\",round(Manchester_pos_1,3))\n",
        "print(\"Manchester_neg_1\",round(Manchester_neg_1,3))\n",
        "\n",
        "#Newcastle\n",
        "Newcastle_1 = df1[df1['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_1 = Newcastle_1.reset_index(drop=True)\n",
        "Newcastle_pos_1=len(Newcastle_1[Newcastle_1['senti_vader']==1])/len(Newcastle_1)\n",
        "Newcastle_neg_1=len(Newcastle_1[Newcastle_1['senti_vader']==-1])/len(Newcastle_1)\n",
        "print(\"Newcastle_pos_1\",round(Newcastle_pos_1,3))\n",
        "print(\"Newcastle_neg_1\",round(Newcastle_neg_1,3))\n",
        "\n",
        "#Nottingham\n",
        "Nottingham_1 = df1[df1['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_1 = Nottingham_1.reset_index(drop=True)\n",
        "Nottingham_pos_1=len(Nottingham_1[Nottingham_1['senti_vader']==1])/len(Nottingham_1)\n",
        "Nottingham_neg_1=len(Nottingham_1[Nottingham_1['senti_vader']==-1])/len(Nottingham_1)\n",
        "print(\"Nottingham_pos_1\",round(Nottingham_pos_1,3))\n",
        "print(\"Nottingham_neg_1\",round(Nottingham_neg_1,3))\n",
        "\n",
        "#Sheffield\n",
        "Sheffield_1 = df1[df1['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_1 = Sheffield_1.reset_index(drop=True)\n",
        "Sheffield_pos_1=len(Sheffield_1[Sheffield_1['senti_vader']==1])/len(Sheffield_1)\n",
        "Sheffield_neg_1=len(Sheffield_1[Sheffield_1['senti_vader']==-1])/len(Sheffield_1)\n",
        "print(\"Sheffield_pos_1\",round(Sheffield_pos_1,3))\n",
        "print(\"Sheffield_neg_1\",round(Sheffield_neg_1,3))\n",
        "\n",
        "#Southampton\n",
        "Southampton_1 = df1[df1['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_1 = Southampton_1.reset_index(drop=True)\n",
        "Southampton_pos_1=len(Southampton_1[Southampton_1['senti_vader']==1])/len(Southampton_1)\n",
        "Southampton_neg_1=len(Southampton_1[Southampton_1['senti_vader']==-1])/len(Southampton_1)\n",
        "print(\"Southampton_pos_1\",round(Southampton_pos_1,3))\n",
        "print(\"Southampton_neg_1\",round(Southampton_neg_1,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwa36uyC_2S-"
      },
      "outputs": [],
      "source": [
        "#step2--VADER\n",
        "#London\n",
        "London_2 = df2[df2['near'].str.contains('London|london')]\n",
        "London_2 = London_2.reset_index(drop=True)\n",
        "#rows include 1/ all rows\n",
        "london_pos_2=len(London_2[London_2['senti_vader']==1])/len(London_2)\n",
        "#len(London_1[London_1['senti_textblob']==0])/len(London_1)\n",
        "london_neg_2=len(London_2[London_2['senti_vader']==-1])/len(London_2)\n",
        "print(\"london_pos_2\",round(london_pos_2,3))\n",
        "print(\"london_neg_2\",round(london_neg_2,3))\n",
        "\n",
        "#Birmingham\n",
        "Birmingham_2 = df2[df2['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_2 = Birmingham_2.reset_index(drop=True)\n",
        "#rows include 1 or -1/ all rows\n",
        "Birmingham_pos_2=len(Birmingham_2[Birmingham_2['senti_vader']==1])/len(Birmingham_2)\n",
        "Birmingham_neg_2=len(Birmingham_2[Birmingham_2['senti_vader']==-1])/len(Birmingham_2)\n",
        "print(\"Birmingham_pos_2\",round(Birmingham_pos_2,3))\n",
        "print(\"Birmingham_neg_2\",round(Birmingham_neg_2,3))\n",
        "\n",
        "#Bristol\n",
        "Bristol_2 = df2[df2['near'].str.contains('Bristol|bristol')]\n",
        "Bristol_2 = Bristol_2.reset_index(drop=True)\n",
        "Bristol_pos_2=len(Bristol_2[Bristol_2['senti_vader']==1])/len(Bristol_2)\n",
        "Bristol_neg_2=len(Bristol_2[Bristol_2['senti_vader']==-1])/len(Bristol_2)\n",
        "print(\"Bristol_pos_2\",round(Bristol_pos_2,3))\n",
        "print(\"Bristol_neg_2\",round(Bristol_neg_2,3))\n",
        "\n",
        "# #Leeds\n",
        "Leeds_2 = df2[df2['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_2 = Leeds_2.reset_index(drop=True)\n",
        "Leeds_pos_2=len(Leeds_2[Leeds_2['senti_vader']==1])/len(Leeds_2)\n",
        "Leeds_neg_2=len(Leeds_2[Leeds_2['senti_vader']==-1])/len(Leeds_2)\n",
        "print(\"Leeds_pos_2\",round(Leeds_pos_2,3))\n",
        "print(\"Leeds_neg_2\",round(Leeds_neg_2,3))\n",
        "\n",
        "# #Liverpool\n",
        "Liverpool_2 = df2[df2['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_2 = Liverpool_2.reset_index(drop=True)\n",
        "Liverpool_pos_2=len(Liverpool_2[Liverpool_2['senti_vader']==1])/len(Liverpool_2)\n",
        "Liverpool_neg_2=len(Liverpool_2[Liverpool_2['senti_vader']==-1])/len(Liverpool_2)\n",
        "print(\"Liverpool_pos_2\",round(Liverpool_pos_2,3))\n",
        "print(\"Liverpool_neg_2\",round(Liverpool_neg_2,3))\n",
        "\n",
        "#Manchester\n",
        "Manchester_2 = df2[df2['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_2 = Manchester_2.reset_index(drop=True)\n",
        "Manchester_pos_2=len(Manchester_2[Manchester_2['senti_vader']==1])/len(Manchester_2)\n",
        "Manchester_neg_2=len(Manchester_2[Manchester_2['senti_vader']==-1])/len(Manchester_2)\n",
        "print(\"Manchester_pos_2\",round(Manchester_pos_2,3))\n",
        "print(\"Manchester_neg_2\",round(Manchester_neg_2,3))\n",
        "\n",
        "#Newcastle\n",
        "Newcastle_2 = df2[df2['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_2 = Newcastle_2.reset_index(drop=True)\n",
        "Newcastle_pos_2=len(Newcastle_2[Newcastle_2['senti_vader']==1])/len(Newcastle_2)\n",
        "Newcastle_neg_2=len(Newcastle_2[Newcastle_2['senti_vader']==-1])/len(Newcastle_2)\n",
        "print(\"Newcastle_pos_2\",round(Newcastle_pos_2,3))\n",
        "print(\"Newcastle_neg_2\",round(Newcastle_neg_2,3))\n",
        "\n",
        "#Nottingham\n",
        "Nottingham_2 = df2[df2['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_2 = Nottingham_2.reset_index(drop=True)\n",
        "Nottingham_pos_2=len(Nottingham_2[Nottingham_2['senti_vader']==1])/len(Nottingham_2)\n",
        "Nottingham_neg_2=len(Nottingham_2[Nottingham_2['senti_vader']==-1])/len(Nottingham_2)\n",
        "print(\"Nottingham_pos_2\",round(Nottingham_pos_2,3))\n",
        "print(\"Nottingham_neg_2\",round(Nottingham_neg_2,3))\n",
        "\n",
        "#Sheffield\n",
        "Sheffield_2 = df2[df2['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_2 = Sheffield_2.reset_index(drop=True)\n",
        "Sheffield_pos_2=len(Sheffield_2[Sheffield_2['senti_vader']==1])/len(Sheffield_2)\n",
        "Sheffield_neg_2=len(Sheffield_2[Sheffield_2['senti_vader']==-1])/len(Sheffield_2)\n",
        "print(\"Sheffield_pos_2\",round(Sheffield_pos_2,3))\n",
        "print(\"Sheffield_neg_2\",round(Sheffield_neg_2,3))\n",
        "\n",
        "#Southampton\n",
        "Southampton_2 = df2[df2['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_2 = Southampton_2.reset_index(drop=True)\n",
        "Southampton_pos_2=len(Southampton_2[Southampton_2['senti_vader']==1])/len(Southampton_2)\n",
        "Southampton_neg_2=len(Southampton_2[Southampton_2['senti_vader']==-1])/len(Southampton_2)\n",
        "print(\"Southampton_pos_2\",round(Southampton_pos_2,3))\n",
        "print(\"Southampton_neg_2\",round(Southampton_neg_2,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ4hJpQ9_2S-"
      },
      "outputs": [],
      "source": [
        "#step3--VADER\n",
        "#London\n",
        "London_3 = df3[df3['near'].str.contains('London|london')]\n",
        "London_3 = London_3.reset_index(drop=True)\n",
        "#rows include 1/ all rows\n",
        "london_pos_3=len(London_3[London_3['senti_vader']==1])/len(London_3)\n",
        "#len(London_1[London_1['senti_textblob']==0])/len(London_1)\n",
        "london_neg_3=len(London_3[London_3['senti_vader']==-1])/len(London_3)\n",
        "print(\"london_pos_3\",round(london_pos_3,3))\n",
        "print(\"london_neg_3\",round(london_neg_3,3))\n",
        "\n",
        "#Birmingham\n",
        "Birmingham_3 = df3[df3['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_3 = Birmingham_3.reset_index(drop=True)\n",
        "#rows include 1 or -1/ all rows\n",
        "Birmingham_pos_3=len(Birmingham_3[Birmingham_3['senti_vader']==1])/len(Birmingham_3)\n",
        "Birmingham_neg_3=len(Birmingham_3[Birmingham_3['senti_vader']==-1])/len(Birmingham_3)\n",
        "print(\"Birmingham_pos_3\",round(Birmingham_pos_3,3))\n",
        "print(\"Birmingham_neg_3\",round(Birmingham_neg_3,3))\n",
        "\n",
        "#Bristol\n",
        "Bristol_3 = df3[df3['near'].str.contains('Bristol|bristol')]\n",
        "Bristol_3 = Bristol_3.reset_index(drop=True)\n",
        "Bristol_pos_3=len(Bristol_3[Bristol_3['senti_vader']==1])/len(Bristol_3)\n",
        "Bristol_neg_3=len(Bristol_3[Bristol_3['senti_vader']==-1])/len(Bristol_3)\n",
        "print(\"Bristol_pos_3\",round(Bristol_pos_3,3))\n",
        "print(\"Bristol_neg_3\",round(Bristol_neg_3,3))\n",
        "\n",
        "# #Leeds\n",
        "Leeds_3 = df3[df3['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_3 = Leeds_3.reset_index(drop=True)\n",
        "Leeds_pos_3=len(Leeds_3[Leeds_3['senti_vader']==1])/len(Leeds_3)\n",
        "Leeds_neg_3=len(Leeds_3[Leeds_3['senti_vader']==-1])/len(Leeds_3)\n",
        "print(\"Leeds_pos_3\",round(Leeds_pos_3,3))\n",
        "print(\"Leeds_neg_3\",round(Leeds_neg_3,3))\n",
        "\n",
        "# #Liverpool\n",
        "Liverpool_3 = df3[df3['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_3 = Liverpool_3.reset_index(drop=True)\n",
        "Liverpool_pos_3=len(Liverpool_3[Liverpool_3['senti_vader']==1])/len(Liverpool_3)\n",
        "Liverpool_neg_3=len(Liverpool_3[Liverpool_3['senti_vader']==-1])/len(Liverpool_3)\n",
        "print(\"Liverpool_pos_3\",round(Liverpool_pos_3,3))\n",
        "print(\"Liverpool_neg_3\",round(Liverpool_neg_3,3))\n",
        "\n",
        "#Manchester\n",
        "Manchester_3 = df3[df3['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_3 = Manchester_3.reset_index(drop=True)\n",
        "Manchester_pos_3=len(Manchester_3[Manchester_3['senti_vader']==1])/len(Manchester_3)\n",
        "Manchester_neg_3=len(Manchester_3[Manchester_3['senti_vader']==-1])/len(Manchester_3)\n",
        "print(\"Manchester_pos_3\",round(Manchester_pos_3,2))\n",
        "print(\"Manchester_neg_3\",round(Manchester_neg_3,2))\n",
        "\n",
        "#Newcastle\n",
        "Newcastle_3 = df3[df3['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_3 = Newcastle_3.reset_index(drop=True)\n",
        "Newcastle_pos_3=len(Newcastle_3[Newcastle_3['senti_vader']==1])/len(Newcastle_3)\n",
        "Newcastle_neg_3=len(Newcastle_3[Newcastle_3['senti_vader']==-1])/len(Newcastle_3)\n",
        "print(\"Newcastle_pos_3\",round(Newcastle_pos_3,3))\n",
        "print(\"Newcastle_neg_3\",round(Newcastle_neg_3,3))\n",
        "\n",
        "#Nottingham\n",
        "Nottingham_3 = df3[df3['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_3 = Nottingham_3.reset_index(drop=True)\n",
        "Nottingham_pos_3=len(Nottingham_3[Nottingham_3['senti_vader']==1])/len(Nottingham_3)\n",
        "Nottingham_neg_3=len(Nottingham_3[Nottingham_3['senti_vader']==-1])/len(Nottingham_3)\n",
        "print(\"Nottingham_pos_3\",round(Nottingham_pos_3,3))\n",
        "print(\"Nottingham_neg_3\",round(Nottingham_neg_3,3))\n",
        "\n",
        "#Sheffield\n",
        "Sheffield_3 = df3[df3['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_3 = Sheffield_3.reset_index(drop=True)\n",
        "Sheffield_pos_3=len(Sheffield_3[Sheffield_3['senti_vader']==1])/len(Sheffield_3)\n",
        "Sheffield_neg_3=len(Sheffield_3[Sheffield_3['senti_vader']==-1])/len(Sheffield_3)\n",
        "print(\"Sheffield_pos_3\",round(Sheffield_pos_3,3))\n",
        "print(\"Sheffield_neg_3\",round(Sheffield_neg_3,3))\n",
        "\n",
        "#Southampton\n",
        "Southampton_3 = df3[df3['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_3 = Southampton_3.reset_index(drop=True)\n",
        "Southampton_pos_3=len(Southampton_3[Southampton_3['senti_vader']==1])/len(Southampton_3)\n",
        "Southampton_neg_3=len(Southampton_3[Southampton_3['senti_vader']==-1])/len(Southampton_3)\n",
        "print(\"Southampton_pos_3\",round(Southampton_pos_3,3))\n",
        "print(\"Southampton_neg_3\",round(Southampton_neg_3,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ0FBFdm_2S-"
      },
      "outputs": [],
      "source": [
        "#step1--WordNet\n",
        "#London\n",
        "# senti_wordnet\n",
        "#London\n",
        "London_1 = df1[df1['near'].str.contains('London|london')]\n",
        "London_1 = London_1.reset_index(drop=True)\n",
        "#rows include 1/ all rows\n",
        "london_pos_1=len(London_1[London_1['senti_wordnet']==1])/len(London_1)\n",
        "#london_neu=len(London_1[London_1['senti_textblob']==0])/len(London_1)\n",
        "london_neg_1=len(London_1[London_1['senti_wordnet']==-1])/len(London_1)\n",
        "print(\"london_pos_1\",round(london_pos_1,3))\n",
        "print(\"london_neg_1\",round(london_neg_1,3))\n",
        "# print(\"london_neg_1\",round(london_neu,3))\n",
        "#Birmingham\n",
        "Birmingham_1 = df1[df1['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_1 = Birmingham_1.reset_index(drop=True)\n",
        "#rows include 1 or -1/ all rows\n",
        "Birmingham_pos_1=len(Birmingham_1[Birmingham_1['senti_wordnet']==1])/len(Birmingham_1)\n",
        "Birmingham_neg_1=len(Birmingham_1[Birmingham_1['senti_wordnet']==-1])/len(Birmingham_1)\n",
        "print(\"Birmingham_pos_1\",round(Birmingham_pos_1,3))\n",
        "print(\"Birmingham_neg_1\",round(Birmingham_neg_1,3))\n",
        "\n",
        "#Bristol\n",
        "Bristol_1 = df1[df1['near'].str.contains('Bristol|bristol')]\n",
        "Bristol_1 = Bristol_1.reset_index(drop=True)\n",
        "Bristol_pos_1=len(Bristol_1[Bristol_1['senti_wordnet']==1])/len(Bristol_1)\n",
        "Bristol_neg_1=len(Bristol_1[Bristol_1['senti_wordnet']==-1])/len(Bristol_1)\n",
        "print(\"Bristol_pos_1\",round(Bristol_pos_1,3))\n",
        "print(\"Bristol_neg_1\",round(Bristol_neg_1,3))\n",
        "\n",
        "# #Leeds\n",
        "Leeds_1 = df1[df1['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_1 = Leeds_1.reset_index(drop=True)\n",
        "Leeds_pos_1=len(Leeds_1[Leeds_1['senti_wordnet']==1])/len(Leeds_1)\n",
        "Leeds_neg_1=len(Leeds_1[Leeds_1['senti_wordnet']==-1])/len(Leeds_1)\n",
        "print(\"Leeds_pos_1\",round(Leeds_pos_1,3))\n",
        "print(\"Leeds_neg_1\",round(Leeds_neg_1,3))\n",
        "\n",
        "#Liverpool\n",
        "Liverpool_1 = df1[df1['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_1 = Liverpool_1.reset_index(drop=True)\n",
        "Liverpool_pos_1=len(Liverpool_1[Liverpool_1['senti_wordnet']==1])/len(Liverpool_1)\n",
        "Liverpool_neg_1=len(Liverpool_1[Liverpool_1['senti_wordnet']==-1])/len(Liverpool_1)\n",
        "print(\"Liverpool_pos_1\",round(Liverpool_pos_1,3))\n",
        "print(\"Liverpool_neg_1\",round(Liverpool_neg_1,3))\n",
        "\n",
        "#Manchester\n",
        "Manchester_1 = df1[df1['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_1 = Manchester_1.reset_index(drop=True)\n",
        "Manchester_pos_1=len(Manchester_1[Manchester_1['senti_wordnet']==1])/len(Manchester_1)\n",
        "Manchester_neg_1=len(Manchester_1[Manchester_1['senti_wordnet']==-1])/len(Manchester_1)\n",
        "print(\"Manchester_pos_1\",round(Manchester_pos_1,3))\n",
        "print(\"Manchester_neg_1\",round(Manchester_neg_1,3))\n",
        "\n",
        "#Newcastle\n",
        "Newcastle_1 = df1[df1['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_1 = Newcastle_1.reset_index(drop=True)\n",
        "Newcastle_pos_1=len(Newcastle_1[Newcastle_1['senti_wordnet']==1])/len(Newcastle_1)\n",
        "Newcastle_neg_1=len(Newcastle_1[Newcastle_1['senti_wordnet']==-1])/len(Newcastle_1)\n",
        "print(\"Newcastle_pos_1\",round(Newcastle_pos_1,3))\n",
        "print(\"Newcastle_neg_1\",round(Newcastle_neg_1,3))\n",
        "\n",
        "#Nottingham\n",
        "Nottingham_1 = df1[df1['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_1 = Nottingham_1.reset_index(drop=True)\n",
        "Nottingham_pos_1=len(Nottingham_1[Nottingham_1['senti_wordnet']==1])/len(Nottingham_1)\n",
        "Nottingham_neg_1=len(Nottingham_1[Nottingham_1['senti_wordnet']==-1])/len(Nottingham_1)\n",
        "print(\"Nottingham_pos_1\",round(Nottingham_pos_1,3))\n",
        "print(\"Nottingham_neg_1\",round(Nottingham_neg_1,3))\n",
        "\n",
        "#Sheffield\n",
        "Sheffield_1 = df1[df1['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_1 = Sheffield_1.reset_index(drop=True)\n",
        "Sheffield_pos_1=len(Sheffield_1[Sheffield_1['senti_wordnet']==1])/len(Sheffield_1)\n",
        "Sheffield_neg_1=len(Sheffield_1[Sheffield_1['senti_wordnet']==-1])/len(Sheffield_1)\n",
        "print(\"Sheffield_pos_1\",round(Sheffield_pos_1,3))\n",
        "print(\"Sheffield_neg_1\",round(Sheffield_neg_1,3))\n",
        "\n",
        "#Southampton\n",
        "Southampton_1 = df1[df1['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_1 = Southampton_1.reset_index(drop=True)\n",
        "Southampton_pos_1=len(Southampton_1[Southampton_1['senti_wordnet']==1])/len(Southampton_1)\n",
        "Southampton_neg_1=len(Southampton_1[Southampton_1['senti_wordnet']==-1])/len(Southampton_1)\n",
        "print(\"Southampton_pos_1\",round(Southampton_pos_1,3))\n",
        "print(\"Southampton_neg_1\",round(Southampton_neg_1,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AX1it1M_2S_"
      },
      "outputs": [],
      "source": [
        "#step2--WordNet\n",
        "#London\n",
        "#step2--VADER\n",
        "#London\n",
        "London_2 = df2[df2['near'].str.contains('London|london')]\n",
        "London_2 = London_2.reset_index(drop=True)\n",
        "#rows include 1/ all rows\n",
        "london_pos_2=len(London_2[London_2['senti_wordnet']==1])/len(London_2)\n",
        "#len(London_1[London_1['senti_textblob']==0])/len(London_1)\n",
        "london_neg_2=len(London_2[London_2['senti_wordnet']==-1])/len(London_2)\n",
        "print(\"london_pos_2\",round(london_pos_2,3))\n",
        "print(\"london_neg_2\",round(london_neg_2,3))\n",
        "\n",
        "#Birmingham\n",
        "Birmingham_2 = df2[df2['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_2 = Birmingham_2.reset_index(drop=True)\n",
        "#rows include 1 or -1/ all rows\n",
        "Birmingham_pos_2=len(Birmingham_2[Birmingham_2['senti_wordnet']==1])/len(Birmingham_2)\n",
        "Birmingham_neg_2=len(Birmingham_2[Birmingham_2['senti_wordnet']==-1])/len(Birmingham_2)\n",
        "print(\"Birmingham_pos_2\",round(Birmingham_pos_2,3))\n",
        "print(\"Birmingham_neg_2\",round(Birmingham_neg_2,3))\n",
        "\n",
        "#Bristol\n",
        "Bristol_2 = df2[df2['near'].str.contains('Bristol|bristol')]\n",
        "Bristol_2 = Bristol_2.reset_index(drop=True)\n",
        "Bristol_pos_2=len(Bristol_2[Bristol_2['senti_wordnet']==1])/len(Bristol_2)\n",
        "Bristol_neg_2=len(Bristol_2[Bristol_2['senti_wordnet']==-1])/len(Bristol_2)\n",
        "print(\"Bristol_pos_2\",round(Bristol_pos_2,3))\n",
        "print(\"Bristol_neg_2\",round(Bristol_neg_2,3))\n",
        "\n",
        "# #Leeds\n",
        "Leeds_2 = df2[df2['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_2 = Leeds_2.reset_index(drop=True)\n",
        "Leeds_pos_2=len(Leeds_2[Leeds_2['senti_wordnet']==1])/len(Leeds_2)\n",
        "Leeds_neg_2=len(Leeds_2[Leeds_2['senti_wordnet']==-1])/len(Leeds_2)\n",
        "print(\"Leeds_pos_2\",round(Leeds_pos_2,3))\n",
        "print(\"Leeds_neg_2\",round(Leeds_neg_2,3))\n",
        "\n",
        "# #Liverpool\n",
        "Liverpool_2 = df2[df2['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_2 = Liverpool_2.reset_index(drop=True)\n",
        "Liverpool_pos_2=len(Liverpool_2[Liverpool_2['senti_wordnet']==1])/len(Liverpool_2)\n",
        "Liverpool_neg_2=len(Liverpool_2[Liverpool_2['senti_wordnet']==-1])/len(Liverpool_2)\n",
        "print(\"Liverpool_pos_2\",round(Liverpool_pos_2,3))\n",
        "print(\"Liverpool_neg_2\",round(Liverpool_neg_2,3))\n",
        "\n",
        "#Manchester\n",
        "Manchester_2 = df2[df2['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_2 = Manchester_2.reset_index(drop=True)\n",
        "Manchester_pos_2=len(Manchester_2[Manchester_2['senti_wordnet']==1])/len(Manchester_2)\n",
        "Manchester_neg_2=len(Manchester_2[Manchester_2['senti_wordnet']==-1])/len(Manchester_2)\n",
        "print(\"Manchester_pos_2\",round(Manchester_pos_2,3))\n",
        "print(\"Manchester_neg_2\",round(Manchester_neg_2,3))\n",
        "\n",
        "#Newcastle\n",
        "Newcastle_2 = df2[df2['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_2 = Newcastle_2.reset_index(drop=True)\n",
        "Newcastle_pos_2=len(Newcastle_2[Newcastle_2['senti_wordnet']==1])/len(Newcastle_2)\n",
        "Newcastle_neg_2=len(Newcastle_2[Newcastle_2['senti_wordnet']==-1])/len(Newcastle_2)\n",
        "print(\"Newcastle_pos_2\",round(Newcastle_pos_2,3))\n",
        "print(\"Newcastle_neg_2\",round(Newcastle_neg_2,3))\n",
        "\n",
        "#Nottingham\n",
        "Nottingham_2 = df2[df2['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_2 = Nottingham_2.reset_index(drop=True)\n",
        "Nottingham_pos_2=len(Nottingham_2[Nottingham_2['senti_wordnet']==1])/len(Nottingham_2)\n",
        "Nottingham_neg_2=len(Nottingham_2[Nottingham_2['senti_wordnet']==-1])/len(Nottingham_2)\n",
        "print(\"Nottingham_pos_2\",round(Nottingham_pos_2,3))\n",
        "print(\"Nottingham_neg_2\",round(Nottingham_neg_2,3))\n",
        "\n",
        "#Sheffield\n",
        "Sheffield_2 = df2[df2['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_2 = Sheffield_2.reset_index(drop=True)\n",
        "Sheffield_pos_2=len(Sheffield_2[Sheffield_2['senti_wordnet']==1])/len(Sheffield_2)\n",
        "Sheffield_neg_2=len(Sheffield_2[Sheffield_2['senti_wordnet']==-1])/len(Sheffield_2)\n",
        "print(\"Sheffield_pos_2\",round(Sheffield_pos_2,3))\n",
        "print(\"Sheffield_neg_2\",round(Sheffield_neg_2,3))\n",
        "\n",
        "#Southampton\n",
        "Southampton_2 = df2[df2['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_2 = Southampton_2.reset_index(drop=True)\n",
        "Southampton_pos_2=len(Southampton_2[Southampton_2['senti_wordnet']==1])/len(Southampton_2)\n",
        "Southampton_neg_2=len(Southampton_2[Southampton_2['senti_wordnet']==-1])/len(Southampton_2)\n",
        "print(\"Southampton_pos_2\",round(Southampton_pos_2,3))\n",
        "print(\"Southampton_neg_2\",round(Southampton_neg_2,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk2LeSLs_2S_"
      },
      "outputs": [],
      "source": [
        "#step3--Wordnet\n",
        "#London\n",
        "#London\n",
        "London_3 = df3[df3['near'].str.contains('London|london')]\n",
        "London_3 = London_3.reset_index(drop=True)\n",
        "#rows include 1/ all rows\n",
        "london_pos_3=len(London_3[London_3['senti_wordnet']==1])/len(London_3)\n",
        "#len(London_1[London_1['senti_textblob']==0])/len(London_1)\n",
        "london_neg_3=len(London_3[London_3['senti_wordnet']==-1])/len(London_3)\n",
        "print(\"london_pos_3\",round(london_pos_3,3))\n",
        "print(\"london_neg_3\",round(london_neg_3,3))\n",
        "\n",
        "#Birmingham\n",
        "Birmingham_3 = df3[df3['near'].str.contains('Birmingham|birmingham')]\n",
        "Birmingham_3 = Birmingham_3.reset_index(drop=True)\n",
        "#rows include 1 or -1/ all rows\n",
        "Birmingham_pos_3=len(Birmingham_3[Birmingham_3['senti_wordnet']==1])/len(Birmingham_3)\n",
        "Birmingham_neg_3=len(Birmingham_3[Birmingham_3['senti_wordnet']==-1])/len(Birmingham_3)\n",
        "print(\"Birmingham_pos_3\",round(Birmingham_pos_3,3))\n",
        "print(\"Birmingham_neg_3\",round(Birmingham_neg_3,3))\n",
        "\n",
        "#Bristol\n",
        "Bristol_3 = df3[df3['near'].str.contains('Bristol|bristol')]\n",
        "Bristol_3 = Bristol_3.reset_index(drop=True)\n",
        "Bristol_pos_3=len(Bristol_3[Bristol_3['senti_wordnet']==1])/len(Bristol_3)\n",
        "Bristol_neg_3=len(Bristol_3[Bristol_3['senti_wordnet']==-1])/len(Bristol_3)\n",
        "print(\"Bristol_pos_3\",round(Bristol_pos_3,3))\n",
        "print(\"Bristol_neg_3\",round(Bristol_neg_3,3))\n",
        "\n",
        "# #Leeds\n",
        "Leeds_3 = df3[df3['near'].str.contains('Leeds|leeds')]\n",
        "Leeds_3 = Leeds_3.reset_index(drop=True)\n",
        "Leeds_pos_3=len(Leeds_3[Leeds_3['senti_wordnet']==1])/len(Leeds_3)\n",
        "Leeds_neg_3=len(Leeds_3[Leeds_3['senti_wordnet']==-1])/len(Leeds_3)\n",
        "print(\"Leeds_pos_3\",round(Leeds_pos_3,3))\n",
        "print(\"Leeds_neg_3\",round(Leeds_neg_3,3))\n",
        "\n",
        "# #Liverpool\n",
        "Liverpool_3 = df3[df3['near'].str.contains('Liverpool|liverpool')]\n",
        "Liverpool_3 = Liverpool_3.reset_index(drop=True)\n",
        "Liverpool_pos_3=len(Liverpool_3[Liverpool_3['senti_wordnet']==1])/len(Liverpool_3)\n",
        "Liverpool_neg_3=len(Liverpool_3[Liverpool_3['senti_wordnet']==-1])/len(Liverpool_3)\n",
        "print(\"Liverpool_pos_3\",round(Liverpool_pos_3,3))\n",
        "print(\"Liverpool_neg_3\",round(Liverpool_neg_3,3))\n",
        "\n",
        "#Manchester\n",
        "Manchester_3 = df3[df3['near'].str.contains('Manchester|manchester')]\n",
        "Manchester_3 = Manchester_3.reset_index(drop=True)\n",
        "Manchester_pos_3=len(Manchester_3[Manchester_3['senti_wordnet']==1])/len(Manchester_3)\n",
        "Manchester_neg_3=len(Manchester_3[Manchester_3['senti_wordnet']==-1])/len(Manchester_3)\n",
        "print(\"Manchester_pos_3\",round(Manchester_pos_3,2))\n",
        "print(\"Manchester_neg_3\",round(Manchester_neg_3,2))\n",
        "\n",
        "#Newcastle\n",
        "Newcastle_3 = df3[df3['near'].str.contains('Newcastle|newcastle')]\n",
        "Newcastle_3 = Newcastle_3.reset_index(drop=True)\n",
        "Newcastle_pos_3=len(Newcastle_3[Newcastle_3['senti_wordnet']==1])/len(Newcastle_3)\n",
        "Newcastle_neg_3=len(Newcastle_3[Newcastle_3['senti_wordnet']==-1])/len(Newcastle_3)\n",
        "print(\"Newcastle_pos_3\",round(Newcastle_pos_3,3))\n",
        "print(\"Newcastle_neg_3\",round(Newcastle_neg_3,3))\n",
        "\n",
        "#Nottingham\n",
        "Nottingham_3 = df3[df3['near'].str.contains('Nottingham|nottingham')]\n",
        "Nottingham_3 = Nottingham_3.reset_index(drop=True)\n",
        "Nottingham_pos_3=len(Nottingham_3[Nottingham_3['senti_wordnet']==1])/len(Nottingham_3)\n",
        "Nottingham_neg_3=len(Nottingham_3[Nottingham_3['senti_wordnet']==-1])/len(Nottingham_3)\n",
        "print(\"Nottingham_pos_3\",round(Nottingham_pos_3,3))\n",
        "print(\"Nottingham_neg_3\",round(Nottingham_neg_3,3))\n",
        "\n",
        "#Sheffield\n",
        "Sheffield_3 = df3[df3['near'].str.contains('Sheffield|sheffield')]\n",
        "Sheffield_3 = Sheffield_3.reset_index(drop=True)\n",
        "Sheffield_pos_3=len(Sheffield_3[Sheffield_3['senti_wordnet']==1])/len(Sheffield_3)\n",
        "Sheffield_neg_3=len(Sheffield_3[Sheffield_3['senti_wordnet']==-1])/len(Sheffield_3)\n",
        "print(\"Sheffield_pos_3\",round(Sheffield_pos_3,3))\n",
        "print(\"Sheffield_neg_3\",round(Sheffield_neg_3,3))\n",
        "\n",
        "#Southampton\n",
        "Southampton_3 = df3[df3['near'].str.contains('Southampton|southampton')]\n",
        "Southampton_3 = Southampton_3.reset_index(drop=True)\n",
        "Southampton_pos_3=len(Southampton_3[Southampton_3['senti_wordnet']==1])/len(Southampton_3)\n",
        "Southampton_neg_3=len(Southampton_3[Southampton_3['senti_wordnet']==-1])/len(Southampton_3)\n",
        "print(\"Southampton_pos_3\",round(Southampton_pos_3,3))\n",
        "print(\"Southampton_neg_3\",round(Southampton_neg_3,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDflw_k1_2S_"
      },
      "source": [
        "# Supervised Approaches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwXQGs6q_2S_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim \n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzu6Q_pE_2TA"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_csv('supervised_sample_datasets/lexicon_step1.csv')\n",
        "df = supervised_lexicon_step1\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TW9kAug_2TA"
      },
      "outputs": [],
      "source": [
        "#sample data\n",
        "positive_df = df[df[\"senti_vader\"] == 1]\n",
        "positive_df = positive_df[:1000] #1000 positive sentiment\n",
        "neutral_df = df[df[\"senti_vader\"] == 0]\n",
        "neutral_df = neutral_df[:1000] #1000 neutral sentiment\n",
        "negative_df = df[df[\"senti_vader\"] == -1]\n",
        "negative_df = negative_df[:1000] #1000 neutral sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Of5zEKc_2TA"
      },
      "outputs": [],
      "source": [
        "df=[positive_df, neutral_df, negative_df]\n",
        "df=pd.concat(df)\n",
        "df=df.reset_index(drop=True)\n",
        "df.to_csv('sample_data.csv', index=False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqIoLLGtAhix"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QULsQ5OCOKeo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUNndAYvOM4a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim \n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sample_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNZq1Lp3_2TA"
      },
      "outputs": [],
      "source": [
        "#after manual check\n",
        "negative_num=len(df[df['senti_vader'] < 0])\n",
        "print(\"negative:\", negative_num)\n",
        "neutral_num=len(df[df['senti_vader'] == 0])\n",
        "print(\"neutral\", neutral_num)\n",
        "positive_num=len(df[df['senti_vader'] > 0])\n",
        "print(\"positive\", positive_num) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjY0NcjW_2TA"
      },
      "outputs": [],
      "source": [
        "#BoW\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def bag_of_words(df):\n",
        "#     bow_vectorizer = CountVectorizer(max_df=0.90, min_df=0.2, stop_words=None, tokenizer=word_tokenize) \n",
        "    bow_vectorizer = CountVectorizer() \n",
        "    bow = bow_vectorizer.fit_transform(df['lemma_sentence(with POS)']) \n",
        "    #print(bow_vectorizer.get_feature_names())\n",
        "    #print(bow_vectorizer.vocabulary_)\n",
        "    return bow\n",
        "\n",
        "df_bow=bag_of_words(df)\n",
        "df_bow.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofxvGqW9_2TA"
      },
      "outputs": [],
      "source": [
        "#TF-IDF\n",
        "def tf_idf(df):\n",
        "#     tf_idf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.2, stop_words=None, tokenizer=word_tokenize, norm='l2') \n",
        "    tf_idf_vectorizer = TfidfVectorizer(norm='l2') #extract features\n",
        "    tfidf = tf_idf_vectorizer.fit_transform(df['lemma_sentence(with POS)']) #vectors\n",
        "    return tfidf\n",
        "df_tfidf=tf_idf(df)\n",
        "df_tfidf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0sBj_zD_2TB"
      },
      "outputs": [],
      "source": [
        "#Word2vec\n",
        "#reference：https://www.pythonf.cn/read/93491\n",
        "#https://github.com/Shwetago/Sentiment_Analysis/blob/master/Twitter_Sentiment_Analysis.ipynb\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "Tokenize_tweet = df['lemma_sentence(with POS)'].apply(word_tokenize)\n",
        "print(Tokenize_tweet)\n",
        "\n",
        "Model_W2V = gensim.models.Word2Vec(Tokenize_tweet, vector_size=200, #features\n",
        "                                   window=5, \n",
        "                                   min_count=1, \n",
        "                                   sg=1,  #skip-gram model\n",
        "                                   hs=0,\n",
        "                                   negative=10, \n",
        "                                   workers=2, \n",
        "                                   seed=34) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5432152_2TB"
      },
      "outputs": [],
      "source": [
        "#Each word can get its own vector. The representation of a tweets can the vector sum of each word divided by the total number(average) \n",
        "#or just the sum of each word vector\n",
        "def word2vec_tweet(tokens, size):\n",
        "    vector=np.zeros(size).reshape((1,size))\n",
        "    vector_cnt = 0\n",
        "    for word in tokens:\n",
        "        vector += Model_W2V.wv[word].reshape((1, size))\n",
        "        vector_cnt += 1\n",
        "    return vector/vector_cnt  #average for tweets\n",
        "\n",
        "def word2vec_tweet_2(tokens, size):\n",
        "    vector=np.zeros(size).reshape((1,size))\n",
        "    vector_cnt = 0\n",
        "    for word in tokens:\n",
        "        vector += Model_W2V.wv[word].reshape((1, size))\n",
        "    return vector  #sum of tweets\n",
        "\n",
        "tweet_arr=np.zeros((len(Tokenize_tweet), 200))\n",
        "\n",
        "for i in range (len(Tokenize_tweet)):\n",
        "    tweet_arr[i,:] = word2vec_tweet(Tokenize_tweet[i], 200)\n",
        "tweet_vec_df = pd.DataFrame(tweet_arr)\n",
        "tweet_vec_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Wbag2M_2TF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP2W_K2B_2TG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzaynIHu_2TG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex3V_fHr_2TG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6Fa8sZK_2TG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCeQ37Z__2TG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}